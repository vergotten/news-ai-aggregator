#!/bin/bash

# =============================================================================
# News Aggregator - –°–∫—Ä–∏–ø—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—è –ø—Ä–æ–µ–∫—Ç–∞
# =============================================================================
# –°–æ–∑–¥–∞–µ—Ç –ø–æ–ª–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø—Ä–æ–µ–∫—Ç–∞ —Å–æ –≤—Å–µ–º–∏ —Ñ–∞–π–ª–∞–º–∏ –∏ –∫–æ–¥–æ–º
# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ: bash setup_project.sh
# =============================================================================

set -e  # –û—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø—Ä–∏ –æ—à–∏–±–∫–µ

# –¶–≤–µ—Ç–∞ –¥–ª—è –≤—ã–≤–æ–¥–∞
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è –∫—Ä–∞—Å–∏–≤–æ–≥–æ –≤—ã–≤–æ–¥–∞
log_info() {
    echo -e "${BLUE}[INFO]${NC} $1"
}

log_success() {
    echo -e "${GREEN}[SUCCESS]${NC} $1"
}

log_warning() {
    echo -e "${YELLOW}[WARNING]${NC} $1"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

# –ó–∞–≥–æ–ª–æ–≤–æ–∫
echo -e "${GREEN}"
cat << "EOF"
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë                  NEWS AGGREGATOR SETUP                        ‚ïë
‚ïë             –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞              ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
EOF
echo -e "${NC}"

PROJECT_ROOT=$(pwd)
log_info "–ö–æ—Ä–Ω–µ–≤–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: $PROJECT_ROOT"

# =============================================================================
# –°–æ–∑–¥–∞–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π
# =============================================================================
log_info "–°–æ–∑–¥–∞–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π..."

mkdir -p src/{models,scrapers,utils}
mkdir -p tests/{unit,integration}
mkdir -p docs
mkdir -p data logs sessions

log_success "–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π —Å–æ–∑–¥–∞–Ω–∞"

# =============================================================================
# 1. docker-compose.yml
# =============================================================================
log_info "–°–æ–∑–¥–∞–Ω–∏–µ docker-compose.yml..."

cat > docker-compose.yml << 'DCEOF'
services:
  postgres:
    image: postgres:15-alpine
    container_name: news-aggregator-db
    restart: unless-stopped
    environment:
      POSTGRES_USER: ${POSTGRES_USER:-newsaggregator}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-changeme123}
      POSTGRES_DB: ${POSTGRES_DB:-news_aggregator}
      PGDATA: /var/lib/postgresql/data/pgdata
    ports:
      - "${POSTGRES_PORT:-5432}:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./init-db.sql:/docker-entrypoint-initdb.d/init.sql:ro
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-newsaggregator} -d ${POSTGRES_DB:-news_aggregator}"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    networks:
      - news-aggregator-network

  app:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: news-aggregator-app
    restart: unless-stopped
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      DATABASE_URL: postgresql://${POSTGRES_USER:-newsaggregator}:${POSTGRES_PASSWORD:-changeme123}@postgres:5432/${POSTGRES_DB:-news_aggregator}
      REDDIT_CLIENT_ID: ${REDDIT_CLIENT_ID}
      REDDIT_CLIENT_SECRET: ${REDDIT_CLIENT_SECRET}
      REDDIT_USER_AGENT: ${REDDIT_USER_AGENT:-NewsAggregator/1.0}
      TELEGRAM_API_ID: ${TELEGRAM_API_ID}
      TELEGRAM_API_HASH: ${TELEGRAM_API_HASH}
      TELEGRAM_PHONE: ${TELEGRAM_PHONE}
      PYTHONUNBUFFERED: 1
      TZ: ${TZ:-UTC}
      STREAMLIT_SERVER_PORT: ${STREAMLIT_SERVER_PORT:-8501}
      STREAMLIT_SERVER_ADDRESS: ${STREAMLIT_SERVER_ADDRESS:-localhost}
    ports:
      - "${APP_PORT:-8501}:8501"
    volumes:
      - ./src:/app/src:ro
      - app_logs:/app/logs
      - app_data:/app/data
    networks:
      - news-aggregator-network

  n8n:
    image: n8nio/n8n:latest
    container_name: news-aggregator-n8n
    restart: unless-stopped
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      DB_TYPE: postgresdb
      DB_POSTGRESDB_HOST: postgres
      DB_POSTGRESDB_PORT: 5432
      DB_POSTGRESDB_DATABASE: ${N8N_DB:-n8n}
      DB_POSTGRESDB_USER: ${POSTGRES_USER:-newsaggregator}
      DB_POSTGRESDB_PASSWORD: ${POSTGRES_PASSWORD:-changeme123}
      N8N_BASIC_AUTH_ACTIVE: ${N8N_BASIC_AUTH_ACTIVE:-true}
      N8N_BASIC_AUTH_USER: ${N8N_BASIC_AUTH_USER:-admin}
      N8N_BASIC_AUTH_PASSWORD: ${N8N_BASIC_AUTH_PASSWORD:-admin123}
      N8N_ENFORCE_SETTINGS_FILE_PERMISSIONS: ${N8N_ENFORCE_SETTINGS_FILE_PERMISSIONS:-true}
      TZ: ${TZ:-UTC}
    ports:
      - "${N8N_PORT:-5678}:5678"
    volumes:
      - n8n_data:/home/node/.n8n
    networks:
      - news-aggregator-network

  ollama:
    image: ollama/ollama:latest
    container_name: news-aggregator-ollama
    restart: unless-stopped
    environment:
      OLLAMA_HOST: ${OLLAMA_HOST:-http://0.0.0.0:11434}
      OLLAMA_MODELS: ${OLLAMA_MODELS:-/root/.ollama/models}
      OLLAMA_DEBUG: ${OLLAMA_DEBUG:-INFO}
      OLLAMA_CONTEXT_LENGTH: ${OLLAMA_CONTEXT_LENGTH:-4096}
    ports:
      - "${OLLAMA_PORT:-11434}:11434"
    volumes:
      - ollama_data:/root/.ollama
    networks:
      - news-aggregator-network

  adminer:
    image: adminer:latest
    container_name: news-aggregator-adminer
    restart: unless-stopped
    ports:
      - "${ADMINER_PORT:-8080}:8080"
    networks:
      - news-aggregator-network

volumes:
  postgres_data:
  n8n_data:
  ollama_data:
  app_logs:
  app_data:

networks:
  news-aggregator-network:
    driver: bridge
DCEOF

log_success "docker-compose.yml —Å–æ–∑–¥–∞–Ω"

# =============================================================================
# 2. Dockerfile
# =============================================================================
log_info "–°–æ–∑–¥–∞–Ω–∏–µ Dockerfile..."

cat > Dockerfile << 'DKEOF'
FROM python:3.10-slim as builder

RUN apt-get update && apt-get install -y --no-install-recommends \
    build-essential gcc g++ && \
    rm -rf /var/lib/apt/lists/*

RUN python -m venv /opt/venv
ENV PATH="/opt/venv/bin:$PATH"

COPY requirements.txt .
RUN pip install --no-cache-dir --upgrade pip && \
    pip install --no-cache-dir -r requirements.txt

FROM python:3.10-slim

RUN apt-get update && apt-get install -y --no-install-recommends \
    curl && rm -rf /var/lib/apt/lists/*

RUN groupadd --gid 1000 appuser && \
    useradd --uid 1000 --gid appuser --create-home appuser

COPY --from=builder /opt/venv /opt/venv

WORKDIR /app

RUN mkdir -p /app/logs /app/data && \
    chown -R appuser:appuser /app

COPY --chown=appuser:appuser src/ /app/src/
COPY --chown=appuser:appuser requirements.txt /app/

RUN mkdir -p /home/appuser/.streamlit && \
    echo '[server]' > /home/appuser/.streamlit/config.toml && \
    echo 'port = 8501' >> /home/appuser/.streamlit/config.toml && \
    echo 'address = "0.0.0.0"' >> /home/appuser/.streamlit/config.toml && \
    echo 'headless = true' >> /home/appuser/.streamlit/config.toml && \
    echo '' >> /home/appuser/.streamlit/config.toml && \
    echo '[browser]' >> /home/appuser/.streamlit/config.toml && \
    echo 'gatherUsageStats = false' >> /home/appuser/.streamlit/config.toml && \
    echo 'serverAddress = "localhost"' >> /home/appuser/.streamlit/config.toml && \
    echo 'serverPort = 8501' >> /home/appuser/.streamlit/config.toml && \
    chown -R appuser:appuser /home/appuser/.streamlit

ENV PYTHONUNBUFFERED=1 \
    PYTHONPATH=/app \
    PATH="/opt/venv/bin:$PATH"

USER appuser

EXPOSE 8501

CMD ["python", "-m", "streamlit", "run", "src/app.py", \
     "--server.port=8501", \
     "--server.address=0.0.0.0", \
     "--browser.serverAddress=localhost", \
     "--browser.serverPort=8501"]
DKEOF

log_success "Dockerfile —Å–æ–∑–¥–∞–Ω"

# =============================================================================
# 3. .env.example
# =============================================================================
log_info "–°–æ–∑–¥–∞–Ω–∏–µ .env.example..."

cat > .env.example << 'ENVEOF'
# PostgreSQL Configuration
POSTGRES_USER=newsaggregator
POSTGRES_PASSWORD=changeme123
POSTGRES_DB=news_aggregator
POSTGRES_PORT=5432

# Database URL for application
DATABASE_URL=postgresql://newsaggregator:changeme123@postgres:5432/news_aggregator

# Reddit API Credentials
REDDIT_CLIENT_ID=your_client_id_here
REDDIT_CLIENT_SECRET=your_client_secret_here
REDDIT_USER_AGENT=NewsAggregatorBot/1.0 by /u/your_username

# Telegram API Credentials
TELEGRAM_API_ID=your_api_id
TELEGRAM_API_HASH=your_api_hash
TELEGRAM_PHONE=your_phone_number

# n8n Configuration
N8N_BASIC_AUTH_ACTIVE=true
N8N_BASIC_AUTH_USER=admin
N8N_BASIC_AUTH_PASSWORD=admin123
N8N_DB=n8n
N8N_PORT=5678

# Ollama Configuration
OLLAMA_HOST=http://ollama:11434
OLLAMA_MODELS=/root/.ollama/models
OLLAMA_DEBUG=INFO
OLLAMA_CONTEXT_LENGTH=4096
OLLAMA_PORT=11434

# Application Configuration
APP_PORT=8501
STREAMLIT_SERVER_PORT=8501
STREAMLIT_SERVER_ADDRESS=localhost
TZ=UTC

# Adminer
ADMINER_PORT=8080
ENVEOF

log_success ".env.example —Å–æ–∑–¥–∞–Ω"

# =============================================================================
# 4. init-db.sql
# =============================================================================
log_info "–°–æ–∑–¥–∞–Ω–∏–µ init-db.sql..."

cat > init-db.sql << 'SQLEOF'
-- Create databases if they don't exist
SELECT 'CREATE DATABASE news_aggregator OWNER newsaggregator'
WHERE NOT EXISTS (SELECT FROM pg_database WHERE datname = 'news_aggregator')\gexec

SELECT 'CREATE DATABASE n8n OWNER newsaggregator'
WHERE NOT EXISTS (SELECT FROM pg_database WHERE datname = 'n8n')\gexec

-- Connect to news_aggregator
\c news_aggregator;

CREATE SCHEMA IF NOT EXISTS app;

GRANT ALL PRIVILEGES ON SCHEMA public TO newsaggregator;
GRANT ALL PRIVILEGES ON SCHEMA app TO newsaggregator;

CREATE TABLE IF NOT EXISTS app.articles (
    id SERIAL PRIMARY KEY,
    title TEXT NOT NULL,
    url TEXT UNIQUE NOT NULL,
    source VARCHAR(100) NOT NULL,
    published_date TIMESTAMP,
    content TEXT,
    summary TEXT,
    tags TEXT[],
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE IF NOT EXISTS app.sources (
    id SERIAL PRIMARY KEY,
    name VARCHAR(100) UNIQUE NOT NULL,
    type VARCHAR(50) NOT NULL,
    url TEXT,
    enabled BOOLEAN DEFAULT TRUE,
    last_scraped TIMESTAMP,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE IF NOT EXISTS app.user_preferences (
    id SERIAL PRIMARY KEY,
    user_id VARCHAR(100) UNIQUE NOT NULL,
    preferred_sources TEXT[],
    preferred_tags TEXT[],
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX IF NOT EXISTS idx_articles_source ON app.articles(source);
CREATE INDEX IF NOT EXISTS idx_articles_published_date ON app.articles(published_date);
CREATE INDEX IF NOT EXISTS idx_articles_created_at ON app.articles(created_at);
CREATE INDEX IF NOT EXISTS idx_sources_enabled ON app.sources(enabled);
CREATE INDEX IF NOT EXISTS idx_user_preferences_user_id ON app.user_preferences(user_id);

GRANT ALL PRIVILEGES ON ALL TABLES IN SCHEMA app TO newsaggregator;
GRANT ALL PRIVILEGES ON ALL SEQUENCES IN SCHEMA app TO newsaggregator;
GRANT ALL PRIVILEGES ON ALL TABLES IN SCHEMA public TO newsaggregator;
GRANT ALL PRIVILEGES ON ALL SEQUENCES IN SCHEMA public TO newsaggregator;

ALTER DEFAULT PRIVILEGES IN SCHEMA app GRANT ALL ON TABLES TO newsaggregator;
ALTER DEFAULT PRIVILEGES IN SCHEMA app GRANT ALL ON SEQUENCES TO newsaggregator;
ALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT ALL ON TABLES TO newsaggregator;
ALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT ALL ON SEQUENCES TO newsaggregator;

-- Connect to n8n
\c n8n;
GRANT ALL PRIVILEGES ON DATABASE n8n TO newsaggregator;
GRANT ALL PRIVILEGES ON SCHEMA public TO newsaggregator;
ALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT ALL ON TABLES TO newsaggregator;
ALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT ALL ON SEQUENCES TO newsaggregator;
SQLEOF

log_success "init-db.sql —Å–æ–∑–¥–∞–Ω"

# =============================================================================
# 5. requirements.txt
# =============================================================================
log_info "–°–æ–∑–¥–∞–Ω–∏–µ requirements.txt..."

cat > requirements.txt << 'REQEOF'
# Web Framework
streamlit==1.31.0

# Database
sqlalchemy==2.0.25
psycopg2-binary==2.9.9

# Reddit API
praw==7.8.1

# Web Scraping
requests==2.31.0
beautifulsoup4==4.12.3
lxml==5.1.0

# Telegram (optional)
telethon==1.34.0

# Utilities
python-dotenv==1.0.1
pandas==2.1.4
REQEOF

log_success "requirements.txt —Å–æ–∑–¥–∞–Ω"

# =============================================================================
# 6. .gitignore
# =============================================================================
log_info "–°–æ–∑–¥–∞–Ω–∏–µ .gitignore..."

cat > .gitignore << 'GITEOF'
# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
*.egg-info/
.installed.cfg
*.egg

# Virtual Environment
venv/
.venv/
ENV/
env/

# Environment Variables
.env
.env.local
.env.*.local

# Logs
logs/
*.log

# Data
data/
sessions/

# Docker
postgres_data/
n8n_data/
ollama_data/
app_logs/
app_data/

# IDEs
.vscode/
.idea/
*.swp
*.swo
*~

# OS
.DS_Store
Thumbs.db
GITEOF

log_success ".gitignore —Å–æ–∑–¥–∞–Ω"

# =============================================================================
# 7. __init__.py —Ñ–∞–π–ª—ã
# =============================================================================
log_info "–°–æ–∑–¥–∞–Ω–∏–µ __init__.py —Ñ–∞–π–ª–æ–≤..."

touch src/__init__.py
touch src/models/__init__.py
touch src/scrapers/__init__.py
touch src/utils/__init__.py
touch tests/__init__.py
touch tests/unit/__init__.py
touch tests/integration/__init__.py

log_success "__init__.py —Ñ–∞–π–ª—ã —Å–æ–∑–¥–∞–Ω—ã"

# =============================================================================
# 8. src/models/database.py
# =============================================================================
log_info "–°–æ–∑–¥–∞–Ω–∏–µ src/models/database.py..."

cat > src/models/database.py << 'PYEOF'
"""Database models and operations for News Aggregator."""
import os
from datetime import datetime
from typing import List, Dict, Optional
from dataclasses import dataclass

from sqlalchemy import create_engine, Column, Integer, String, Text, DateTime, Boolean, ARRAY
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker, Session
from sqlalchemy.sql import func

DATABASE_URL = os.getenv("DATABASE_URL")
if not DATABASE_URL:
    raise ValueError("DATABASE_URL environment variable not set")

engine = create_engine(DATABASE_URL, pool_pre_ping=True, pool_size=5, max_overflow=10)
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)
Base = declarative_base()


@dataclass
class RedditPost:
    """Reddit post data container."""
    id: int
    subreddit: str
    title: str
    url: str
    score: int
    num_comments: int
    author: str
    created_utc: datetime
    selftext: Optional[str] = None


@dataclass
class MediumArticle:
    """Medium article data container."""
    id: int
    title: str
    url: str
    author: str
    claps: int
    is_paywalled: bool
    published_at: datetime


class Article(Base):
    """Article model for app.articles table."""
    __tablename__ = "articles"
    __table_args__ = {"schema": "app"}

    id = Column(Integer, primary_key=True, autoincrement=True)
    title = Column(Text, nullable=False)
    url = Column(Text, unique=True, nullable=False)
    source = Column(String(100), nullable=False)
    published_date = Column(DateTime)
    content = Column(Text)
    summary = Column(Text)
    tags = Column(ARRAY(Text))
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)


class Source(Base):
    """Source model for app.sources table."""
    __tablename__ = "sources"
    __table_args__ = {"schema": "app"}

    id = Column(Integer, primary_key=True, autoincrement=True)
    name = Column(String(100), unique=True, nullable=False)
    type = Column(String(50), nullable=False)
    url = Column(Text)
    enabled = Column(Boolean, default=True)
    last_scraped = Column(DateTime)
    created_at = Column(DateTime, default=datetime.utcnow)


def init_db() -> None:
    """Initialize database connection and verify tables exist."""
    try:
        Base.metadata.create_all(bind=engine)
        print("‚úÖ –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –ë–î: {e}")
        raise RuntimeError(f"Database initialization failed: {e}")


def get_session() -> Session:
    """Create new database session."""
    return SessionLocal()


def save_article(
    title: str,
    url: str,
    source: str,
    published_date: Optional[datetime] = None,
    content: Optional[str] = None,
    summary: Optional[str] = None,
    tags: Optional[List[str]] = None
) -> bool:
    """Save article to database."""
    session = get_session()
    try:
        existing = session.query(Article).filter_by(url=url).first()
        if existing:
            return False

        article = Article(
            title=title,
            url=url,
            source=source,
            published_date=published_date,
            content=content,
            summary=summary,
            tags=tags or []
        )
        session.add(article)
        session.commit()
        return True
    except Exception as e:
        session.rollback()
        raise RuntimeError(f"Failed to save article: {e}")
    finally:
        session.close()


def get_stats_extended() -> Dict[str, int]:
    """Get extended statistics from database."""
    session = get_session()
    try:
        reddit_count = session.query(func.count(Article.id)).filter(
            Article.source.like('reddit_%')
        ).scalar() or 0

        telegram_count = session.query(func.count(Article.id)).filter(
            Article.source == 'telegram'
        ).scalar() or 0

        medium_count = session.query(func.count(Article.id)).filter(
            Article.source == 'medium'
        ).scalar() or 0

        return {
            'reddit_posts': reddit_count,
            'telegram_messages': telegram_count,
            'medium_articles': medium_count,
            'latest_reddit': None,
            'latest_telegram': None,
            'latest_medium': None
        }
    finally:
        session.close()


def get_posts_by_subreddit(subreddit: str, limit: int = 10) -> List[RedditPost]:
    """Get Reddit posts by subreddit."""
    session = get_session()
    try:
        articles = session.query(Article).filter(
            Article.source == f'reddit_{subreddit.lower()}'
        ).order_by(Article.created_at.desc()).limit(limit).all()

        posts = []
        for article in articles:
            tags = article.tags or []
            posts.append(RedditPost(
                id=article.id,
                subreddit=subreddit,
                title=article.title,
                url=article.url,
                score=int(tags[0]) if tags and tags[0].isdigit() else 0,
                num_comments=int(tags[1]) if len(tags) > 1 and tags[1].isdigit() else 0,
                author=tags[2] if len(tags) > 2 else 'unknown',
                created_utc=article.published_date or article.created_at,
                selftext=article.content
            ))
        return posts
    finally:
        session.close()


def get_medium_articles(limit: int = 10) -> List[MediumArticle]:
    """Get Medium articles."""
    session = get_session()
    try:
        articles = session.query(Article).filter(
            Article.source == 'medium'
        ).order_by(Article.created_at.desc()).limit(limit).all()

        medium_articles = []
        for article in articles:
            tags = article.tags or []
            medium_articles.append(MediumArticle(
                id=article.id,
                title=article.title,
                url=article.url,
                author=tags[0] if tags else 'unknown',
                claps=int(tags[1]) if len(tags) > 1 and tags[1].isdigit() else 0,
                is_paywalled=len(tags) > 2 and tags[2] == 'paywalled',
                published_at=article.published_date or article.created_at
            ))
        return medium_articles
    finally:
        session.close()
PYEOF

log_success "database.py —Å–æ–∑–¥–∞–Ω"

# =============================================================================
# 9. src/scrapers/reddit_scraper.py
# =============================================================================
log_info "–°–æ–∑–¥–∞–Ω–∏–µ reddit_scraper.py..."

cat > src/scrapers/reddit_scraper.py << 'PYEOF'
"""Reddit scraper using PRAW."""
import os
import time
from datetime import datetime
from typing import List, Dict

import praw
from praw.exceptions import PRAWException

from src.models.database import save_article


def get_reddit_client() -> praw.Reddit:
    """Initialize Reddit API client."""
    client_id = os.getenv("REDDIT_CLIENT_ID")
    client_secret = os.getenv("REDDIT_CLIENT_SECRET")
    user_agent = os.getenv("REDDIT_USER_AGENT", "NewsAggregator/1.0")

    if not client_id or not client_secret:
        raise ValueError("Reddit API credentials not configured")

    return praw.Reddit(
        client_id=client_id,
        client_secret=client_secret,
        user_agent=user_agent
    )


def scrape_subreddit(
    subreddit_name: str,
    max_posts: int = 50,
    sort_by: str = "hot"
) -> Dict[str, any]:
    """
    Scrape posts from a subreddit.

    Args:
        subreddit_name: Name of subreddit
        max_posts: Maximum posts to fetch
        sort_by: Sort method ('hot', 'new', 'top')

    Returns:
        Dict with results
    """
    print(f"üîç –ü–∞—Ä—Å–∏–Ω–≥ r/{subreddit_name}")

    reddit = get_reddit_client()
    saved = 0
    skipped = 0
    errors = []

    try:
        subreddit = reddit.subreddit(subreddit_name)

        if sort_by == "hot":
            posts = subreddit.hot(limit=max_posts)
        elif sort_by == "new":
            posts = subreddit.new(limit=max_posts)
        elif sort_by == "top":
            posts = subreddit.top(limit=max_posts)
        else:
            posts = subreddit.hot(limit=max_posts)

        for post in posts:
            try:
                published_date = datetime.utcfromtimestamp(post.created_utc)

                # Store metadata in tags
                tags = [
                    str(post.score),
                    str(post.num_comments),
                    str(post.author) if post.author else 'deleted'
                ]

                is_saved = save_article(
                    title=post.title,
                    url=post.url,
                    source=f'reddit_{subreddit_name.lower()}',
                    published_date=published_date,
                    content=post.selftext if hasattr(post, 'selftext') else None,
                    tags=tags
                )

                if is_saved:
                    saved += 1
                else:
                    skipped += 1

            except Exception as e:
                errors.append(str(e))
                continue

        print(f"‚úÖ r/{subreddit_name}: {saved} –Ω–æ–≤—ã—Ö, {skipped} –ø—Ä–æ–ø—É—â–µ–Ω–æ")

        return {
            'success': True,
            'subreddit': subreddit_name,
            'saved': saved,
            'skipped': skipped,
            'errors': errors
        }

    except PRAWException as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ r/{subreddit_name}: {e}")
        return {
            'success': False,
            'subreddit': subreddit_name,
            'saved': saved,
            'skipped': skipped,
            'errors': [f"PRAW error: {str(e)}"]
        }
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ r/{subreddit_name}: {e}")
        return {
            'success': False,
            'subreddit': subreddit_name,
            'saved': saved,
            'skipped': skipped,
            'errors': [f"Unexpected error: {str(e)}"]
        }


def scrape_multiple_subreddits(
    subreddits: List[str],
    max_posts: int = 50,
    sort_by: str = "hot",
    delay: int = 5
) -> List[Dict[str, any]]:
    """
    Scrape multiple subreddits with delay between requests.

    Args:
        subreddits: List of subreddit names
        max_posts: Maximum posts per subreddit
        sort_by: Sort method
        delay: Delay in seconds between subreddits

    Returns:
        List of result dicts
    """
    results = []

    for idx, subreddit in enumerate(subreddits):
        result = scrape_subreddit(subreddit, max_posts, sort_by)
        results.append(result)

        if idx < len(subreddits) - 1:
            time.sleep(delay)

    return results
PYEOF

log_success "reddit_scraper.py —Å–æ–∑–¥–∞–Ω"

# =============================================================================
# 10. src/scrapers/medium_scraper.py
# =============================================================================
log_info "–°–æ–∑–¥–∞–Ω–∏–µ medium_scraper.py..."

cat > src/scrapers/medium_scraper.py << 'PYEOF'
"""Medium scraper using web scraping."""
import time
from typing import List, Dict
from datetime import datetime

import requests
from bs4 import BeautifulSoup

from src.models.database import save_article


HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
}


def scrape_medium_tag(tag: str, max_articles: int = 10) -> Dict[str, any]:
    """
    Scrape Medium articles by tag.

    Args:
        tag: Medium tag (e.g., 'machine-learning')
        max_articles: Maximum articles to fetch

    Returns:
        Dict with results
    """
    print(f"üîç –ü–∞—Ä—Å–∏–Ω–≥ Medium: #{tag}")

    saved = 0
    skipped = 0
    errors = []

    try:
        url = f"https://medium.com/tag/{tag}"
        response = requests.get(url, headers=HEADERS, timeout=10)
        response.raise_for_status()

        soup = BeautifulSoup(response.text, 'html.parser')

        # Find article cards - Medium's structure varies
        articles = soup.find_all('article', limit=max_articles * 2)

        for article in articles[:max_articles]:
            try:
                # Extract title
                title_elem = article.find('h2') or article.find('h3')
                if not title_elem:
                    continue
                title = title_elem.get_text(strip=True)

                # Extract link
                link_elem = article.find('a', href=True)
                if not link_elem:
                    continue
                article_url = link_elem['href']
                if not article_url.startswith('http'):
                    article_url = f"https://medium.com{article_url}"

                # Extract author
                author_elem = article.find('p', class_=lambda x: x and 'author' in x.lower())
                author = author_elem.get_text(strip=True) if author_elem else 'Unknown'

                # Check paywall indicator
                is_paywalled = bool(article.find(text=lambda t: t and 'member' in t.lower()))

                # Estimate claps (not always available via scraping)
                claps = 0
                claps_elem = article.find(text=lambda t: t and 'clap' in t.lower())
                if claps_elem:
                    try:
                        claps = int(''.join(filter(str.isdigit, claps_elem)))
                    except ValueError:
                        pass

                tags = [author, str(claps)]
                if is_paywalled:
                    tags.append('paywalled')

                is_saved = save_article(
                    title=title,
                    url=article_url,
                    source='medium',
                    published_date=datetime.utcnow(),
                    tags=tags
                )

                if is_saved:
                    saved += 1
                else:
                    skipped += 1

            except Exception as e:
                errors.append(f"Article parse error: {str(e)}")
                continue

        print(f"‚úÖ #{tag}: {saved} –Ω–æ–≤—ã—Ö, {skipped} –ø—Ä–æ–ø—É—â–µ–Ω–æ")

        return {
            'success': True,
            'tag': tag,
            'saved': saved,
            'skipped': skipped,
            'errors': errors
        }

    except requests.RequestException as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ Medium #{tag}: {e}")
        return {
            'success': False,
            'tag': tag,
            'saved': saved,
            'skipped': skipped,
            'errors': [f"Request error: {str(e)}"]
        }
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ Medium #{tag}: {e}")
        return {
            'success': False,
            'tag': tag,
            'saved': saved,
            'skipped': skipped,
            'errors': [f"Unexpected error: {str(e)}"]
        }


def scrape_multiple_sources(
    tags: List[str],
    max_articles: int = 10,
    delay: int = 3
) -> List[Dict[str, any]]:
    """
    Scrape multiple Medium tags with delay.

    Args:
        tags: List of Medium tags
        max_articles: Maximum articles per tag
        delay: Delay in seconds between requests

    Returns:
        List of result dicts
    """
    results = []

    for idx, tag in enumerate(tags):
        result = scrape_medium_tag(tag, max_articles)
        results.append(result)

        if idx < len(tags) - 1:
            time.sleep(delay)

    return results
PYEOF

log_success "medium_scraper.py —Å–æ–∑–¥–∞–Ω"

# =============================================================================
# 11. src/app.py - Streamlit UI
# =============================================================================
log_info "–°–æ–∑–¥–∞–Ω–∏–µ app.py (Streamlit UI)..."

cat > src/app.py << 'APPEOF'
"""Streamlit –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è News Aggregator."""
import streamlit as st
import os
import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent))

st.set_page_config(
    page_title="News Aggregator",
    page_icon="üì∞",
    layout="wide"
)

DATABASE_URL = os.getenv("DATABASE_URL")
if not DATABASE_URL:
    st.error("‚ùå DATABASE_URL –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω")
    st.stop()

try:
    from src.models.database import (
        init_db,
        get_stats_extended,
        get_posts_by_subreddit,
        get_medium_articles
    )
    init_db()
except Exception as e:
    st.error(f"‚ùå –û—à–∏–±–∫–∞ –ë–î: {e}")
    st.stop()

st.title("üì∞ News Aggregator")
st.caption("Reddit ‚Ä¢ Telegram ‚Ä¢ Medium")

col1, col2, col3 = st.columns(3)
with col1:
    if os.getenv("REDDIT_CLIENT_ID"):
        st.success("‚úÖ Reddit API")
    else:
        st.warning("‚ö†Ô∏è Reddit API")
with col2:
    if os.getenv("TELEGRAM_API_ID"):
        st.success("‚úÖ Telegram API")
    else:
        st.warning("‚ö†Ô∏è Telegram API")
with col3:
    st.success("‚úÖ Database")

st.markdown("---")

try:
    stats = get_stats_extended()
except:
    stats = {
        'reddit_posts': 0,
        'telegram_messages': 0,
        'medium_articles': 0,
        'latest_reddit': None,
        'latest_telegram': None,
        'latest_medium': None
    }

tab1, tab2, tab3, tab4 = st.tabs(["üî¥ Reddit", "üîµ Telegram", "üìù Medium", "üìä Analytics"])

with tab1:
    st.header("Reddit Parser")

    col1, col2 = st.columns([2, 1])

    with col1:
        st.subheader("‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏")

        default_subs = ["MachineLearning", "artificial", "Python"]
        selected_subs = st.multiselect(
            "Subreddits:",
            default_subs,
            default=["MachineLearning"]
        )

        col_a, col_b, col_c = st.columns(3)
        with col_a:
            max_posts = st.slider("–ú–∞–∫—Å. –ø–æ—Å—Ç–æ–≤:", 10, 200, 50)
        with col_b:
            delay = st.slider("–ó–∞–¥–µ—Ä–∂–∫–∞ (—Å–µ–∫):", 3, 30, 5)
        with col_c:
            sort_by = st.selectbox("–°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞:", ["hot", "new", "top"])

        if st.button("üöÄ –ù–∞—á–∞—Ç—å –ø–∞—Ä—Å–∏–Ω–≥ Reddit", type="primary", use_container_width=True):
            if not selected_subs:
                st.error("‚ùå –í—ã–±–µ—Ä–∏—Ç–µ subreddits")
            elif not os.getenv("REDDIT_CLIENT_ID"):
                st.error("‚ùå Reddit API –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω")
            else:
                with st.spinner("üîÑ –ü–∞—Ä—Å–∏–Ω–≥..."):
                    try:
                        from src.scrapers.reddit_scraper import scrape_multiple_subreddits

                        results = scrape_multiple_subreddits(
                            subreddits=selected_subs,
                            max_posts=max_posts,
                            sort_by=sort_by,
                            delay=delay
                        )

                        st.success("‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω–æ!")
                        for result in results:
                            if result['success']:
                                st.write(
                                    f"**r/{result['subreddit']}**: "
                                    f"‚úÖ {result['saved']}, "
                                    f"‚è≠Ô∏è {result['skipped']}"
                                )
                        st.rerun()
                    except Exception as e:
                        st.error(f"‚ùå {str(e)}")

    with col2:
        st.subheader("üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞")
        st.metric("–ü–æ—Å—Ç–æ–≤", f"{stats['reddit_posts']:,}")

        if stats['reddit_posts'] > 0 and selected_subs:
            st.subheader("üìÑ –ü–æ—Å–ª–µ–¥–Ω–∏–µ")
            try:
                posts = get_posts_by_subreddit(selected_subs[0], limit=5)
                for post in posts:
                    with st.expander(f"‚¨ÜÔ∏è {post.score} | {post.title[:40]}..."):
                        st.write(f"**r/{post.subreddit}** ‚Ä¢ {post.num_comments} –∫–æ–º–º.")
                        if post.url:
                            st.link_button("–û—Ç–∫—Ä—ã—Ç—å", post.url)
            except Exception as e:
                st.caption(f"–û—à–∏–±–∫–∞: {e}")

with tab2:
    st.header("Telegram Parser")
    st.info("üöß –í —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ")

with tab3:
    st.header("Medium Parser")

    col1, col2 = st.columns([2, 1])

    with col1:
        st.subheader("‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏")

        default_tags = ["machine-learning", "python"]
        selected_tags = st.multiselect(
            "–¢–µ–≥–∏:",
            default_tags,
            default=["machine-learning"]
        )

        col_a, col_b = st.columns(2)
        with col_a:
            max_articles = st.slider("–°—Ç–∞—Ç–µ–π:", 5, 50, 10)
        with col_b:
            delay_medium = st.slider("–ó–∞–¥–µ—Ä–∂–∫–∞:", 2, 10, 3)

        if st.button("üöÄ –ù–∞—á–∞—Ç—å –ø–∞—Ä—Å–∏–Ω–≥ Medium", type="primary", use_container_width=True):
            if not selected_tags:
                st.error("‚ùå –í—ã–±–µ—Ä–∏—Ç–µ —Ç–µ–≥–∏")
            else:
                with st.spinner("üîÑ –ü–∞—Ä—Å–∏–Ω–≥..."):
                    try:
                        from src.scrapers.medium_scraper import scrape_multiple_sources

                        results = scrape_multiple_sources(
                            tags=selected_tags,
                            max_articles=max_articles,
                            delay=delay_medium
                        )

                        st.success("‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω–æ!")
                        for result in results:
                            if result['success']:
                                st.write(
                                    f"üè∑Ô∏è **{result['tag']}**: "
                                    f"‚úÖ {result['saved']}, "
                                    f"‚è≠Ô∏è {result['skipped']}"
                                )
                        st.rerun()
                    except Exception as e:
                        st.error(f"‚ùå {str(e)}")

    with col2:
        st.subheader("üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞")
        st.metric("–°—Ç–∞—Ç–µ–π", f"{stats['medium_articles']:,}")

        if stats['medium_articles'] > 0:
            st.subheader("üìÑ –ü–æ—Å–ª–µ–¥–Ω–∏–µ")
            try:
                articles = get_medium_articles(limit=5)
                for article in articles:
                    icon = "üîí" if article.is_paywalled else "üìñ"
                    with st.expander(f"{icon} {article.title[:40]}..."):
                        st.write(f"**{article.author}** ‚Ä¢ üëè {article.claps}")
                        col_a, col_b = st.columns(2)
                        with col_a:
                            st.link_button("–û—Ç–∫—Ä—ã—Ç—å", article.url)
                        with col_b:
                            if article.is_paywalled:
                                st.link_button("üîì Freedium", f"https://freedium.cfd/{article.url}")
            except Exception as e:
                st.caption(f"–û—à–∏–±–∫–∞: {e}")

with tab4:
    st.header("–ê–Ω–∞–ª–∏—Ç–∏–∫–∞")

    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("Reddit", f"{stats['reddit_posts']:,}")
    with col2:
        st.metric("Telegram", f"{stats['telegram_messages']:,}")
    with col3:
        st.metric("Medium", f"{stats['medium_articles']:,}")
    with col4:
        total = stats['reddit_posts'] + stats['telegram_messages'] + stats['medium_articles']
        st.metric("–í—Å–µ–≥–æ", f"{total:,}")

    st.markdown("---")

    if total > 0:
        st.subheader("üìà –ê–∫—Ç–∏–≤–Ω–æ—Å—Ç—å")
        import pandas as pd

        chart_data = pd.DataFrame({
            '–ü–ª–∞—Ç—Ñ–æ—Ä–º–∞': ['Reddit', 'Telegram', 'Medium'],
            '–ó–∞–ø–∏—Å–µ–π': [
                stats['reddit_posts'],
                stats['telegram_messages'],
                stats['medium_articles']
            ]
        })
        st.bar_chart(chart_data.set_index('–ü–ª–∞—Ç—Ñ–æ—Ä–º–∞'))
    else:
        st.info("üìä –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö. –ó–∞–ø—É—Å—Ç–∏—Ç–µ –ø–∞—Ä—Å–∏–Ω–≥!")

st.markdown("---")
col_f1, col_f2 = st.columns([3, 1])
with col_f1:
    st.caption("PostgreSQL ‚Ä¢ Docker ‚Ä¢ N8N ‚Ä¢ Ollama")
with col_f2:
    if st.button("üîÑ –û–±–Ω–æ–≤–∏—Ç—å"):
        st.rerun()
APPEOF

log_success "app.py —Å–æ–∑–¥–∞–Ω"

# =============================================================================
# 12. –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Å–∫—Ä–∏–ø—Ç—ã
# =============================================================================
log_info "–°–æ–∑–¥–∞–Ω–∏–µ –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã—Ö —Å–∫—Ä–∏–ø—Ç–æ–≤..."

cat > start.sh << 'STARTEOF'
#!/bin/bash
echo "üöÄ –ó–∞–ø—É—Å–∫ News Aggregator..."
docker compose up -d
echo ""
echo "‚úÖ –°–µ—Ä–≤–∏—Å—ã –∑–∞–ø—É—â–µ–Ω—ã!"
echo ""
echo "üì± –ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å—ã:"
echo "   Streamlit:  http://localhost:8501"
echo "   N8N:        http://localhost:5678 (admin/admin123)"
echo "   Adminer:    http://localhost:8080"
echo "   Ollama:     http://localhost:11434"
echo ""
echo "üìä –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ç—É—Å–∞:"
echo "   docker compose ps"
echo ""
echo "üìã –õ–æ–≥–∏:"
echo "   docker compose logs -f app"
STARTEOF
chmod +x start.sh

cat > stop.sh << 'STOPEOF'
#!/bin/bash
echo "üõë –û—Å—Ç–∞–Ω–æ–≤–∫–∞ News Aggregator..."
docker compose down
echo "‚úÖ –í—Å–µ —Å–µ—Ä–≤–∏—Å—ã –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã"
STOPEOF
chmod +x stop.sh

cat > logs.sh << 'LOGSEOF'
#!/bin/bash
SERVICE=${1:-app}
echo "üìä –õ–æ–≥–∏ —Å–µ—Ä–≤–∏—Å–∞: $SERVICE"
echo "–ù–∞–∂–º–∏—Ç–µ Ctrl+C –¥–ª—è –≤—ã—Ö–æ–¥–∞"
echo "---"
docker compose logs -f $SERVICE
LOGSEOF
chmod +x logs.sh

cat > clean.sh << 'CLEANEOF'
#!/bin/bash
echo "üßπ –ü–æ–ª–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –ø—Ä–æ–µ–∫—Ç–∞..."
read -p "–£–¥–∞–ª–∏—Ç—å –≤—Å–µ –¥–∞–Ω–Ω—ã–µ –∏ volumes? (y/N): " -n 1 -r
echo
if [[ $REPLY =~ ^[Yy]$ ]]
then
    docker compose down -v
    docker volume prune -f
    echo "‚úÖ –û—á–∏—Å—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞"
else
    echo "‚ùå –û—Ç–º–µ–Ω–µ–Ω–æ"
fi
CLEANEOF
chmod +x clean.sh

log_success "–í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Å–∫—Ä–∏–ø—Ç—ã —Å–æ–∑–¥–∞–Ω—ã"

# =============================================================================
# 13. README.md
# =============================================================================
log_info "–°–æ–∑–¥–∞–Ω–∏–µ README.md..."

cat > README.md << 'READMEEOF'
# News Aggregator

–ú–Ω–æ–≥–æ—Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –∞–≥—Ä–µ–≥–∞—Ç–æ—Ä –Ω–æ–≤–æ—Å—Ç–µ–π —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –ø–∞—Ä—Å–∏–Ω–≥–∞ Reddit, Telegram –∏ Medium.

## üöÄ –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç

```bash
# 1. –ö–ª–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞
git clone <repository>
cd news-aggregator

# 2. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–∫—Ä—É–∂–µ–Ω–∏—è
cp .env.example .env
nano .env  # –î–æ–±–∞–≤—å—Ç–µ API –∫–ª—é—á–∏

# 3. –ó–∞–ø—É—Å–∫
./start.sh

# 4. –û—Ç–∫—Ä—ã—Ç—å –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
open http://localhost:8501
```

## üìã –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è

- Docker 20.10+
- Docker Compose 2.0+
- 2GB —Å–≤–æ–±–æ–¥–Ω–æ–≥–æ –º–µ—Å—Ç–∞ –Ω–∞ –¥–∏—Å–∫–µ
- –î–æ—Å—Ç—É–ø –∫ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç—É

## üèóÔ∏è –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞

```
news-aggregator/
‚îú‚îÄ‚îÄ docker-compose.yml      # –û—Ä–∫–µ—Å—Ç—Ä–∞—Ü–∏—è —Å–µ—Ä–≤–∏—Å–æ–≤
‚îú‚îÄ‚îÄ Dockerfile              # –û–±—Ä–∞–∑ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
‚îú‚îÄ‚îÄ init-db.sql             # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ë–î
‚îú‚îÄ‚îÄ requirements.txt        # Python –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
‚îú‚îÄ‚îÄ .env                    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è (–Ω–µ –∫–æ–º–º–∏—Ç–∏—Ç—å!)
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ app.py              # Streamlit –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
‚îÇ   ‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ database.py     # SQLAlchemy –º–æ–¥–µ–ª–∏
‚îÇ   ‚îî‚îÄ‚îÄ scrapers/
‚îÇ       ‚îú‚îÄ‚îÄ reddit_scraper.py
‚îÇ       ‚îî‚îÄ‚îÄ medium_scraper.py
‚îú‚îÄ‚îÄ docs/
‚îÇ   ‚îú‚îÄ‚îÄ DATABASE_ACCESS.md  # –†—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –ø–æ –ë–î
‚îÇ   ‚îî‚îÄ‚îÄ REDDIT_API_SETUP.md # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ Reddit API
‚îî‚îÄ‚îÄ tests/
    ‚îú‚îÄ‚îÄ unit/
    ‚îî‚îÄ‚îÄ integration/
```

## üîß –ù–∞—Å—Ç—Ä–æ–π–∫–∞ API

### Reddit API

1. –ü–µ—Ä–µ–π–¥–∏—Ç–µ: https://www.reddit.com/prefs/apps
2. –°–æ–∑–¥–∞–π—Ç–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ —Ç–∏–ø–∞ "script"
3. –°–∫–æ–ø–∏—Ä—É–π—Ç–µ `client_id` –∏ `client_secret`
4. –î–æ–±–∞–≤—å—Ç–µ –≤ `.env`:

```bash
REDDIT_CLIENT_ID=–≤–∞—à_client_id
REDDIT_CLIENT_SECRET=–≤–∞—à_client_secret
REDDIT_USER_AGENT=NewsAggregatorBot/1.0 by /u/–≤–∞—à_username
```

–ü–æ–¥—Ä–æ–±–Ω–µ–µ: [docs/REDDIT_API_SETUP.md](docs/REDDIT_API_SETUP.md)

### Telegram API (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)

1. –ü–µ—Ä–µ–π–¥–∏—Ç–µ: https://my.telegram.org/apps
2. –°–æ–∑–¥–∞–π—Ç–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ
3. –°–∫–æ–ø–∏—Ä—É–π—Ç–µ `api_id` –∏ `api_hash`

## üìä –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ

### –í–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)

```bash
./start.sh
open http://localhost:8501
```

–§—É–Ω–∫—Ü–∏–∏:
- ‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ Reddit subreddits
- ‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ Medium —Å—Ç–∞—Ç–µ–π
- ‚úÖ –ü—Ä–æ—Å–º–æ—Ç—Ä —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
- ‚úÖ –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
- üöß –ü–∞—Ä—Å–∏–Ω–≥ Telegram –∫–∞–Ω–∞–ª–æ–≤ (–≤ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ)

### –î–æ—Å—Ç—É–ø –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö

Adminer (–≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å):
```
URL: http://localhost:8080
System: PostgreSQL
Server: postgres
Username: newsaggregator
Password: changeme123
Database: news_aggregator
```

–ü–æ–¥—Ä–æ–±–Ω–µ–µ: [docs/DATABASE_ACCESS.md](docs/DATABASE_ACCESS.md)

## üê≥ Docker –∫–æ–º–∞–Ω–¥—ã

```bash
# –ó–∞–ø—É—Å–∫ –≤—Å–µ—Ö —Å–µ—Ä–≤–∏—Å–æ–≤
./start.sh
# –∏–ª–∏
docker compose up -d

# –û—Å—Ç–∞–Ω–æ–≤–∫–∞
./stop.sh
# –∏–ª–∏
docker compose down

# –ü—Ä–æ—Å–º–æ—Ç—Ä –ª–æ–≥–æ–≤
./logs.sh app
./logs.sh postgres
./logs.sh n8n

# –ü–µ—Ä–µ–∑–∞–ø—É—Å–∫ –æ–¥–Ω–æ–≥–æ —Å–µ—Ä–≤–∏—Å–∞
docker compose restart app

# –ü–µ—Ä–µ—Å–±–æ—Ä–∫–∞ –ø–æ—Å–ª–µ –∏–∑–º–µ–Ω–µ–Ω–∏–π
docker compose up --build

# –ü–æ–ª–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ (—É–¥–∞–ª–µ–Ω–∏–µ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö)
./clean.sh
```

## üì¶ –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã —Å–∏—Å—Ç–µ–º—ã

| –°–µ—Ä–≤–∏—Å | –ü–æ—Ä—Ç | –û–ø–∏—Å–∞–Ω–∏–µ |
|--------|------|----------|
| **Streamlit App** | 8501 | –í–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø–∞—Ä—Å–∏–Ω–≥–æ–º |
| **PostgreSQL** | 5432 | –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Å—Ç–∞—Ç–µ–π |
| **N8N** | 5678 | –ê–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è –∏ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–¥–∞—á |
| **Ollama** | 11434 | –õ–æ–∫–∞–ª—å–Ω–∞—è LLM –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–∫—Å—Ç–∞ |
| **Adminer** | 8080 | –í–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ë–î |

## üîê –ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å

‚ö†Ô∏è **–í–∞–∂–Ω–æ –¥–ª—è –ø—Ä–æ–¥–∞–∫—à–µ–Ω–∞:**

1. –ò–∑–º–µ–Ω–∏—Ç–µ –≤—Å–µ –ø–∞—Ä–æ–ª–∏ –≤ `.env`
2. –ù–µ –∫–æ–º–º–∏—Ç—å—Ç–µ `.env` –≤ Git
3. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ HTTPS –¥–ª—è –ø—É–±–ª–∏—á–Ω–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞
4. –û–≥—Ä–∞–Ω–∏—á—å—Ç–µ –¥–æ—Å—Ç—É–ø –∫ –ø–æ—Ä—Ç–∞–º —á–µ—Ä–µ–∑ firewall

## üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ

```bash
# –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–æ–≤
pytest tests/ -v

# –° –ø–æ–∫—Ä—ã—Ç–∏–µ–º
pytest tests/ --cov=src --cov-report=html
```

## üìà –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥

```bash
# –°—Ç–∞—Ç—É—Å –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–≤
docker compose ps

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ä–µ—Å—É—Ä—Å–æ–≤
docker stats

# –†–∞–∑–º–µ—Ä volumes
docker system df -v
```

## üêõ –†–µ—à–µ–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º

### Adminer –Ω–µ –æ—Ç–∫—Ä—ã–≤–∞–µ—Ç—Å—è

```bash
docker compose up -d adminer
docker logs news-aggregator-adminer
```

### Reddit API –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç 401

–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å `client_id` –∏ `client_secret` –≤ `.env`

### –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç—Å—è

```bash
docker compose down -v  # –£–¥–∞–ª–∏—Ç—å volumes
docker compose up -d    # –ü–µ—Ä–µ—Å–æ–∑–¥–∞—Ç—å
```

### –ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –ø–∞–¥–∞–µ—Ç —Å—Ä–∞–∑—É –ø–æ—Å–ª–µ —Å—Ç–∞—Ä—Ç–∞

```bash
docker compose logs app  # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –ª–æ–≥–∏
# –£–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ DATABASE_URL –Ω–∞—Å—Ç—Ä–æ–µ–Ω
```

## üìö –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è

- [–î–æ—Å—Ç—É–ø –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö](docs/DATABASE_ACCESS.md)
- [–ù–∞—Å—Ç—Ä–æ–π–∫–∞ Reddit API](docs/REDDIT_API_SETUP.md)

## ü§ù –í–∫–ª–∞–¥ –≤ –ø—Ä–æ–µ–∫—Ç

1. Fork —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è
2. –°–æ–∑–¥–∞–π—Ç–µ feature branch (`git checkout -b feature/amazing-feature`)
3. Commit –∏–∑–º–µ–Ω–µ–Ω–∏–π (`git commit -m 'Add amazing feature'`)
4. Push –≤ branch (`git push origin feature/amazing-feature`)
5. –û—Ç–∫—Ä–æ–π—Ç–µ Pull Request

## üìù –õ–∏—Ü–µ–Ω–∑–∏—è

MIT License - —Å–º. [LICENSE](LICENSE)

## üë• –ê–≤—Ç–æ—Ä—ã

- –í–∞—à–µ –∏–º—è - [@your_handle](https://github.com/your_handle)

## üôè –ë–ª–∞–≥–æ–¥–∞—Ä–Ω–æ—Å—Ç–∏

- [PRAW](https://praw.readthedocs.io/) - Reddit API wrapper
- [Streamlit](https://streamlit.io/) - –§—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞
- [SQLAlchemy](https://www.sqlalchemy.org/) - ORM –¥–ª—è Python
READMEEOF

log_success "README.md —Å–æ–∑–¥–∞–Ω"

# =============================================================================
# 14. Makefile
# =============================================================================
log_info "–°–æ–∑–¥–∞–Ω–∏–µ Makefile..."

cat > Makefile << 'MAKEFILEEOF'
.PHONY: help build up down restart logs clean test docs

help:
	@echo "News Aggregator - –ö–æ–º–∞–Ω–¥—ã Make"
	@echo ""
	@echo "make build     - –°–æ–±—Ä–∞—Ç—å Docker –æ–±—Ä–∞–∑—ã"
	@echo "make up        - –ó–∞–ø—É—Å—Ç–∏—Ç—å –≤—Å–µ —Å–µ—Ä–≤–∏—Å—ã"
	@echo "make down      - –û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –≤—Å–µ —Å–µ—Ä–≤–∏—Å—ã"
	@echo "make restart   - –ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç—å —Å–µ—Ä–≤–∏—Å—ã"
	@echo "make logs      - –ü–æ–∫–∞–∑–∞—Ç—å –ª–æ–≥–∏ (app)"
	@echo "make clean     - –£–¥–∞–ª–∏—Ç—å –≤—Å–µ –¥–∞–Ω–Ω—ã–µ –∏ volumes"
	@echo "make test      - –ó–∞–ø—É—Å—Ç–∏—Ç—å —Ç–µ—Å—Ç—ã"
	@echo "make docs      - –û—Ç–∫—Ä—ã—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é"

build:
	docker compose build --no-cache

up:
	./start.sh

down:
	./stop.sh

restart:
	docker compose restart

logs:
	docker compose logs -f app

clean:
	./clean.sh

test:
	pytest tests/ -v

docs:
	@echo "üìö –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è:"
	@echo "   DATABASE_ACCESS.md - docs/DATABASE_ACCESS.md"
	@echo "   REDDIT_API_SETUP.md - docs/REDDIT_API_SETUP.md"
MAKEFILEEOF

log_success "Makefile —Å–æ–∑–¥–∞–Ω"

# =============================================================================
# –°–æ–∑–¥–∞–Ω–∏–µ .env –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
# =============================================================================
if [ ! -f .env ]; then
    log_info "–°–æ–∑–¥–∞–Ω–∏–µ .env –∏–∑ .env.example..."
    cp .env.example .env
    log_warning ".env —Å–æ–∑–¥–∞–Ω - –Ω–µ –∑–∞–±—É–¥—å—Ç–µ –¥–æ–±–∞–≤–∏—Ç—å API –∫–ª—é—á–∏!"
fi

# =============================================================================
# –§–∏–Ω–∞–ª—å–Ω–∞—è —Å–≤–æ–¥–∫–∞
# =============================================================================
echo ""
echo "=========================================================================="
log_success "–ü–†–û–ï–ö–¢ –£–°–ü–ï–®–ù–û –°–û–ó–î–ê–ù!"
echo "=========================================================================="
echo ""
echo "üìÅ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞:"
echo "   $(find src -type f -name '*.py' 2>/dev/null | wc -l | tr -d ' ') Python —Ñ–∞–π–ª–æ–≤"
echo "   $(find tests -type f -name '*.py' 2>/dev/null | wc -l | tr -d ' ') —Ç–µ—Å—Ç–æ–≤"
echo "   $(find docs -type f 2>/dev/null | wc -l | tr -d ' ') –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"
echo ""
echo "‚úÖ –°–æ–∑–¥–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã:"
echo "   ‚Ä¢ docker-compose.yml"
echo "   ‚Ä¢ Dockerfile"
echo "   ‚Ä¢ .env (–Ω–∞—Å—Ç—Ä–æ–π—Ç–µ API –∫–ª—é—á–∏!)"
echo "   ‚Ä¢ init-db.sql"
echo "   ‚Ä¢ requirements.txt"
echo "   ‚Ä¢ src/app.py"
echo "   ‚Ä¢ src/models/database.py"
echo "   ‚Ä¢ src/scrapers/reddit_scraper.py"
echo "   ‚Ä¢ src/scrapers/medium_scraper.py"
echo "   ‚Ä¢ docs/DATABASE_ACCESS.md"
echo "   ‚Ä¢ docs/REDDIT_API_SETUP.md"
echo "   ‚Ä¢ README.md"
echo "   ‚Ä¢ Makefile"
echo "   ‚Ä¢ start.sh, stop.sh, logs.sh, clean.sh"
echo ""
echo "=========================================================================="
echo "üöÄ –°–õ–ï–î–£–Æ–©–ò–ï –®–ê–ì–ò:"
echo "=========================================================================="
echo ""
echo "1Ô∏è‚É£  –ù–∞—Å—Ç—Ä–æ–π—Ç–µ .env —Ñ–∞–π–ª:"
echo "    nano .env"
echo "    # –î–æ–±–∞–≤—å—Ç–µ REDDIT_CLIENT_ID –∏ REDDIT_CLIENT_SECRET"
echo "    # –°–º. docs/REDDIT_API_SETUP.md"
echo ""
echo "2Ô∏è‚É£  –ó–∞–ø—É—Å—Ç–∏—Ç–µ –ø—Ä–æ–µ–∫—Ç:"
echo "    ./start.sh"
echo ""
echo "3Ô∏è‚É£  –û—Ç–∫—Ä–æ–π—Ç–µ –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å—ã:"
echo "    ‚Ä¢ Streamlit:  http://localhost:8501"
echo "    ‚Ä¢ N8N:        http://localhost:5678 (admin/admin123)"
echo "    ‚Ä¢ Adminer:    http://localhost:8080 (newsaggregator/changeme123)"
echo "    ‚Ä¢ Ollama:     http://localhost:11434"
echo ""
echo "4Ô∏è‚É£  –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Å—Ç–∞—Ç—É—Å:"
echo "    docker compose ps"
echo "    ./logs.sh app"
echo ""
echo "=========================================================================="
log_success "–£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞! üéâ"
echo "=========================================================================="
echo ""
log_info "–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è:"
echo "   ‚Ä¢ –î–æ—Å—Ç—É–ø –∫ –ë–î: docs/DATABASE_ACCESS.md"
echo "   ‚Ä¢ –ù–∞—Å—Ç—Ä–æ–π–∫–∞ Reddit API: docs/REDDIT_API_SETUP.md"
echo "   ‚Ä¢ –û—Å–Ω–æ–≤–Ω–æ–π README: README.md"
echo ""