"""
HABR SCRAPER (SAFE MODE + RSS FALLBACK)
========================================

–¶–µ–ª—å:
    –°—Ç–∞–±–∏–ª—å–Ω—ã–π –∏ –±–µ–∑–æ–ø–∞—Å–Ω—ã–π —Å–±–æ—Ä —Å—Ç–∞—Ç–µ–π —Å –•–∞–±—Ä–∞.

–°—Ç—Ä–∞—Ç–µ–≥–∏—è:
    1. –û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥: HTML –ø–∞—Ä—Å–∏–Ω–≥ —á–µ—Ä–µ–∑ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã
    2. Fallback –º–µ—Ç–æ–¥: RSS –ø–∞—Ä—Å–∏–Ω–≥ (–ø–æ—Å–ª–µ–¥–Ω–∏–µ ~20 —Å—Ç–∞—Ç–µ–π)

–ü—Ä–∏–Ω—Ü–∏–ø—ã:
    ‚Ä¢ –û–±—Ö–æ–¥ —Ö–∞–±–æ–≤ —á–µ—Ä–µ–∑ /hub/ –∏ /hubs/ (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π fallback)
    ‚Ä¢ –†–æ—Ç–∞—Ü–∏—è User-Agent –∏ Referer
    ‚Ä¢ –ü–ª–∞–≤–Ω–∞—è –ø–∞–≥–∏–Ω–∞—Ü–∏—è (DOWNLOAD_DELAY + AUTOTHROTTLE)
    ‚Ä¢ RSS –∫–∞–∫ –∑–∞–ø–∞—Å–Ω–æ–π –≤–∞—Ä–∏–∞–Ω—Ç –ø—Ä–∏ –ø—Ä–æ–±–ª–µ–º–∞—Ö —Å HTML
    ‚Ä¢ –°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å deduplication_service –∏ editorial_service

–†–µ–∑—É–ª—å—Ç–∞—Ç:
    –°—Ç–∞—Ç—å–∏ –∏–∑–≤–ª–µ–∫–∞—é—Ç—Å—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ, –ø—Ä–∏ –±–ª–æ–∫–∏—Ä–æ–≤–∫–µ HTML - –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –Ω–∞ RSS.
"""

import os
import logging
import scrapy
from scrapy.http import Response, Request
from datetime import datetime
from urllib.parse import urljoin
import html
import sys
import random
import xml.etree.ElementTree as ET
from typing import Optional, List, Dict, Callable
from scrapy.crawler import CrawlerProcess

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from src.models.database import save_habr_article, HabrArticle
from src.utils.log_manager import get_log_manager

logger = logging.getLogger(__name__)

USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 12_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36",
    "Mozilla/5.0 (X11; Linux x86_64; rv:122.0) Gecko/20100101 Firefox/122.0",
]


class HabrArticleSpider(scrapy.Spider):
    """
    –ì–ª–∞–≤–Ω—ã–π spider –¥–ª—è –æ–±—Ö–æ–¥–∞ –∫—Ä—É–ø–Ω–µ–π—à–∏—Ö —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏—Ö —Ö–∞–±–æ–≤.
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç HTML –ø–∞—Ä—Å–∏–Ω–≥ + RSS fallback.
    """

    name = "habr_articles"
    allowed_domains = ["habr.com"]

    # –†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ö–∞–±—ã –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞
    RELEVANT_HUBS = [
        "artificial_intelligence", "machine_learning", "neural_networks",
        "deep_learning", "data_mining", "natural_language_processing",
        "computer_vision", "python", "programming", "backend",
        "devops", "docker", "kubernetes", "cloud_services",
    ]

    custom_settings = {
        "CONCURRENT_REQUESTS": 1,
        "DOWNLOAD_DELAY": 8,
        "RANDOMIZE_DOWNLOAD_DELAY": 4,
        "AUTOTHROTTLE_ENABLED": True,
        "AUTOTHROTTLE_START_DELAY": 8,
        "AUTOTHROTTLE_MAX_DELAY": 20,
        "COOKIES_ENABLED": True,
        "RETRY_TIMES": 4,
        "RETRY_HTTP_CODES": [429, 403, 500, 502, 503, 504],
        "LOG_LEVEL": "INFO",
    }

    def __init__(
        self,
        max_articles: int = 10,
        hubs: Optional[List[str]] = None,
        enable_llm: bool = True,
        enable_deduplication: bool = True,
        log_callback: Optional[Callable] = None,
        stats_dict: Optional[Dict] = None,
        use_rss_fallback: bool = True,
        *args,
        **kwargs
    ):
        super().__init__(*args, **kwargs)

        self.max_articles = max_articles
        self.hubs = hubs or self.RELEVANT_HUBS
        self.enable_llm = enable_llm
        self.enable_deduplication = enable_deduplication
        self.log_callback = log_callback
        self.use_rss_fallback = use_rss_fallback

        self.stats_dict = stats_dict or {
            "saved": 0,
            "skipped": 0,
            "semantic_duplicates": 0,
            "errors": 0,
            "blocked": 0,
            "rss_used": 0,
        }

        self.collected = 0
        self.html_failed_hubs = []  # –•–∞–±—ã –≥–¥–µ HTML –ø–∞—Ä—Å–∏–Ω–≥ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ç–æ—Ä–∞ (–µ—Å–ª–∏ –≤–∫–ª—é—á—ë–Ω)
        if enable_deduplication:
            try:
                from src.services.deduplication_service import get_deduplication_service
                self.dedup = get_deduplication_service()
                self.log_message("–î–µ–¥—É–ø–ª–∏–∫–∞—Ç–æ—Ä –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω", "INFO")
            except Exception as e:
                self.log_message(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–µ–¥—É–ø–ª–∏–∫–∞—Ç–æ—Ä–∞: {e}", "WARNING")
                self.dedup = None
                self.enable_deduplication = False
        else:
            self.dedup = None

        self.log_message(f"Spider –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω: max_articles={max_articles}, hubs={len(self.hubs)}", "INFO")
        self.log_message(f"LLM –æ–±—Ä–∞–±–æ—Ç–∫–∞: {'–í–ö–õ–Æ–ß–ï–ù–ê' if enable_llm else '–í–´–ö–õ–Æ–ß–ï–ù–ê'}", "INFO")

    def start(self):
        """
        –ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ start() –¥–ª—è Scrapy 2.13+
        """
        logger.info(f"–ó–∞–ø—É—Å–∫ –ø–∞—É–∫–∞ {self.name}")
        return super().start()

    # -----------------------------------------------------
    # –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–æ–¥—ã
    # -----------------------------------------------------
    def log_message(self, text: str, level="INFO"):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º callback –∏–ª–∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ –ª–æ–≥–≥–µ—Ä–∞."""
        if self.log_callback:
            try:
                self.log_callback(text, level)
            except:
                pass

        # –¢–∞–∫–∂–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –ª–æ–≥–≥–µ—Ä Scrapy
        log_level = getattr(logging, level.upper(), logging.INFO)
        self.logger.log(log_level, text)

    def headers(self, referer: Optional[str] = None):
        h = {"User-Agent": random.choice(USER_AGENTS)}
        if referer:
            h["Referer"] = referer
        return h

    # -----------------------------------------------------
    # –°—Ç–∞—Ä—Ç–æ–≤—ã–π –æ–±—Ö–æ–¥ ‚Äî HTML –ø–∞—Ä—Å–∏–Ω–≥, –∑–∞—Ç–µ–º RSS fallback
    # -----------------------------------------------------
    def start_requests(self):
        self.log_message(f"–ù–∞—á–∞–ª–æ –æ–±—Ö–æ–¥–∞ {len(self.hubs)} —Ö–∞–±–æ–≤", "INFO")

        for hub in self.hubs:
            url = f"https://habr.com/ru/hub/{hub}/articles/"
            self.log_message(f"üì° –û–±—Ö–æ–¥ —Ö–∞–±–∞ (HTML): {hub}", "INFO")
            yield Request(
                url,
                callback=self.parse_list,
                meta={"hub": hub, "path": "hub"},
                headers=self.headers(),
                errback=self.handle_error
            )

    def handle_error(self, failure):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫ –∑–∞–ø—Ä–æ—Å–æ–≤."""
        hub = failure.request.meta.get("hub")
        if hub:
            self.log_message(f"‚ö† –û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –¥–ª—è —Ö–∞–±–∞ {hub}: {failure}", "WARNING")
            self.html_failed_hubs.append(hub)

    # -----------------------------------------------------
    # –°–ø–∏—Å–æ–∫ —Å—Ç–∞—Ç–µ–π –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ —Ö–∞–±–∞ (HTML)
    # -----------------------------------------------------
    def parse_list(self, response: Response):
        """–ü–∞—Ä—Å–∏–Ω–≥ —Å–ø–∏—Å–∫–∞ —Å—Ç–∞—Ç–µ–π —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –ª–∏–º–∏—Ç–∞."""
        hub = response.meta["hub"]
        path = response.meta["path"]

        # –ü–†–û–í–ï–†–ö–ê –õ–ò–ú–ò–¢–ê –í –ù–ê–ß–ê–õ–ï
        if self.collected >= self.max_articles:
            self.log_message(f"‚úì –î–æ—Å—Ç–∏–≥–Ω—É—Ç –ª–∏–º–∏—Ç —Å—Ç–∞—Ç–µ–π: {self.max_articles}", "INFO")
            return

        self.log_message(f"–ü–∞—Ä—Å–∏–Ω–≥ —Å–ø–∏—Å–∫–∞ —Å—Ç–∞—Ç–µ–π –¥–ª—è —Ö–∞–±–∞: {hub}, —Å—Ç–∞—Ç—É—Å: {response.status}", "INFO")

        if response.status == 404 and path == "hub":
            alt = f"https://habr.com/ru/hubs/{hub}/articles/"
            self.log_message(f"Fallback /hubs/ –¥–ª—è —Ö–∞–±–∞: {hub}", "INFO")
            yield Request(alt, callback=self.parse_list, meta={"hub": hub, "path": "hubs"}, headers=self.headers())
            return

        link_selectors = [
            "article a[href*='/articles/']::attr(href)",
            "h2 a[href*='/articles/']::attr(href)",
            "a[href*='/articles/'][class*='title']::attr(href)",
            "a.tm-title__link::attr(href)",
            "a.tm-article-snippet__title-link::attr(href)",
            "h2.tm-title a::attr(href)",
        ]

        links = []
        for sel in link_selectors:
            found = response.css(sel).getall()
            if found:
                links.extend(found)

        article_links = []
        for link in links:
            if '/articles/' not in link and '/post/' not in link:
                continue
            if '/comments/' in link:
                continue
            if '#' in link:
                link = link.split('#')[0]
            if '/companies/' in link and link.endswith('/articles/'):
                continue
            import re
            if not re.search(r'/(?:articles|post)/\d+', link):
                continue

            full_url = response.urljoin(link)
            article_links.append(full_url)

        article_links = list(dict.fromkeys(article_links))

        # –û–ì–†–ê–ù–ò–ß–ò–í–ê–ï–ú –ö–û–õ–ò–ß–ï–°–¢–í–û –°–°–´–õ–û–ö
        remaining = self.max_articles - self.collected
        if remaining > 0:
            article_links = article_links[:remaining]

        self.log_message(f"üìä –ù–∞–π–¥–µ–Ω–æ {len(article_links)} —Å—Ç–∞—Ç–µ–π (–æ—Å—Ç–∞–ª–æ—Å—å —Å–æ–±—Ä–∞—Ç—å: {remaining})", "INFO")

        if not article_links:
            self.log_message(f"‚ö† HTML –ø–∞—Ä—Å–∏–Ω–≥ –Ω–µ –Ω–∞—à–µ–ª —Å—Ç–∞—Ç–µ–π –¥–ª—è —Ö–∞–±–∞ {hub}", "WARNING")

            if self.use_rss_fallback and hub not in self.html_failed_hubs:
                self.html_failed_hubs.append(hub)
                self.log_message(f"‚Üí –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –Ω–∞ RSS –¥–ª—è —Ö–∞–±–∞: {hub}", "INFO")
                rss_url = f"https://habr.com/ru/rss/hub/{hub}/articles/"
                yield Request(rss_url, callback=self.parse_rss, meta={"hub": hub}, headers=self.headers())
            return

        # –û–±—Ö–æ–¥ —Å—Ç–∞—Ç–µ–π
        for idx, url in enumerate(article_links, 1):
            if self.collected >= self.max_articles:
                self.log_message(f"‚úì –î–æ—Å—Ç–∏–≥–Ω—É—Ç –ª–∏–º–∏—Ç: {self.max_articles} —Å—Ç–∞—Ç–µ–π", "INFO")
                return

            self.log_message(f"‚Üí [{idx}/{len(article_links)}] {url}", "INFO")
            yield Request(url, callback=self.parse_article, headers=self.headers(response.url), meta={"retry": 0})

        # –ü–∞–≥–∏–Ω–∞—Ü–∏—è –¢–û–õ–¨–ö–û –µ—Å–ª–∏ –Ω–µ –¥–æ—Å—Ç–∏–≥–ª–∏ –ª–∏–º–∏—Ç–∞
        if self.collected < self.max_articles:
            next_selectors = [
                "a.tm-pagination__page--next::attr(href)",
                "a[rel='next']::attr(href)",
                "a[class*='next']::attr(href)",
            ]

            next_page = None
            for sel in next_selectors:
                next_page = response.css(sel).get()
                if next_page:
                    break

            if next_page:
                next_url = response.urljoin(next_page)
                self.log_message(f"‚Üí –ü–∞–≥–∏–Ω–∞—Ü–∏—è: {next_url} (–æ—Å—Ç–∞–ª–æ—Å—å: {self.max_articles - self.collected})", "INFO")
                yield Request(next_url, callback=self.parse_list, meta={"hub": hub, "path": path},
                              headers=self.headers(response.url))

    # -----------------------------------------------------
    # RSS –ø–∞—Ä—Å–∏–Ω–≥ (fallback –º–µ—Ç–æ–¥)
    # -----------------------------------------------------
    def parse_rss(self, response: Response):
        """–ü–∞—Ä—Å–∏–Ω–≥ RSS —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –ª–∏–º–∏—Ç–∞."""
        hub = response.meta["hub"]
        self.log_message(f"üì° RSS –ø–∞—Ä—Å–∏–Ω–≥ –¥–ª—è —Ö–∞–±–∞: {hub}", "INFO")

        # –ü–†–û–í–ï–†–ö–ê –õ–ò–ú–ò–¢–ê
        if self.collected >= self.max_articles:
            self.log_message(f"‚úì –î–æ—Å—Ç–∏–≥–Ω—É—Ç –ª–∏–º–∏—Ç —Å—Ç–∞—Ç–µ–π: {self.max_articles}", "INFO")
            return

        try:
            root = ET.fromstring(response.text)
            items = root.findall('.//item')

            # –û–ì–†–ê–ù–ò–ß–ò–í–ê–ï–ú –ö–û–õ–ò–ß–ï–°–¢–í–û –≠–õ–ï–ú–ï–ù–¢–û–í –ò–ó RSS
            remaining = self.max_articles - self.collected
            if remaining > 0:
                items = items[:remaining]

            self.log_message(f"üìä –ù–∞–π–¥–µ–Ω–æ {len(items)} —Å—Ç–∞—Ç–µ–π –≤ RSS (–æ—Å—Ç–∞–ª–æ—Å—å: {remaining})", "INFO")

            for idx, item in enumerate(items, 1):
                if self.collected >= self.max_articles:
                    self.log_message(f"‚úì –î–æ—Å—Ç–∏–≥–Ω—É—Ç –ª–∏–º–∏—Ç: {self.max_articles} —Å—Ç–∞—Ç–µ–π", "INFO")
                    break

                title = item.find('title').text if item.find('title') is not None else "–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞"
                link = item.find('link').text if item.find('link') is not None else None
                description = item.find('description').text if item.find('description') is not None else ""
                pub_date_str = item.find('pubDate').text if item.find('pubDate') is not None else None
                author_elem = item.find('.//{http://purl.org/dc/elements/1.1/}creator')
                author = author_elem.text if author_elem is not None else None

                if not link:
                    continue

                pub_date = None
                if pub_date_str:
                    try:
                        from email.utils import parsedate_to_datetime
                        pub_date = parsedate_to_datetime(pub_date_str)
                    except:
                        pub_date = datetime.utcnow()
                else:
                    pub_date = datetime.utcnow()

                self.log_message(f"‚Üí RSS [{idx}/{len(items)}] {title[:50]}...", "INFO")

                yield Request(
                    link,
                    callback=self.parse_article,
                    headers=self.headers(),
                    meta={
                        "retry": 0,
                        "from_rss": True,
                        "rss_title": title,
                        "rss_description": description,
                        "rss_author": author,
                        "rss_pub_date": pub_date,
                    }
                )

                self.stats_dict["rss_used"] += 1

        except ET.ParseError as e:
            self.log_message(f"‚úó –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ RSS XML: {e}", "ERROR")
        except Exception as e:
            self.log_message(f"‚úó –û—à–∏–±–∫–∞ RSS –ø–∞—Ä—Å–∏–Ω–≥–∞: {e}", "ERROR")
            logger.exception("RSS parsing error:")

    # -----------------------------------------------------
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏
    # -----------------------------------------------------
    def blocked(self, response: Response) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä–∫–∞ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏ - —Ç–æ–ª—å–∫–æ –¥–ª–∏–Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã.
        """
        page_size = len(response.text)

        # –ù–æ—Ä–º–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç—å—è –•–∞–±—Ä–∞ > 10KB
        if page_size < 10000:
            self.log_message(f"[BLOCKED] –ö–æ—Ä–æ—Ç–∫–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞: {page_size} –±–∞–π—Ç", "DEBUG")
            return True

        self.log_message(f"[OK] {page_size} –±–∞–π—Ç", "DEBUG")
        return False

    # -----------------------------------------------------
    # –ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç–∞—Ç—å–∏
    # -----------------------------------------------------
    def parse_article(self, response: Response):
        """–ü–∞—Ä—Å–∏–Ω–≥ –æ—Ç–¥–µ–ª—å–Ω–æ–π —Å—Ç–∞—Ç—å–∏ —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –ª–∏–º–∏—Ç–∞."""
        from_rss = response.meta.get("from_rss", False)

        # –ü–†–û–í–ï–†–ö–ê –õ–ò–ú–ò–¢–ê –í –ù–ê–ß–ê–õ–ï
        if self.collected >= self.max_articles:
            self.log_message(f"‚úì –î–æ—Å—Ç–∏–≥–Ω—É—Ç –ª–∏–º–∏—Ç —Å—Ç–∞—Ç–µ–π: {self.max_articles}", "INFO")
            return

        self.log_message(f"‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ", "INFO")
        self.log_message(f"[–ü–ê–†–°–ò–ù–ì] {response.url}", "INFO")
        self.log_message(f"[–†–ê–ó–ú–ï–†] {len(response.text)} —Å–∏–º–≤–æ–ª–æ–≤", "INFO")
        self.log_message(f"[RSS?] {from_rss}", "DEBUG")
        self.log_message(f"[COLLECTED] {self.collected}/{self.max_articles}", "DEBUG")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –±–ª–æ–∫–∏—Ä–æ–≤–∫—É
        if self.blocked(response):
            retry = response.meta.get("retry", 0)
            if retry >= 3:
                self.log_message(f"‚úó –ó–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω –ø–æ—Å–ª–µ {retry} –ø–æ–ø—ã—Ç–æ–∫", "WARNING")
                self.stats_dict["blocked"] += 1
                if from_rss:
                    self.save_from_rss_metadata(response)
                return

            self.log_message(f"‚ö† –í–æ–∑–º–æ–∂–Ω–∞—è –±–ª–æ–∫–∏—Ä–æ–≤–∫–∞, –ø–æ–≤—Ç–æ—Ä {retry + 1}/3", "WARNING")
            yield Request(
                response.url,
                callback=self.parse_article,
                headers=self.headers(),
                meta={**response.meta, "retry": retry + 1},
                dont_filter=True,
            )
            return

        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
        try:
            title = self.extract_title(response) or response.meta.get("rss_title", "–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞")
            self.log_message(f"[TITLE] {title[:60]}...", "INFO")

            content = self.extract_content(response)
            self.log_message(f"[CONTENT] {len(content)} —Å–∏–º–≤–æ–ª–æ–≤", "INFO")

            if (not content or len(content) < 150) and from_rss:
                content = response.meta.get("rss_description", "")
                self.log_message("‚Üí –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ –∏–∑ RSS –∫–∞–∫ –∫–æ–Ω—Ç–µ–Ω—Ç", "INFO")

            author = self.extract_author(response) or response.meta.get("rss_author")
            self.log_message(f"[AUTHOR] {author}", "DEBUG")

            published = self.extract_pub_date(response) or response.meta.get("rss_pub_date", datetime.utcnow())
            self.log_message(f"[DATE] {published}", "DEBUG")

            images = self.extract_images(response)
            self.log_message(f"[IMAGES] {len(images)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "DEBUG")

            url = response.url

            # –í–∞–ª–∏–¥–∞—Ü–∏—è –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏–Ω—ã –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            if not content or len(content) < 100:
                self.log_message(f"‚úó –ö–æ–Ω—Ç–µ–Ω—Ç —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π ({len(content)} —Å–∏–º–≤–æ–ª–æ–≤)", "WARNING")
                self.stats_dict["skipped"] += 1
                return

            # –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è
            if self.enable_deduplication and self.dedup:
                try:
                    self.log_message(f"[DEDUP] –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏...", "DEBUG")
                    is_dup, dup_id, score = self.dedup.check_duplicate(
                        text=f"{title}\n\n{content}",
                        source="habr"
                    )

                    if is_dup:
                        self.log_message(
                            f"‚úó –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –¥—É–±–ª–∏–∫–∞—Ç: {dup_id} (—Å—Ö–æ–∂–µ—Å—Ç—å: {score:.2%})",
                            "INFO"
                        )
                        self.stats_dict["semantic_duplicates"] += 1
                        return
                    else:
                        self.log_message(f"[‚úì] –ù–µ –¥—É–±–ª–∏–∫–∞—Ç", "DEBUG")

                except Exception as e:
                    self.log_message(f"‚ö† –û—à–∏–±–∫–∞ –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏: {e}", "WARNING")

            # LLM –æ–±—Ä–∞–±–æ—Ç–∫–∞ (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–∞)
            processed_title = title
            processed_content = content
            teaser = None
            image_prompt = None
            relevance_score = None

            if self.enable_llm:
                try:
                    self.log_message(f"[LLM] –†–µ–¥–∞–∫—Ç–æ—Ä—Å–∫–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞...", "INFO")
                    from src.services.editorial_service import get_editorial_service

                    editorial = get_editorial_service()
                    result = editorial.process_post(
                        title=title,
                        content=content,
                        source="habr",
                        default_relevant=True  # Habr –≤—Å–µ–≥–¥–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–µ–Ω –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
                    )

                    # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–ª–Ω–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ LLM
                    self.log_message(f"[LLM] –ü–æ–ª–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {result}", "DEBUG")

                    # –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –æ—à–∏–±–æ–∫
                    if result.get('error'):
                        self.log_message(
                            f"‚úó –û—à–∏–±–∫–∞ LLM: {result['error']}\n"
                            f"   –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –±–µ–∑ LLM –æ–±—Ä–∞–±–æ—Ç–∫–∏",
                            "WARNING"
                        )
                        # Fallback: —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ –µ—Å—Ç—å, –Ω–æ –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º
                        self.stats_dict["errors"] += 1
                        # –ù–ï –≤–æ–∑–≤—Ä–∞—â–∞–µ–º—Å—è - –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –±–µ–∑ LLM –¥–∞–Ω–Ω—ã—Ö

                    elif not result.get('is_news'):
                        # –°—Ç–∞—Ç—å—è –Ω–µ –ø—Ä–æ—à–ª–∞ —Ñ–∏–ª—å—Ç—Ä —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
                        self.log_message(
                            f"‚è≠ –ü–†–û–ü–£–°–ö: –°—Ç–∞—Ç—å—è –Ω–µ –ø–æ–¥—Ö–æ–¥–∏—Ç\n"
                            f"   –ü—Ä–∏—á–∏–Ω–∞: {result.get('relevance_reason', 'N/A')}\n"
                            f"   –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {result.get('relevance_score', 0):.2f}",
                            "INFO"
                        )
                        self.stats_dict["skipped"] += 1
                        return  # –¢–æ–ª—å–∫–æ –∑–¥–µ—Å—å –ø—Ä–µ—Ä—ã–≤–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É

                    else:
                        # –°—Ç–∞—Ç—å—è –ø—Ä–æ—à–ª–∞ - –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
                        processed_title = result.get('title') or title
                        processed_content = result.get('rewritten_post') or content
                        teaser = result.get('teaser')
                        image_prompt = result.get('image_prompt')
                        relevance_score = result.get('relevance_score', 0.0)

                        # –ü—Ä–æ–≤–µ—Ä–∫–∞, —á—Ç–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è –æ—Ç –æ—Ä–∏–≥–∏–Ω–∞–ª–∞
                        if processed_content != content:
                            self.log_message(f"[LLM] ‚úì –û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è –æ—Ç –æ—Ä–∏–≥–∏–Ω–∞–ª–∞", "INFO")
                            self.log_message(f"[LLM] –û—Ä–∏–≥–∏–Ω–∞–ª (–Ω–∞—á–∞–ª–æ): {content[:200]}...", "DEBUG")
                            self.log_message(f"[LLM] –û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π (–Ω–∞—á–∞–ª–æ): {processed_content[:200]}...", "DEBUG")
                        else:
                            self.log_message(f"[LLM] ‚ö†Ô∏è –û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç –∏–¥–µ–Ω—Ç–∏—á–µ–Ω –æ—Ä–∏–≥–∏–Ω–∞–ª—É!", "WARNING")

                        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å—á–µ—Ç—á–∏–∫–∞ –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç
                        if "editorial_processed" not in self.stats_dict:
                            self.stats_dict["editorial_processed"] = 0

                        self.stats_dict["editorial_processed"] += 1

                        self.log_message(f"[‚úì] LLM –æ–±—Ä–∞–±–æ—Ç–∫–∞ —É—Å–ø–µ—à–Ω–∞", "INFO")
                        self.log_message(f"   –ù–æ–≤—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫: {processed_title[:60]}...", "DEBUG")
                        self.log_message(f"   –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {relevance_score:.2f}", "DEBUG")

                except ImportError as e:
                    self.log_message(f"‚úó Editorial service –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}", "WARNING")
                    # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –±–µ–∑ LLM
                except Exception as e:
                    self.log_message(f"‚úó –û—à–∏–±–∫–∞ LLM –æ–±—Ä–∞–±–æ—Ç–∫–∏: {e}", "ERROR")
                    logger.exception("LLM processing error:")
                    self.stats_dict["errors"] += 1
                    # –ù–ï –ø—Ä–µ—Ä—ã–≤–∞–µ–º - –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –±–µ–∑ LLM

            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å—Ç–∞—Ç—å–∏
            try:
                self.log_message(f"[DB] –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ PostgreSQL...", "INFO")

                # –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê: –õ–æ–≥–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –ø–µ—Ä–µ–¥ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º
                self.log_message("=" * 80, "INFO")
                self.log_message("–î–ò–ê–ì–ù–û–°–¢–ò–ö–ê –ü–ï–†–ï–î save_habr_article:", "INFO")
                self.log_message(f"  original_title: {title[:100]}", "INFO")
                self.log_message(f"  processed_title: {processed_title[:100] if processed_title else 'NONE'}", "INFO")
                self.log_message(f"  original_content: {len(content)} —Å–∏–º–≤–æ–ª–æ–≤", "INFO")
                self.log_message(f"  processed_content: {len(processed_content) if processed_content else 0} —Å–∏–º–≤–æ–ª–æ–≤", "INFO")
                self.log_message(f"  teaser: {'–î–ê (' + str(len(teaser)) + ' —Å–∏–º.)' if teaser else 'NONE'}", "INFO")
                self.log_message(f"  image_prompt: {'–î–ê (' + str(len(image_prompt)) + ' —Å–∏–º.)' if image_prompt else 'NONE'}", "INFO")
                self.log_message(f"  relevance_score: {relevance_score}", "INFO")
                self.log_message(f"  title_changed: {processed_title != title}", "INFO")
                self.log_message(f"  content_changed: {processed_content != content}", "INFO")

                # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
                if processed_content != content:
                    self.log_message("  ‚úì –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç –†–ê–ó–õ–ò–ß–ê–Æ–¢–°–Ø", "INFO")
                    self.log_message(f"  –û—Ä–∏–≥–∏–Ω–∞–ª (–Ω–∞—á–∞–ª–æ): {content[:200]}...", "DEBUG")
                    self.log_message(f"  –û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π (–Ω–∞—á–∞–ª–æ): {processed_content[:200]}...", "DEBUG")
                else:
                    self.log_message("  ‚ö†Ô∏è –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç –ò–î–ï–ù–¢–ò–ß–ù–´!", "WARNING")

                self.log_message("=" * 80, "INFO")

                # –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
                save_kwargs = {
                    'is_news': True,  # –Ø–≤–Ω–æ —É–∫–∞–∑—ã–≤–∞–µ–º, —á—Ç–æ —ç—Ç–æ –Ω–æ–≤–æ—Å—Ç—å
                    'relevance_score': relevance_score,
                    'enable_llm': False,  # LLM —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–ª–∏ –≤—ã—à–µ
                }

                # –î–æ–±–∞–≤–ª—è–µ–º –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –ø–æ–ª—è –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å
                if processed_title != title:
                    save_kwargs['title'] = processed_title
                    self.log_message(f"[SCRAPER] –î–æ–±–∞–≤–ª–µ–Ω title: {processed_title[:50]}")

                if processed_content != content:
                    save_kwargs['rewritten_post'] = processed_content
                    self.log_message(f"[SCRAPER] –î–æ–±–∞–≤–ª–µ–Ω rewritten_post: {len(processed_content)} —Å–∏–º–≤–æ–ª–æ–≤")

                if teaser:
                    save_kwargs['teaser'] = teaser
                    self.log_message(f"[SCRAPER] –î–æ–±–∞–≤–ª–µ–Ω teaser: {teaser[:50]}")

                if image_prompt:
                    save_kwargs['image_prompt'] = image_prompt
                    self.log_message(f"[SCRAPER] –î–æ–±–∞–≤–ª–µ–Ω image_prompt: {image_prompt[:50]}")

                # –õ–æ–≥–∏—Ä—É–µ–º –∏—Ç–æ–≥–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
                self.log_message(f"[SCRAPER] –ò—Ç–æ–≥–æ save_kwargs: {list(save_kwargs.keys())}")

                saved = save_habr_article(
                    url=url,
                    title=processed_title,
                    content=processed_content,
                    author=author,
                    published_at=published,
                    images=images,
                    tags=self.hubs,
                    **save_kwargs
                )

                if saved:
                    self.stats_dict["saved"] += 1
                    self.collected += 1
                    source_type = "RSS" if from_rss else "HTML"
                    self.log_message(
                        f"‚úì‚úì‚úì –°–û–•–†–ê–ù–ï–ù–û [{self.collected}/{self.max_articles}] ({source_type}): {processed_title[:50]}...",
                        "INFO"
                    )

                    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ Qdrant (–µ—Å–ª–∏ –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –≤–∫–ª—é—á–µ–Ω–∞)
                    if self.enable_deduplication and self.dedup:
                        try:
                            self.log_message(f"[QDRANT] –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ embedding...", "DEBUG")
                            qdrant_text = f"{processed_title}\n\n{processed_content}"

                            # –ò–∑–≤–ª–µ–∫–∞–µ–º article_id –∏–∑ URL
                            import re
                            match = re.search(r'/(?:articles|post)/(\d+)', url)
                            article_id = match.group(1) if match else url

                            qdrant_id = self.dedup.save_to_qdrant(
                                text=qdrant_text,
                                record_id=article_id,
                                metadata={
                                    'title': processed_title,
                                    'url': url,
                                    'teaser': teaser or '',
                                    'author': author or '',
                                    'hubs': self.hubs,
                                    'relevance_score': relevance_score or 0.0
                                },
                                source="habr"
                            )

                            if qdrant_id:
                                self.log_message(f"[‚úì] –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ Qdrant: {qdrant_id[:8]}...", "DEBUG")
                        except Exception as e:
                            self.log_message(f"‚ö† –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ Qdrant: {e}", "WARNING")
                else:
                    self.log_message(f"‚äò –°—Ç–∞—Ç—å—è —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –≤ –ë–î: {processed_title[:50]}...", "INFO")
                    self.stats_dict["skipped"] += 1

            except Exception as e:
                self.log_message(f"‚úó –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {e}", "ERROR")
                self.stats_dict["errors"] += 1
                logger.exception(f"–î–µ—Ç–∞–ª–∏ –æ—à–∏–±–∫–∏ –¥–ª—è {url}:")

        except Exception as e:
            self.log_message(f"‚úó –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: {e}", "ERROR")
            self.stats_dict["errors"] += 1
            logger.exception(f"–î–µ—Ç–∞–ª–∏ –æ—à–∏–±–∫–∏ –¥–ª—è {response.url}:")

    def save_from_rss_metadata(self, response: Response):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å—Ç–∞—Ç—å–∏ –∏—Å–ø–æ–ª—å–∑—É—è —Ç–æ–ª—å–∫–æ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–∑ RSS."""
        try:
            title = response.meta.get("rss_title", "–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞")
            content = response.meta.get("rss_description", "–ö–æ–Ω—Ç–µ–Ω—Ç –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
            author = response.meta.get("rss_author")
            published = response.meta.get("rss_pub_date", datetime.utcnow())
            url = response.url

            self.log_message(f"[RSS] –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–∑ RSS –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö: {title[:50]}...", "INFO")

            # LLM –æ–±—Ä–∞–±–æ—Ç–∫–∞ (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–∞)
            processed_title = title
            processed_content = content
            teaser = None
            image_prompt = None
            relevance_score = None

            if self.enable_llm:
                try:
                    self.log_message(f"[LLM] –†–µ–¥–∞–∫—Ç–æ—Ä—Å–∫–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ RSS...", "INFO")
                    from src.services.editorial_service import get_editorial_service

                    editorial = get_editorial_service()
                    result = editorial.process_post(
                        title=title,
                        content=content,
                        source="habr",
                        default_relevant=True
                    )

                    if result.get('error'):
                        self.log_message(f"‚úó –û—à–∏–±–∫–∞ LLM (RSS): {result['error']}", "ERROR")
                        self.stats_dict["errors"] += 1
                        return

                    if not result.get('is_news'):
                        self.log_message(f"‚è≠ RSS —Å—Ç–∞—Ç—å—è –Ω–µ –ø–æ–¥—Ö–æ–¥–∏—Ç: {result.get('relevance_reason', 'N/A')}", "INFO")
                        self.stats_dict["skipped"] += 1
                        return

                    processed_title = result.get('title', title)
                    processed_content = result.get('rewritten_post', content)
                    teaser = result.get('teaser')
                    image_prompt = result.get('image_prompt')
                    relevance_score = result.get('relevance_score', 0.0)

                    self.stats_dict["editorial_processed"] += 1
                    self.log_message(f"[‚úì] LLM –æ–±—Ä–∞–±–æ—Ç–∫–∞ RSS —É—Å–ø–µ—à–Ω–∞", "INFO")

                except Exception as e:
                    self.log_message(f"‚úó –û—à–∏–±–∫–∞ LLM –æ–±—Ä–∞–±–æ—Ç–∫–∏ RSS: {e}", "ERROR")
                    self.stats_dict["errors"] += 1
                    return

            # –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
            save_kwargs = {
                'is_news': True,  # –Ø–≤–Ω–æ —É–∫–∞–∑—ã–≤–∞–µ–º, —á—Ç–æ —ç—Ç–æ –Ω–æ–≤–æ—Å—Ç—å
                'relevance_score': relevance_score,
                'enable_llm': False,  # LLM —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–ª–∏ –≤—ã—à–µ
            }

            # –î–æ–±–∞–≤–ª—è–µ–º –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –ø–æ–ª—è –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å
            if processed_title != title:
                save_kwargs['title'] = processed_title

            if processed_content != content:
                save_kwargs['rewritten_post'] = processed_content

            if teaser:
                save_kwargs['teaser'] = teaser

            if image_prompt:
                save_kwargs['image_prompt'] = image_prompt

            saved = save_habr_article(
                url=url,
                title=processed_title,
                content=processed_content,
                author=author,
                published_at=published,
                images=[],
                tags=self.hubs,
                **save_kwargs
            )

            if saved:
                self.stats_dict["saved"] += 1
                self.stats_dict["rss_used"] += 1
                self.collected += 1
                self.log_message(f"‚úì RSS –¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã", "INFO")

        except Exception as e:
            self.log_message(f"‚úó –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è RSS –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö: {e}", "ERROR")
            self.stats_dict["errors"] += 1
            logger.exception("RSS save error:")

    # -----------------------------------------------------
    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø–æ–ª–µ–π
    # -----------------------------------------------------
    def extract_title(self, response: Response):
        """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞."""
        selectors = [
            "h1 span[class*='title']::text",
            "h1[class*='title']::text",
            "h1 span::text",
            "h1::text",
            "meta[property='og:title']::attr(content)",
            "title::text",
        ]

        for sel in selectors:
            title = response.css(sel).get()
            if title:
                return title.strip()

        return None

    def extract_author(self, response: Response):
        """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∞–≤—Ç–æ—Ä–∞."""
        selectors = [
            "a[class*='user'] span::text",
            "a[class*='author']::text",
            "[class*='author'] a::text",
            "meta[name='author']::attr(content)",
        ]

        for sel in selectors:
            author = response.css(sel).get()
            if author:
                return author.strip()

        return None

    def extract_pub_date(self, response: Response):
        """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞—Ç—ã."""
        dt = response.css("time::attr(datetime)").get()
        if dt:
            try:
                return datetime.fromisoformat(dt.replace("Z", "+00:00"))
            except:
                pass

        dt = response.css("meta[property='article:published_time']::attr(content)").get()
        if dt:
            try:
                return datetime.fromisoformat(dt.replace("Z", "+00:00"))
            except:
                pass

        return None

    def extract_content(self, response: Response):
        """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –¥–ª—è –ª—é–±–æ–π –≤–µ—Ä—Å–∏–∏ –•–∞–±—Ä–∞."""
        content_selectors = [
            "article[id*='post']",
            "div[class*='article-formatted']",
            "div[id*='post-content']",
            "div.tm-article-body",
            "article.tm-article-presenter__body",
            "div.post__text",
            "div.content",
        ]

        blocks = None
        used_selector = None

        for sel in content_selectors:
            container = response.css(sel).get()
            if container:
                blocks = response.css(f"{sel} > *")
                used_selector = sel
                break

        if not blocks:
            blocks = response.css("article *")
            used_selector = "article *"

        if not blocks:
            return ""

        self.log_message(f"‚úì –ö–æ–Ω—Ç–µ–Ω—Ç –∏–∑–≤–ª–µ—á—ë–Ω —Å–µ–ª–µ–∫—Ç–æ—Ä–æ–º: {used_selector}", "DEBUG")

        result = []
        for block in blocks:
            if block.css("pre, code"):
                code_text = "".join(block.css("::text").getall())
                if code_text.strip():
                    result.append(f"<code>{html.escape(code_text.strip())}</code>")
            elif block.css("h1, h2, h3, h4, h5, h6"):
                header_text = "".join(block.css("::text").getall()).strip()
                if header_text:
                    result.append(f"\n{header_text}\n")
            else:
                text = "".join(block.css("::text").getall()).strip()
                if text and len(text) > 5:
                    result.append(text)

        content = "\n\n".join(result).strip()
        self.log_message(f"‚úì –ò–∑–≤–ª–µ—á–µ–Ω–æ {len(result)} –±–ª–æ–∫–æ–≤, {len(content)} —Å–∏–º–≤–æ–ª–æ–≤", "DEBUG")

        return content

    def extract_images(self, response: Response):
        """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π."""
        img_selectors = [
            "article img::attr(src)",
            "div[class*='article'] img::attr(src)",
            "img[class*='article']::attr(src)",
        ]

        images = []
        for sel in img_selectors:
            imgs = response.css(sel).getall()
            images.extend(imgs)

        result = []
        for img in images:
            if any(skip in img.lower() for skip in ["icon", "avatar", "emoji", "logo"]):
                continue

            if img.startswith("//"):
                img = "https:" + img
            elif img.startswith("/"):
                img = "https://habr.com" + img

            result.append(img)

        return list(dict.fromkeys(result))


def scrape_habr(
    max_articles: int = 10,
    hubs=None,
    enable_llm: bool = True,
    enable_deduplication: bool = True,
    debug: bool = False,
    log_callback=None,
    save_html: bool = False
):
    """
    –ì–ª–∞–≤–Ω–∞—è —Ç–æ—á–∫–∞ –∑–∞–ø—É—Å–∫–∞ –ø–∞—Ä—Å–µ—Ä–∞, –≤—ã–∑—ã–≤–∞–µ–º–∞—è run_habr_scraper.py
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É.
    """

    stats = {
        "success": False,
        "saved": 0,
        "skipped": 0,
        "semantic_duplicates": 0,
        "editorial_processed": 0,
        "errors": 0,
        "blocked": 0,
        "rss_used": 0,
        "total_attempts": 0,
    }

    scrapy_settings = {
        'ROBOTSTXT_OBEY': False,
        'LOG_ENABLED': debug,
        'LOG_LEVEL': 'DEBUG' if debug else 'INFO',
        'COOKIES_ENABLED': True,
        'REDIRECT_ENABLED': True,

        'CONCURRENT_REQUESTS': 1,
        'DOWNLOAD_DELAY': 8,
        'RANDOMIZE_DOWNLOAD_DELAY': 4,
        'RETRY_TIMES': 5,
        'RETRY_HTTP_CODES': [403, 429, 500, 502, 503, 504],

        'AUTOTHROTTLE_ENABLED': True,
        'AUTOTHROTTLE_START_DELAY': 8,
        'AUTOTHROTTLE_MAX_DELAY': 20,
        'AUTOTHROTTLE_TARGET_CONCURRENCY': 1,

        'USER_AGENT': random.choice(USER_AGENTS),
    }

    try:
        process = CrawlerProcess(settings=scrapy_settings)

        process.crawl(
            HabrArticleSpider,
            max_articles=max_articles,
            hubs=hubs,
            enable_llm=enable_llm,
            enable_deduplication=enable_deduplication,
            log_callback=log_callback,
            stats_dict=stats,
            debug=debug,
            save_html=save_html,
            use_rss_fallback=True,  # –í–∫–ª—é—á–µ–Ω RSS fallback
        )

        process.start()
        stats["success"] = True
        return stats

    except Exception as e:
        logger.exception("–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø–∞—Ä—Å–µ—Ä–∞:")
        stats["success"] = False
        stats["error"] = str(e)
        return stats