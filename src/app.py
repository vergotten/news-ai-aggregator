"""–í–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å Streamlit –¥–ª—è News Aggregator —Å –∂–∏–≤—ã–º–∏ –ª–æ–≥–∞–º–∏."""

import streamlit as st
import os
import sys
from pathlib import Path
import pandas as pd
import time
from datetime import datetime, timezone
from collections import deque
import json

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤ sys.path –¥–ª—è –∏–º–ø–æ—Ä—Ç–æ–≤
sys.path.insert(0, str(Path(__file__).parent.parent))

st.set_page_config(
    page_title="News Aggregator",
    page_icon="üì∞",
    layout="wide"
)

from src.utils.translations import TRANSLATIONS

def t(key: str, **kwargs) -> str:
    """–ü–µ—Ä–µ–≤–æ–¥ –∫–ª—é—á–∞ –Ω–∞ —Ç–µ–∫—É—â–∏–π —è–∑—ã–∫."""
    lang = st.session_state.get('language', 'ru')
    text = TRANSLATIONS.get(lang, TRANSLATIONS['ru']).get(key, key)
    if kwargs:
        return text.format(**kwargs)
    return text

# –ó–∞–≥—Ä—É–∑–∫–∞ CSS —Å—Ç–∏–ª–µ–π
css_path = Path(__file__).parent / "static" / "style.css"
if css_path.exists():
    with open(css_path) as f:
        st.markdown(f"<style>{f.read()}</style>", unsafe_allow_html=True)

DATABASE_URL = os.getenv("DATABASE_URL")
if not DATABASE_URL:
    st.error("DATABASE_URL –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω")
    st.stop()

try:
    from src.models.database import (
        init_db,
        get_stats_extended,
        get_posts_by_subreddit,
        get_processed_posts,
        get_processed_by_subreddit,
        get_medium_articles,
        get_session,
        RedditPost,
        ProcessedRedditPost,
        MediumArticle,
        TelegramMessage
    )
    from src.config_loader import get_config
    from src.services.editorial_service import EditorialService
    from src.scrapers.reddit_scraper import get_reddit_client, scrape_subreddit

    init_db()
    config = get_config()
except Exception as e:
    st.error(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏: {e}")
    st.stop()

# ============================================================================
# –ù–ê–°–¢–†–û–ô–ö–ò –ò–ó –ü–ï–†–ï–ú–ï–ù–ù–´–• –û–ö–†–£–ñ–ï–ù–ò–Ø
# ============================================================================
SETTINGS = {
    # PostgreSQL
    'postgres_user': os.getenv("POSTGRES_USER", "newsaggregator"),
    'postgres_password': os.getenv("POSTGRES_PASSWORD", "changeme123"),
    'postgres_db': os.getenv("POSTGRES_DB", "news_aggregator"),
    'postgres_port': int(os.getenv("POSTGRES_PORT", 5433)),

    # Reddit API
    'reddit_client_id': os.getenv("REDDIT_CLIENT_ID", "qpyZocAcLob0M8zbug8lrg"),
    'reddit_client_secret': os.getenv("REDDIT_CLIENT_SECRET", "wafZuaOqufek2lpBEaKlG3emDB1bkA"),
    'reddit_user_agent': os.getenv("REDDIT_USER_AGENT", "NewsAggregator/1.0"),

    # Telegram API
    'telegram_api_id': os.getenv("TELEGRAM_API_ID", "your_api_id"),
    'telegram_api_hash': os.getenv("TELEGRAM_API_HASH", "your_api_hash"),
    'telegram_phone': os.getenv("TELEGRAM_PHONE", "+1234567890"),

    # Qdrant
    'qdrant_port': int(os.getenv("QDRANT_PORT", 6333)),
    'qdrant_grpc_port': int(os.getenv("QDRANT_GRPC_PORT", 6334)),
    'qdrant_url': os.getenv("QDRANT_URL", "http://qdrant:6333"),

    # Ollama
    'ollama_port': int(os.getenv("OLLAMA_PORT", 11434)),
    'ollama_base_url': os.getenv("OLLAMA_BASE_URL", "http://ollama:11434"),

    # –ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ
    'app_port': int(os.getenv("APP_PORT", 8501)),
    'tz': os.getenv("TZ", "UTC"),
    'adminer_port': int(os.getenv("ADMINER_PORT", 8080)),

    # LLM Processing
    'max_parallel_tasks': int(os.getenv("MAX_PARALLEL_TASKS", 1)),

    # N8N
    'n8n_port': int(os.getenv("N8N_PORT", 5678)),
    'n8n_db': os.getenv("N8N_DB", "n8n"),
    'n8n_basic_auth_active': os.getenv("N8N_BASIC_AUTH_ACTIVE", "true").lower() == 'true',
    'n8n_basic_auth_user': os.getenv("N8N_BASIC_AUTH_USER", "admin"),
    'n8n_basic_auth_password': os.getenv("N8N_BASIC_AUTH_PASSWORD", "admin123"),

    # LLM
    'llm_provider': os.getenv("LLM_PROVIDER", "gpt-oss"),
    'llm_model': os.getenv("LLM_MODEL", "gpt-oss:20b"),
    'llm_temperature': float(os.getenv("LLM_TEMPERATURE", 0.7)),
    'llm_max_tokens': int(os.getenv("LLM_MAX_TOKENS", 8000)),
    'llm_top_p': float(os.getenv("LLM_TOP_P", 0.9)),
    'llm_base_url': os.getenv("LLM_BASE_URL", "http://localhost:5000"),

    # –ü–∞—Ä—Å–∏–Ω–≥
    'default_max_posts': int(os.getenv("DEFAULT_MAX_POSTS", 1)),
    'default_delay': int(os.getenv("DEFAULT_DELAY", 5)),
    'default_sort': os.getenv("DEFAULT_SORT", "hot"),
    'default_enable_llm': os.getenv("DEFAULT_ENABLE_LLM", "true").lower() == 'true',
    'batch_size': int(os.getenv("BATCH_SIZE", 10)),

    # –ö–∞—á–µ—Å—Ç–≤–æ
    'min_text_length': int(os.getenv("MIN_TEXT_LENGTH", 50)),
    'enable_semantic_dedup': os.getenv("ENABLE_SEMANTIC_DEDUP", "true").lower() == 'true',
    'enable_vectorization': os.getenv("ENABLE_VECTORIZATION", "true").lower() == 'true',

    # UI
    'logs_max_length': int(os.getenv("LOGS_MAX_LENGTH", 500)),
    'viewer_default_limit': int(os.getenv("VIEWER_DEFAULT_LIMIT", 100)),
    'show_debug_info': os.getenv("SHOW_DEBUG_INFO", "false").lower() == 'true'
}

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è session_state
if 'settings' not in st.session_state:
    st.session_state.settings = SETTINGS

if 'parsing_logs' not in st.session_state:
    st.session_state.parsing_logs = deque(maxlen=st.session_state.settings['logs_max_length'])

if 'parsing_active' not in st.session_state:
    st.session_state.parsing_active = False

if 'parsing_in_progress' not in st.session_state:
    st.session_state.parsing_in_progress = False

if 'parsing_results' not in st.session_state:
    st.session_state.parsing_results = None

if 'parsing_progress' not in st.session_state:
    st.session_state.parsing_progress = {'current': 0, 'total': 0, 'status': ''}

if 'language' not in st.session_state:
    st.session_state.language = 'ru'

if 'log_session_counter' not in st.session_state:
    st.session_state.log_session_counter = 0

if 'log_storage' not in st.session_state:
    st.session_state.log_storage = None

# ============================================================================
# –ö–õ–ê–°–°–´ –ò –§–£–ù–ö–¶–ò–ò
# ============================================================================

class StreamlitLogger:
    """–õ–æ–≥–≥–µ—Ä, –∑–∞–ø–∏—Å—ã–≤–∞—é—â–∏–π —Å–æ–æ–±—â–µ–Ω–∏—è –≤ session_state –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ UI."""

    @staticmethod
    def log(message: str, level: str = "INFO") -> None:
        """–î–æ–±–∞–≤–∏—Ç—å –ª–æ–≥-—Å–æ–æ–±—â–µ–Ω–∏–µ."""
        timestamp = datetime.now().strftime("%H:%M:%S")
        icon = {"INFO": "‚ÑπÔ∏è", "SUCCESS": "‚úÖ", "WARNING": "‚ö†Ô∏è", "ERROR": "‚ùå", "DEBUG": "üîç"}.get(level, "üìù")
        log_entry = f"{icon} `{timestamp}` {message}"
        st.session_state.parsing_logs.append(log_entry)

    @staticmethod
    def add_separator(session_number: int) -> None:
        """–î–æ–±–∞–≤–∏—Ç—å —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å –º–µ–∂–¥—É —Å–µ—Å—Å–∏—è–º–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞."""
        separator = f"\n{'='*80}\nüÜï **–ù–û–í–ê–Ø –°–ï–°–°–ò–Ø #{session_number}** - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n{'='*80}\n"
        st.session_state.parsing_logs.append(separator)

    @staticmethod
    def clear() -> None:
        """–û—á–∏—Å—Ç–∏—Ç—å –ª–æ–≥–∏."""
        st.session_state.parsing_logs.clear()
        st.session_state.log_session_counter = 0

def scrape_with_live_logs(subreddits: list[str], max_posts: int, sort_by: str, delay: int, enable_llm: bool) -> list[dict]:
    """–°–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ —Å –ª–æ–≥–∞–º–∏ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏."""
    logger = StreamlitLogger()
    st.session_state.log_session_counter += 1
    logger.add_separator(st.session_state.log_session_counter)

    progress_bar = st.progress(0)
    status_text = st.empty()
    log_container = st.expander("üìã **–õ–æ–≥–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞**", expanded=True)
    log_placeholder = log_container.empty()

    results = []
    total_subs = len(subreddits)
    settings = st.session_state.settings
    logger.log(f"üöÄ –ù–∞—á–∞–ª–æ –ø–∞—Ä—Å–∏–Ω–≥–∞: {total_subs} subreddits", "INFO")
    logger.log(f"–ù–∞—Å—Ç—Ä–æ–π–∫–∏: max_posts={max_posts}, sort={sort_by}, LLM={'ON' if enable_llm else 'OFF'}", "DEBUG")
    logger.log(f"LLM: {settings['llm_model']}, temp={settings['llm_temperature']}", "DEBUG")

    try:
        logger.log("–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Reddit API...", "INFO")
        reddit = get_reddit_client()
        logger.log("‚úì Reddit API –ø–æ–¥–∫–ª—é—á–µ–Ω", "SUCCESS")

        for idx, sub in enumerate(subreddits, 1):
            progress = idx / total_subs
            progress_bar.progress(progress)
            status_text.info(f"üî• –ü–∞—Ä—Å–∏–Ω–≥ **r/{sub}** ({idx}/{total_subs})")

            logger.log(f"{'='*60}", "DEBUG")
            logger.log(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ r/{sub} [{idx}/{total_subs}]", "INFO")
            log_placeholder.markdown("\n".join(st.session_state.parsing_logs))

            try:
                result = scrape_subreddit(
                    subreddit_name=sub,
                    max_posts=max_posts,
                    sort_by=sort_by,
                    enable_llm=enable_llm,
                    log_callback=lambda msg, lvl: logger.log(msg, lvl)
                )
                results.append(result)

                if result.get('success'):
                    saved = result.get('saved', 0)
                    skipped = result.get('skipped', 0)
                    semantic_dups = result.get('semantic_duplicates', 0)
                    editorial = result.get('editorial_processed', 0)
                    msg = f"r/{sub}: —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ {saved}, –ø—Ä–æ–ø—É—â–µ–Ω–æ {skipped}"
                    if semantic_dups > 0:
                        msg += f", –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ {semantic_dups}"
                    if enable_llm and editorial > 0:
                        msg += f", –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ LLM {editorial}"
                    logger.log(msg, "SUCCESS")
                else:
                    error = result.get('error', 'Unknown error')
                    logger.log(f"r/{sub}: –æ—à–∏–±–∫–∞ - {error}", "ERROR")
            except Exception as e:
                logger.log(f"r/{sub}: –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ - {str(e)}", "ERROR")
                results.append({'success': False, 'subreddit': sub, 'error': str(e)})

            log_placeholder.markdown("\n".join(st.session_state.parsing_logs))

            if idx < total_subs:
                logger.log(f"‚è≥ –û–∂–∏–¥–∞–Ω–∏–µ {delay} —Å–µ–∫...", "DEBUG")
                time.sleep(delay)

        logger.log(f"{'='*60}", "DEBUG")
        total_saved = sum(r.get('saved', 0) for r in results if r.get('success'))
        total_semantic = sum(r.get('semantic_duplicates', 0) for r in results if r.get('success'))
        total_editorial = sum(r.get('editorial_processed', 0) for r in results if r.get('success'))
        success_count = sum(1 for r in results if r.get('success'))

        logger.log(f"üéâ –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à—ë–Ω!", "SUCCESS")
        logger.log(f"–£—Å–ø–µ—à–Ω–æ: {success_count}/{total_subs} subreddits", "INFO")
        logger.log(f"–í—Å–µ–≥–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {total_saved} –ø–æ—Å—Ç–æ–≤", "SUCCESS")
        if total_semantic > 0:
            logger.log(f"–î—É–±–ª–∏–∫–∞—Ç–æ–≤ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ: {total_semantic}", "INFO")
        if enable_llm and total_editorial > 0:
            logger.log(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ LLM: {total_editorial}", "SUCCESS")

        progress_bar.progress(1.0)
        status_text.success(f"‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω–æ! –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {total_saved} –ø–æ—Å—Ç–æ–≤")
        log_placeholder.markdown("\n".join(st.session_state.parsing_logs))

        return results

    except Exception as e:
        logger.log(f"–ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: {str(e)}", "ERROR")
        status_text.error(f"‚ùå –û—à–∏–±–∫–∞: {str(e)}")
        log_placeholder.markdown("\n".join(st.session_state.parsing_logs))
        return []


def process_posts_with_live_logs(unprocessed_posts: list) -> dict:
    """–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø–æ—Å—Ç–æ–≤ —á–µ—Ä–µ–∑ LLM —Å –∂–∏–≤—ã–º–∏ –ª–æ–≥–∞–º–∏."""
    import concurrent.futures
    import threading
    from src.utils.thread_safe_logger import get_thread_safe_logger
    from src.models.database import is_post_processed

    # –ö–†–ò–¢–ò–ß–ù–û: –ö–æ–ø–∏—Ä—É–µ–º settings –î–û —Å–æ–∑–¥–∞–Ω–∏—è –ø–æ—Ç–æ–∫–æ–≤
    settings = dict(st.session_state.settings)

    # –ò—Å–ø–æ–ª—å–∑—É–µ–º thread-safe logger
    logger = get_thread_safe_logger()

    progress_bar = st.progress(0)
    status_text = st.empty()
    log_container = st.expander("üìã **–õ–æ–≥–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ LLM**", expanded=True)
    log_placeholder = log_container.empty()

    # –•—Ä–∞–Ω–∏–º –≤—Å–µ –ª–æ–≥–∏ –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
    all_logs = []

    processed_count = 0
    news_count = 0
    error_count = 0
    skipped_count = 0
    total = len(unprocessed_posts)

    # Thread-safe counters
    lock = threading.Lock()
    completed = [0]

    max_workers = settings['max_parallel_tasks']

    def add_log(message: str, level: str = "INFO"):
        """–î–æ–±–∞–≤–∏—Ç—å –ª–æ–≥ –∏ –æ–±–Ω–æ–≤–∏—Ç—å UI."""
        logger.log(message, level)
        timestamp = datetime.now().strftime("%H:%M:%S")
        icons = {
            "INFO": "‚ÑπÔ∏è",
            "SUCCESS": "‚úÖ",
            "WARNING": "‚ö†Ô∏è",
            "ERROR": "‚ùå",
            "DEBUG": "üîç"
        }
        icon = icons.get(level, "üìù")
        formatted = f"{icon} `{timestamp}` {message}"

        with lock:
            all_logs.append(formatted)
            log_placeholder.markdown("\n".join(all_logs[-100:]))

    add_log(f"ü§ñ –ù–∞—á–∞–ª–æ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏: {total} –ø–æ—Å—Ç–æ–≤", "INFO")
    add_log(f"–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –ø–æ—Ç–æ–∫–æ–≤: {max_workers}", "INFO")
    add_log(f"–ú–æ–¥–µ–ª—å: {settings['llm_model']}, temp={settings['llm_temperature']}", "INFO")

    def process_single_post(post, idx, settings_copy):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ –ø–æ—Å—Ç–∞ –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ."""
        nonlocal processed_count, news_count, error_count, skipped_count

        try:
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏
            if is_post_processed(post.post_id):
                with lock:
                    skipped_count += 1
                add_log(f"[{idx}/{total}] ‚≠ïÔ∏è –ü—Ä–æ–ø—É—â–µ–Ω (—É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω): {post.post_id}", "WARNING")
                return True

            full_text = f"{post.title}\n\n{post.selftext or ''}"
            text_length = len(full_text.strip())

            if text_length < settings_copy['min_text_length']:
                add_log(
                    f"[{idx}/{total}] ‚≠ïÔ∏è –ü—Ä–æ–ø—É—â–µ–Ω (—Ç–µ–∫—Å—Ç –∫–æ—Ä–æ—á–µ {settings_copy['min_text_length']} —Å–∏–º–≤–æ–ª–æ–≤)",
                    "WARNING"
                )
                with lock:
                    skipped_count += 1
                return True

            add_log(f"[{idx}/{total}] ü§ñ –û—Ç–ø—Ä–∞–≤–∫–∞ –≤ LLM: {post.title[:60]}...", "DEBUG")

            # –°–æ–∑–¥–∞–µ–º editorial service —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏ –∏–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞
            from src.services.editorial_service import EditorialService
            editorial = EditorialService(model=settings_copy['llm_model'])

            result = editorial.process_post(
                title=post.title,
                content=full_text,
                source='reddit'
            )

            if result.get('error'):
                with lock:
                    error_count += 1
                error_msg = result['error']
                add_log(f"[{idx}/{total}] ‚ùå –û—à–∏–±–∫–∞ LLM: {error_msg}", "ERROR")
                add_log(f"   ‚îî‚îÄ –ü–æ—Å—Ç: {post.title[:50]}...", "ERROR")
                return False

            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –ë–î
            session = get_session()
            try:
                # –î–≤–æ–π–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø–µ—Ä–µ–¥ –∑–∞–ø–∏—Å—å—é
                existing = session.query(ProcessedRedditPost).filter_by(post_id=post.post_id).first()
                if existing:
                    add_log(f"[{idx}/{total}] ‚≠ïÔ∏è –ü—Ä–æ–ø—É—â–µ–Ω (—É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω –¥—Ä—É–≥–∏–º –ø–æ—Ç–æ–∫–æ–º)", "WARNING")
                    with lock:
                        skipped_count += 1
                    return True

                processed_post = ProcessedRedditPost(
                    post_id=post.post_id,
                    original_title=post.title,
                    original_text=post.selftext or '',
                    subreddit=post.subreddit,
                    author=post.author,
                    url=post.url,
                    score=post.score,
                    is_news=result.get('is_news', False),
                    original_summary=result.get('original_summary'),
                    rewritten_post=result.get('rewritten_post'),
                    editorial_title=result.get('title'),
                    teaser=result.get('teaser'),
                    image_prompt=result.get('image_prompt'),
                    processed_at=datetime.utcnow(),
                    processing_time=int(result.get('processing_time', 0) * 1000),
                    model_used=editorial.model
                )

                session.add(processed_post)
                session.commit()

                with lock:
                    processed_count += 1
                    if result.get('is_news'):
                        news_count += 1
                        add_log(f"[{idx}/{total}] üì∞ –ù–û–í–û–°–¢–¨: {result.get('title', 'N/A')[:60]}...", "SUCCESS")
                    else:
                        add_log(f"[{idx}/{total}] üìÑ –ù–µ –Ω–æ–≤–æ—Å—Ç—å", "INFO")

                return True

            except Exception as db_error:
                error_msg = str(db_error)
                add_log(f"[{idx}/{total}] ‚ùå –û—à–∏–±–∫–∞ –ë–î: {error_msg}", "ERROR")
                add_log(f"   ‚îî‚îÄ –ü–æ—Å—Ç ID: {post.post_id}", "ERROR")
                session.rollback()
                with lock:
                    error_count += 1
                return False
            finally:
                session.close()

        except Exception as e:
            with lock:
                error_count += 1

            error_msg = str(e)
            error_type = type(e).__name__

            # –û—Å–Ω–æ–≤–Ω–∞—è –æ—à–∏–±–∫–∞
            add_log(f"[{idx}/{total}] ‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {error_type}", "ERROR")
            add_log(f"   ‚îî‚îÄ –°–æ–æ–±—â–µ–Ω–∏–µ: {error_msg}", "ERROR")

            # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø–æ—Å—Ç–µ
            try:
                post_title = post.title[:50] if hasattr(post, 'title') else 'Unknown'
                add_log(f"   ‚îî‚îÄ –ü–æ—Å—Ç: {post_title}...", "ERROR")
            except:
                add_log(f"   ‚îî‚îÄ –ü–æ—Å—Ç: [–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å]", "ERROR")

            # Traceback - –ø–æ—Å—Ç—Ä–æ—á–Ω–æ
            import traceback
            tb_lines = traceback.format_exc().split('\n')
            add_log(f"   ‚îî‚îÄ Traceback:", "ERROR")
            for line in tb_lines:
                if line.strip():
                    add_log(f"      {line}", "ERROR")

            return False

    # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ
    try:
        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = {
                executor.submit(process_single_post, post, idx, settings): idx
                for idx, post in enumerate(unprocessed_posts, 1)
            }

            for future in concurrent.futures.as_completed(futures):
                idx = futures[future]

                with lock:
                    completed[0] += 1
                    progress = completed[0] / total

                progress_bar.progress(progress)
                status_text.info(f"üìÑ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {completed[0]}/{total} –ø–æ—Å—Ç–æ–≤")

                try:
                    future.result()
                except Exception as e:
                    add_log(f"‚ùå –ù–µ–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –≤ –ø–æ—Ç–æ–∫–µ #{idx}: {str(e)}", "ERROR")

        add_log(f"{'=' * 60}", "DEBUG")
        add_log(f"üéâ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!", "SUCCESS")
        add_log(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {processed_count}", "SUCCESS")
        add_log(f"–ù–æ–≤–æ—Å—Ç–µ–π: {news_count}", "INFO")
        add_log(f"–ü—Ä–æ–ø—É—â–µ–Ω–æ: {skipped_count}", "WARNING")
        add_log(f"–û—à–∏–±–æ–∫: {error_count}", "ERROR" if error_count > 0 else "INFO")

        progress_bar.progress(1.0)
        status_text.success(
            f"‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω–æ! –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {processed_count}, –ù–æ–≤–æ—Å—Ç–µ–π: {news_count}, –ü—Ä–æ–ø—É—â–µ–Ω–æ: {skipped_count}")

        return {
            'processed': processed_count,
            'news': news_count,
            'skipped': skipped_count,
            'errors': error_count
        }

    except Exception as e:
        add_log(f"–ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê EXECUTOR: {str(e)}", "ERROR")

        import traceback
        tb_lines = traceback.format_exc().split('\n')
        add_log(f"–ü–æ–ª–Ω—ã–π traceback:", "ERROR")
        for line in tb_lines:
            if line.strip():
                add_log(f"  {line}", "ERROR")

        status_text.error(f"‚ùå –û—à–∏–±–∫–∞: {str(e)}")

        return {
            'processed': processed_count,
            'news': news_count,
            'skipped': skipped_count,
            'errors': error_count
        }

def format_timedelta(td, lang='ru') -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞."""
    total_seconds = int(td.total_seconds())
    if total_seconds < 60:
        return f"{total_seconds} {t('sec')} {t('ago')}"
    elif total_seconds < 3600:
        return f"{total_seconds // 60} {t('min')} {t('ago')}"
    elif total_seconds < 86400:
        return f"{total_seconds // 3600} {t('hour')} {t('ago')}"
    else:
        days = total_seconds // 86400
        return f"{days} {t('days')} {t('ago')}"

def count_words(text: str) -> int:
    """–ü–æ–¥—Å—á–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å–ª–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ."""
    if not text:
        return 0
    return len(text.split())

def render_raw_post_viewer(post, lang='ru') -> None:
    """–†–µ–Ω–¥–µ—Ä–∏–Ω–≥ —Å—ã—Ä–æ–≥–æ Reddit –ø–æ—Å—Ç–∞ —Å –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π."""
    has_vector = post.qdrant_id is not None
    vector_badge = t('vectorized') if has_vector else t('no_vector')

    now = datetime.now(timezone.utc)
    created_time = post.created_utc.replace(tzinfo=timezone.utc) if post.created_utc.tzinfo is None else post.created_utc
    scraped_time = post.scraped_at.replace(tzinfo=timezone.utc) if post.scraped_at.tzinfo is None else post.scraped_at

    time_since_created = now - created_time
    time_since_scraped = now - scraped_time

    with st.expander(f"r/{post.subreddit} ‚Ä¢ {post.title[:80]}"):
        col_badge1, col_badge2 = st.columns([1, 1], gap="small")
        with col_badge1:
            st.caption(vector_badge)
        with col_badge2:
            if has_vector:
                st.caption(f"`{str(post.qdrant_id)[:8]}...`")

        st.markdown("---")

        col_time1, col_time2 = st.columns(2, gap="small")
        with col_time1:
            st.markdown(f"**{t('published_reddit')}**")
            st.info(f"{post.created_utc.strftime('%Y-%m-%d %H:%M:%S UTC')}")
            st.caption(format_timedelta(time_since_created, lang))
        with col_time2:
            st.markdown(f"**{t('received_db')}**")
            st.success(f"{post.scraped_at.strftime('%Y-%m-%d %H:%M:%S UTC')}")
            st.caption(format_timedelta(time_since_scraped, lang))

        st.markdown("---")

        col_a, col_b = st.columns([2, 1], gap="small")

        with col_a:
            st.markdown(f"**{t('original_title')}**")
            st.write(post.title)

            st.markdown(f"**{t('original_text')}**")
            if post.selftext:
                st.text_area(
                    t('post_text'),
                    post.selftext,
                    height=200,
                    key=f"raw_{post.id}",
                    label_visibility="collapsed"
                )
            else:
                st.caption(f"_{t('text_missing')}_")

        with col_b:
            st.metric(t('score'), post.score)
            st.metric(t('comments'), post.num_comments)
            st.caption(f"**{t('author')}** u/{post.author}")

            if has_vector:
                st.success(t('in_qdrant'))
                with st.expander(t('qdrant_uuid')):
                    st.code(str(post.qdrant_id))
            else:
                st.warning(t('no_vector'))

            if post.url:
                st.link_button(t('open_original'), post.url)


def render_processed_post_viewer(post, raw_post, lang='ru') -> None:
    """–†–µ–Ω–¥–µ—Ä–∏–Ω–≥ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–≥–æ –ø–æ—Å—Ç–∞ —Å –≤–∫–ª–∞–¥–∫–∞–º–∏."""
    status_icon = "üì∞" if post.is_news else "‚ùå"

    # –ò—Å–ø–æ–ª—å–∑—É–µ–º teaser –∫–∞–∫ –æ—Å–Ω–æ–≤–Ω–æ–π –∑–∞–≥–æ–ª–æ–≤–æ–∫, –µ—Å–ª–∏ –µ—Å—Ç—å
    if post.is_news and post.teaser:
        title_display = post.teaser[:80]
    elif post.editorial_title:
        title_display = post.editorial_title[:80]
    else:
        title_display = post.original_title[:80]

    has_vector = raw_post and raw_post.qdrant_id is not None

    now = datetime.now(timezone.utc)

    with st.expander(f"{status_icon} r/{post.subreddit} ‚Ä¢ {title_display}"):
        # Badges - –∫–æ–º–ø–∞–∫—Ç–Ω–µ–µ
        col_badge1, col_badge2, col_badge3, col_badge4 = st.columns([1, 1, 1, 1], gap="small")
        with col_badge1:
            if post.is_news:
                st.success("‚úÖ News")
            else:
                st.error("‚ùå Not News")
        with col_badge2:
            if has_vector:
                st.info("ü§ñ Vector")
            else:
                st.warning("‚ö†Ô∏è No Vec")
        with col_badge3:
            st.caption(f"‚ö° {post.processing_time}ms")
        with col_badge4:
            st.caption(f"ü§ñ {post.model_used or 'gpt-oss'}")

        st.markdown("---")

        # Timeline - –∫–æ–º–ø–∞–∫—Ç–Ω–µ–µ
        if raw_post:
            col_timeline1, col_timeline2, col_timeline3 = st.columns(3, gap="small")
            created_time = raw_post.created_utc.replace(
                tzinfo=timezone.utc) if raw_post.created_utc.tzinfo is None else raw_post.created_utc
            scraped_time = raw_post.scraped_at.replace(
                tzinfo=timezone.utc) if raw_post.scraped_at.tzinfo is None else raw_post.scraped_at
            processed_time = post.processed_at.replace(
                tzinfo=timezone.utc) if post.processed_at.tzinfo is None else post.processed_at

            with col_timeline1:
                st.markdown("**üìÖ –û–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ**")
                st.info(f"{raw_post.created_utc.strftime('%Y-%m-%d %H:%M')}")
                st.caption(format_timedelta(now - created_time, lang))
            with col_timeline2:
                st.markdown("**üíæ –ü–æ–ª—É—á–µ–Ω–æ**")
                st.success(f"{raw_post.scraped_at.strftime('%Y-%m-%d %H:%M')}")
                st.caption(format_timedelta(now - scraped_time, lang))
            with col_timeline3:
                st.markdown("**ü§ñ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ**")
                st.warning(f"{post.processed_at.strftime('%Y-%m-%d %H:%M')}")
                st.caption(format_timedelta(now - processed_time, lang))

        st.markdown("---")

        if post.is_news:
            tab_original, tab_llm, tab_meta = st.tabs(["üìÑ –û—Ä–∏–≥–∏–Ω–∞–ª", "ü§ñ LLM Output", "üìä –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ"])

            with tab_original:
                st.markdown("### üìå –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫")
                st.info(post.original_title)

                st.markdown("### üìù –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç")
                if post.original_text:
                    full_original = f"{post.original_title}\n\n{post.original_text}"
                    st.text_area(
                        "–ü–æ–ª–Ω—ã–π –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç",
                        full_original,
                        height=400,
                        key=f"orig_full_{post.id}",
                        label_visibility="collapsed"
                    )
                    st.caption(f"üìè –î–ª–∏–Ω–∞: {len(full_original)} —Å–∏–º–≤–æ–ª–æ–≤ | –°–ª–æ–≤: {count_words(full_original)}")
                else:
                    st.caption("_–¢–µ–∫—Å—Ç –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç_")

            with tab_llm:
                # Teaser –∫–∞–∫ –æ—Å–Ω–æ–≤–Ω–æ–π –∑–∞–≥–æ–ª–æ–≤–æ–∫
                st.markdown("### ‚ú® –ó–∞–≥–æ–ª–æ–≤–æ–∫ (Teaser)")
                if post.teaser:
                    st.success(f"**{post.teaser}**")
                else:
                    st.caption("_–ù–µ —Å–æ–∑–¥–∞–Ω_")

                st.markdown("### üì∞ –†–µ–¥–∞–∫—Ç–æ—Ä—Å–∫–∏–π –∑–∞–≥–æ–ª–æ–≤–æ–∫")
                if post.editorial_title:
                    st.info(post.editorial_title)
                else:
                    st.caption("_–ù–µ —Å–æ–∑–¥–∞–Ω_")

                st.markdown("### ‚úçÔ∏è –ü–µ—Ä–µ–ø–∏—Å–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç (LLM Output)")
                if post.rewritten_post:
                    st.text_area(
                        "–ü–æ–ª–Ω—ã–π –ø–µ—Ä–µ–ø–∏—Å–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –æ—Ç LLM",
                        post.rewritten_post,
                        height=400,
                        key=f"llm_full_{post.id}",
                        label_visibility="collapsed"
                    )

                    # –î–æ–±–∞–≤–ª–µ–Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Å–ª–æ–≤–∞–º
                    original_text = f"{post.original_title}\n\n{post.original_text or ''}"
                    original_len = len(original_text)
                    original_words = count_words(original_text)
                    llm_len = len(post.rewritten_post)
                    llm_words = count_words(post.rewritten_post)

                    diff_chars = llm_len - original_len
                    diff_words = llm_words - original_words
                    diff_pct = (diff_chars / original_len * 100) if original_len > 0 else 0

                    col_stat1, col_stat2, col_stat3, col_stat4 = st.columns(4, gap="small")
                    with col_stat1:
                        st.metric("–û—Ä–∏–≥–∏–Ω–∞–ª", f"{original_words} —Å–ª–æ–≤")
                        st.caption(f"{original_len} —Å–∏–º–≤–æ–ª–æ–≤")
                    with col_stat2:
                        st.metric("LLM Output", f"{llm_words} —Å–ª–æ–≤")
                        st.caption(f"{llm_len} —Å–∏–º–≤–æ–ª–æ–≤")
                    with col_stat3:
                        st.metric("Œî –°–ª–æ–≤", f"{diff_words:+d}")
                    with col_stat4:
                        st.metric("Œî –°–∏–º–≤–æ–ª–æ–≤", f"{diff_chars:+d} ({diff_pct:+.1f}%)")
                else:
                    st.warning("_–ü–µ—Ä–µ–ø–∏—Å–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –Ω–µ —Å–æ–∑–¥–∞–Ω_")

                # –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω –ø—Ä–æ–º–ø—Ç
                st.markdown("### üé® –ü—Ä–æ–º–ø—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è")
                if post.image_prompt:
                    st.code(post.image_prompt, language="text")
                else:
                    st.caption("_–ù–µ —Å–æ–∑–¥–∞–Ω_")

                st.markdown("### üìã –ö—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ (Summary)")
                if post.original_summary:
                    with st.expander("–ü–æ–∫–∞–∑–∞—Ç—å summary"):
                        st.write(post.original_summary)
                else:
                    st.caption("_–ù–µ —Å–æ–∑–¥–∞–Ω–æ_")

            with tab_meta:
                st.markdown("### üìä –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏")

                # –£–º–µ–Ω—å—à–µ–Ω gap
                col_m1, col_m2 = st.columns(2, gap="small")

                with col_m1:
                    st.markdown("**ü§ñ –ú–æ–¥–µ–ª—å**")
                    st.info(post.model_used or 'gpt-oss')

                    st.markdown("**‚ö° –í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏**")
                    st.info(f"{post.processing_time}ms ({post.processing_time / 1000:.2f}s)")

                    st.markdown("**üìÖ –î–∞—Ç–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏**")
                    st.info(post.processed_at.strftime('%Y-%m-%d %H:%M:%S UTC'))

                with col_m2:
                    st.markdown("**üì∞ –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è**")
                    if post.is_news:
                        st.success("‚úÖ –ù–æ–≤–æ—Å—Ç—å")
                    else:
                        st.error("‚ùå –ù–µ –Ω–æ–≤–æ—Å—Ç—å")

                    st.markdown("**üéØ –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è**")
                    if has_vector:
                        st.success("‚úÖ –í Qdrant")
                        if raw_post:
                            st.code(str(raw_post.qdrant_id), language="text")
                    else:
                        st.warning("‚ö†Ô∏è –ù–µ –≤–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω")

                    st.markdown("**‚¨ÜÔ∏è Score**")
                    st.info(f"{post.score} upvotes")

            st.markdown("---")
            if post.url:
                st.link_button("üîó –û—Ç–∫—Ä—ã—Ç—å –æ—Ä–∏–≥–∏–Ω–∞–ª –Ω–∞ Reddit", post.url)
        else:
            st.warning("**‚ùå –ù–µ —è–≤–ª—è–µ—Ç—Å—è –Ω–æ–≤–æ—Å—Ç—å—é**")
            st.caption(f"**–û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫:** {post.original_title}")
            st.caption(f"**Subreddit:** r/{post.subreddit}")
            st.caption(f"**–ê–≤—Ç–æ—Ä:** u/{post.author}")

            if post.original_text:
                with st.expander("üìÑ –ü–æ–∫–∞–∑–∞—Ç—å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç"):
                    st.text_area(
                        "–û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç",
                        post.original_text,
                        height=200,
                        key=f"not_news_{post.id}",
                        label_visibility="collapsed"
                    )


def render_settings_section(title: str, settings_dict: dict, icon: str = "‚öôÔ∏è"):
    """
    –†–µ–Ω–¥–µ—Ä–∏–Ω–≥ —Å–µ–∫—Ü–∏–∏ –Ω–∞—Å—Ç—Ä–æ–µ–∫ —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏.

    Args:
        title: –ó–∞–≥–æ–ª–æ–≤–æ–∫ —Å–µ–∫—Ü–∏–∏
        settings_dict: –°–ª–æ–≤–∞—Ä—å –Ω–∞—Å—Ç—Ä–æ–µ–∫ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
        icon: –≠–º–æ–¥–∑–∏ –∏–∫–æ–Ω–∫–∞
    """
    with st.expander(f"{icon} {title}", expanded=False):
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –∫–æ–ª–æ–Ω–∫–∏ –ø–æ 2
        items = list(settings_dict.items())
        cols_per_row = 2

        for i in range(0, len(items), cols_per_row):
            cols = st.columns(cols_per_row)
            for j, col in enumerate(cols):
                if i + j < len(items):
                    key, value = items[i + j]
                    with col:
                        # –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–ª—é—á–∞ (—á–µ–ª–æ–≤–µ–∫–æ—á–∏—Ç–∞–µ–º–æ)
                        display_key = key.replace('_', ' ').title()

                        # –ú–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–∞—Ä–æ–ª–µ–π –∏ —Å–µ–∫—Ä–µ—Ç–æ–≤
                        if 'password' in key.lower() or 'secret' in key.lower():
                            display_value = "***" if value else "Not set"
                        else:
                            display_value = str(value)

                        st.metric(display_key, display_value)


# === HEADER ===
col_title, col_spacer, col_lang = st.columns([3, 0.5, 1])

with col_title:
    st.title(t('title'))
    st.caption(t('subtitle'))

with col_lang:
    col_ru, col_en = st.columns(2)
    with col_ru:
        if st.button("üá∑üá∫ RU", key="lang_ru", use_container_width=True,
                     type="primary" if st.session_state.language == 'ru' else "secondary"):
            st.session_state.language = 'ru'
            st.rerun()
    with col_en:
        if st.button("üá¨üáß EN", key="lang_en", use_container_width=True,
                     type="primary" if st.session_state.language == 'en' else "secondary"):
            st.session_state.language = 'en'
            st.rerun()

# === API STATUS ===
col1, col2, col3 = st.columns(3)
with col1:
    if os.getenv("REDDIT_CLIENT_ID"):
        st.success(t('reddit_api'))
    else:
        st.warning(t('reddit_api'))
with col2:
    if os.getenv("TELEGRAM_API_ID"):
        st.success(t('telegram_api'))
    else:
        st.warning(t('telegram_api'))
with col3:
    st.success(t('database'))

st.markdown("---")

try:
    stats = get_stats_extended()
except:
    stats = {
        'reddit_posts': 0,
        'telegram_messages': 0,
        'medium_articles': 0,
        'latest_reddit': None,
        'latest_telegram': None,
        'latest_medium': None
    }

# === TABS ===
tab1, tab2, tab3, tab4, tab5, tab6 = st.tabs([
    t('reddit_tab'),
    t('telegram_tab'),
    t('medium_tab'),
    f"üìä {t('data_viewer_tab')}",
    t('analytics_tab'),
    f"‚öôÔ∏è {t('settings_tab')}"
])

# === TAB 1: REDDIT PARSER ===
with tab1:
    st.markdown('<div class="reddit-section">', unsafe_allow_html=True)
    st.header(f"{t('reddit_tab')} Parser")

    col1, col2 = st.columns([2, 1])

    with col1:
        st.subheader(t('settings'))

        all_subreddits = config.get_reddit_subreddits()
        categories = config.get_reddit_categories()

        category_filter = st.selectbox(
            t('filter_category'),
            [t('all_categories')] + categories,
            index=0,
            key="reddit_category"
        )

        if category_filter == t('all_categories'):
            filtered_subs = all_subreddits
        else:
            filtered_subs = config.get_reddit_subreddits(category=category_filter)

        if 'reddit_selected' not in st.session_state:
            st.session_state.reddit_selected = []
        if 'reddit_widget_key' not in st.session_state:
            st.session_state.reddit_widget_key = 0

        col_sel1, col_sel2 = st.columns([3, 1])

        with col_sel2:
            st.write("")
            st.write("")
            if st.button(t('select_all'), key="select_all_reddit"):
                st.session_state.reddit_selected = filtered_subs.copy()
                st.session_state.reddit_widget_key += 1
                st.rerun()

        with col_sel1:
            selected_subs = st.multiselect(
                t('subreddits'),
                filtered_subs,
                default=[s for s in st.session_state.reddit_selected if s in filtered_subs],
                key=f"reddit_multiselect_{st.session_state.reddit_widget_key}"
            )
            st.session_state.reddit_selected = selected_subs

        settings = st.session_state.settings

        col_a, col_b, col_c, col_d = st.columns(4)
        with col_a:
            max_posts = st.slider(
                t('max_posts'),
                min_value=1,
                max_value=200,
                value=max(1, settings['default_max_posts']),
                key="reddit_max_posts",
                help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Å—Ç–æ–≤ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –∏–∑ –∫–∞–∂–¥–æ–≥–æ subreddit (1-200)",
                disabled=st.session_state.parsing_in_progress
            )
        with col_b:
            delay = st.slider(
                t('delay_sec'),
                3, 30,
                settings['default_delay'],
                key="reddit_delay",
                disabled=st.session_state.parsing_in_progress
            )
        with col_c:
            sort_by = st.selectbox(
                t('sort'),
                ["hot", "new", "top"],
                index=["hot", "new", "top"].index(settings['default_sort']),
                key="reddit_sort",
                disabled=st.session_state.parsing_in_progress
            )
        with col_d:
            enable_llm = st.checkbox(
                t('editorial'),
                value=settings['default_enable_llm'],
                key="reddit_llm",
                disabled=st.session_state.parsing_in_progress
            )

        st.markdown("---")

        # –°—Ç–∞—Ç—É—Å –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        if st.session_state.parsing_results:
            total_saved = sum(r.get('saved', 0) for r in st.session_state.parsing_results if r.get('success'))
            success_count = sum(1 for r in st.session_state.parsing_results if r.get('success'))
            total_count = len(st.session_state.parsing_results)

            st.success(f"‚úÖ –ü–æ—Å–ª–µ–¥–Ω–∏–π –ø–∞—Ä—Å–∏–Ω–≥: {total_saved} –ø–æ—Å—Ç–æ–≤ –∏–∑ {success_count}/{total_count} subreddits")

        # –ö–Ω–æ–ø–∫–∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è
        col_btn1, col_btn2, col_btn3 = st.columns([2, 1, 1])

        with col_btn1:
            start_button_disabled = (
                    not selected_subs or
                    not os.getenv("REDDIT_CLIENT_ID") or
                    st.session_state.parsing_in_progress
            )

            if st.button(
                    "üöÄ " + t('start_parsing') if not st.session_state.parsing_in_progress else "‚è∏Ô∏è –ü–∞—Ä—Å–∏–Ω–≥ –∞–∫—Ç–∏–≤–µ–Ω...",
                    type="primary",
                    use_container_width=True,
                    key="reddit_parse_btn",
                    disabled=start_button_disabled
            ):
                if not selected_subs:
                    st.error(t('select_subreddits'))
                elif not os.getenv("REDDIT_CLIENT_ID"):
                    st.error(t('api_not_configured'))
                else:
                    st.markdown("---")
                    results = scrape_with_live_logs(
                        subreddits=selected_subs,
                        max_posts=max_posts,
                        sort_by=sort_by,
                        delay=delay,
                        enable_llm=enable_llm
                    )
                    st.rerun()

        with col_btn2:
            if st.button(
                    "üîÑ –û–±–Ω–æ–≤–∏—Ç—å",
                    type="secondary",
                    use_container_width=True,
                    key="refresh_page_btn"
            ):
                # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ª–æ–≥–∏ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏
                if st.session_state.log_storage:
                    StreamlitLogger.restore_logs()
                st.rerun()

        with col_btn3:
            if st.button(
                    "üóëÔ∏è –û—á–∏—Å—Ç–∏—Ç—å",
                    type="secondary",
                    use_container_width=True,
                    key="clear_logs_btn",
                    disabled=st.session_state.parsing_in_progress
            ):
                StreamlitLogger.clear()
                st.session_state.parsing_results = None
                st.success("–õ–æ–≥–∏ –æ—á–∏—â–µ–Ω—ã!")
                time.sleep(0.5)
                st.rerun()

        # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –ª–æ–≥–æ–≤
        if st.session_state.parsing_logs or (
                st.session_state.log_storage and st.session_state.log_storage.log_file.exists()):
            with st.expander("üìú –í—Å–µ –ª–æ–≥–∏ —Å–µ—Å—Å–∏–∏", expanded=False):
                # –ï—Å–ª–∏ –ª–æ–≥–∏ –µ—Å—Ç—å –≤ –ø–∞–º—è—Ç–∏
                if st.session_state.parsing_logs:
                    st.markdown("\n".join(list(st.session_state.parsing_logs)))
                # –ï—Å–ª–∏ –Ω–µ—Ç - –ø–æ–ø—Ä–æ–±—É–µ–º –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å
                elif st.session_state.log_storage:
                    if st.button("üîÑ –ó–∞–≥—Ä—É–∑–∏—Ç—å –ª–æ–≥–∏ –∏–∑ —Ñ–∞–π–ª–∞"):
                        StreamlitLogger.restore_logs()
                        st.rerun()

    with col2:
        st.subheader(t('statistics'))
        st.metric(t('posts'), f"{stats['reddit_posts']:,}")

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏
        try:
            from src.models.database import get_processing_statistics

            proc_stats = get_processing_statistics()

            st.metric("–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ", f"{proc_stats['total_processed']:,}")
            st.metric("–ù–æ–≤–æ—Å—Ç–µ–π", f"{proc_stats['total_news']:,}")

            if proc_stats['total_raw'] > 0:
                st.progress(
                    proc_stats['processing_rate'] / 100,
                    text=f"–û–±—Ä–∞–±–æ—Ç–∫–∞: {proc_stats['processing_rate']}%"
                )
        except Exception as e:
            st.caption(f"–û—à–∏–±–∫–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {e}")

    st.markdown('</div>', unsafe_allow_html=True)

# === TAB 2: TELEGRAM ===
with tab2:
    st.markdown('<div class="telegram-section">', unsafe_allow_html=True)
    st.header(f"{t('telegram_tab')} Parser")
    st.info(t('in_development'))
    st.markdown('</div>', unsafe_allow_html=True)

# === TAB 3: MEDIUM ===
with tab3:
    st.markdown('<div class="medium-section">', unsafe_allow_html=True)
    st.header(f"{t('medium_tab')} Parser")
    st.info(t('in_development'))
    st.markdown('</div>', unsafe_allow_html=True)

# === TAB 4: UNIFIED DATA VIEWER ===
with tab4:
    st.header("üìä –ü—Ä–æ—Å–º–æ—Ç—Ä –¥–∞–Ω–Ω—ã—Ö Reddit")

    # –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–æ–π
    with st.expander("üîß –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–æ–π", expanded=False):
        st.markdown(f"### {t('sync_processing')}")
        st.caption(t('process_raw_posts'))

        col_sync1, col_sync2 = st.columns([2, 1])

        with col_sync1:
            st.markdown(f"**{t('statistics')}:**")
            try:
                session = get_session()
                total_raw = session.query(RedditPost).count()
                total_processed = session.query(ProcessedRedditPost).count()

                from sqlalchemy import exists

                unprocessed_count = session.query(RedditPost).filter(
                    ~exists().where(ProcessedRedditPost.post_id == RedditPost.post_id)
                ).count()

                session.close()

                col_a, col_b, col_c = st.columns(3)
                with col_a:
                    st.metric(t('raw_posts'), total_raw)
                with col_b:
                    st.metric(t('processed'), total_processed)
                with col_c:
                    st.metric(t('unprocessed'), unprocessed_count)

            except Exception as e:
                st.error(t('statistics_error', error=str(e)))
                unprocessed_count = 0

        with col_sync2:
            st.markdown(f"**{t('settings')}:**")
            batch_size = st.number_input(
                t('batch_size'),
                min_value=1,
                max_value=100,
                value=st.session_state.settings['batch_size'],
                help=t('batch_size_help'),
                key="batch_size_input"
            )

        st.markdown("---")

        if st.button(
                t('start_processing'),
                type="primary",
                use_container_width=True,
                key="start_processing_btn"
        ):
            if unprocessed_count == 0:
                st.info(t('no_unprocessed'))
            else:
                try:
                    from src.models.database import get_unprocessed_posts

                    unprocessed_posts = get_unprocessed_posts(limit=batch_size)

                    if not unprocessed_posts:
                        st.warning(t('no_posts_process'))
                    else:
                        st.markdown("---")
                        st.subheader("ü§ñ –û–±—Ä–∞–±–æ—Ç–∫–∞ LLM –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ...")

                        results = process_posts_with_live_logs(unprocessed_posts)

                        st.markdown("---")
                        st.success(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")

                        col_r1, col_r2, col_r3, col_r4 = st.columns(4)
                        with col_r1:
                            st.metric("–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ", results['processed'])
                        with col_r2:
                            st.metric("–ù–æ–≤–æ—Å—Ç–µ–π", results['news'])
                        with col_r3:
                            st.metric("–ü—Ä–æ–ø—É—â–µ–Ω–æ", results['skipped'])
                        with col_r4:
                            st.metric("–û—à–∏–±–æ–∫", results['errors'])

                        if results['errors'] > 0:
                            st.error(f"‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {results['errors']} –æ—à–∏–±–æ–∫. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ª–æ–≥–∏ –≤—ã—à–µ –¥–ª—è –¥–µ—Ç–∞–ª–µ–π.")
                            st.info(
                                "üí° –°–æ–≤–µ—Ç: –ü—Ä–æ–∫—Ä—É—Ç–∏—Ç–µ –ª–æ–≥–∏ –≤–≤–µ—Ä—Ö –∏ –Ω–∞–π–¥–∏—Ç–µ —Å–æ–æ–±—â–µ–Ω–∏—è —Å ‚ùå –¥–ª—è –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ –¥–µ—Ç–∞–ª–µ–π –æ—à–∏–±–æ–∫.")

                        if st.button("üîÑ –û–±–Ω–æ–≤–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É", key="refresh_after_processing"):
                            st.rerun()

                except Exception as e:
                    st.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")

                    with st.expander("üîç –î–µ—Ç–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± –æ—à–∏–±–∫–µ"):
                        import traceback

                        st.code(traceback.format_exc(), language="python")

    st.markdown("---")

    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
    col_filter, col_sort, col_limit = st.columns([2, 2, 1])

    with col_filter:
        # –ò–ó–ú–ï–ù–ï–ù–ò–ï: index=2 –¥–ª—è –≤—ã–±–æ—Ä–∞ "–í—Å–µ –ø–æ—Å—Ç—ã" –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        view_mode = st.radio(
            "–†–µ–∂–∏–º –ø—Ä–æ—Å–º–æ—Ç—Ä–∞",
            ["üî¥ –¢–æ–ª—å–∫–æ —Å—ã—Ä—ã–µ", "ü§ñ –¢–æ–ª—å–∫–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ", "üìã –í—Å–µ –ø–æ—Å—Ç—ã"],
            index=2,  # <-- –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é "–í—Å–µ –ø–æ—Å—Ç—ã"
            horizontal=True,
            key="view_mode_radio"
        )

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π —Ñ–∏–ª—å—Ç—Ä –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö
        news_only = False
        if "–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ" in view_mode.lower():
            news_only = st.checkbox("–¢–æ–ª—å–∫–æ –Ω–æ–≤–æ—Å—Ç–∏", value=False, key="news_filter")

    with col_sort:
        # –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ä–µ–∂–∏–º–∞
        if "—Å—ã—Ä—ã–µ" in view_mode.lower():
            sort_options = {
                "–ü–æ–ª—É—á–µ–Ω—ã (–Ω–æ–≤—ã–µ)": "scraped_at_desc",
                "–ü–æ–ª—É—á–µ–Ω—ã (—Å—Ç–∞—Ä—ã–µ)": "scraped_at_asc",
                "–û–ø—É–±–ª–∏–∫–æ–≤–∞–Ω—ã (–Ω–æ–≤—ã–µ)": "created_utc_desc",
                "–û–ø—É–±–ª–∏–∫–æ–≤–∞–Ω—ã (—Å—Ç–∞—Ä—ã–µ)": "created_utc_asc",
                "–†–µ–π—Ç–∏–Ω–≥ ‚¨Ü": "score_desc",
                "–†–µ–π—Ç–∏–Ω–≥ ‚¨á": "score_asc"
            }
        elif "–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ" in view_mode.lower():
            sort_options = {
                "–û–±—Ä–∞–±–æ—Ç–∞–Ω—ã (–Ω–æ–≤—ã–µ)": "processed_at_desc",
                "–û–±—Ä–∞–±–æ—Ç–∞–Ω—ã (—Å—Ç–∞—Ä—ã–µ)": "processed_at_asc",
                "–†–µ–π—Ç–∏–Ω–≥ ‚¨Ü": "score_desc",
                "–†–µ–π—Ç–∏–Ω–≥ ‚¨á": "score_asc"
            }
        else:  # –í—Å–µ –ø–æ—Å—Ç—ã
            sort_options = {
                "–û–±—Ä–∞–±–æ—Ç–∞–Ω—ã (–Ω–æ–≤—ã–µ)": "processed_at_desc",
                "–ü–æ–ª—É—á–µ–Ω—ã (–Ω–æ–≤—ã–µ)": "scraped_at_desc",
                "–†–µ–π—Ç–∏–Ω–≥ ‚¨Ü": "score_desc"
            }

        sort_by = st.selectbox(
            "–°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞",
            list(sort_options.keys()),
            key="unified_sort"
        )
        sort_value = sort_options[sort_by]

    with col_limit:
        limit = st.slider(
            "–ó–∞–ø–∏—Å–µ–π",
            10, 500,
            st.session_state.settings['viewer_default_limit'],
            key="unified_limit"
        )

    st.markdown("---")

    # –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
    try:
        session = get_session()

        if view_mode == "üî¥ –¢–æ–ª—å–∫–æ —Å—ã—Ä—ã–µ":
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ —Å—ã—Ä—ã–µ –ø–æ—Å—Ç—ã
            query = session.query(RedditPost)

            if sort_value == "scraped_at_desc":
                query = query.order_by(RedditPost.scraped_at.desc())
            elif sort_value == "scraped_at_asc":
                query = query.order_by(RedditPost.scraped_at.asc())
            elif sort_value == "created_utc_desc":
                query = query.order_by(RedditPost.created_utc.desc())
            elif sort_value == "created_utc_asc":
                query = query.order_by(RedditPost.created_utc.asc())
            elif sort_value == "score_desc":
                query = query.order_by(RedditPost.score.desc())
            elif sort_value == "score_asc":
                query = query.order_by(RedditPost.score.asc())

            posts = query.limit(limit).all()

            if posts:
                st.caption(f"üî¥ –ù–∞–π–¥–µ–Ω–æ: {len(posts)} —Å—ã—Ä—ã—Ö –ø–æ—Å—Ç–æ–≤")
                for post in posts:
                    render_raw_post_viewer(post, st.session_state.language)
            else:
                st.info("–ù–µ—Ç —Å—ã—Ä—ã—Ö –ø–æ—Å—Ç–æ–≤")

        elif view_mode == "ü§ñ –¢–æ–ª—å–∫–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ":
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –ø–æ—Å—Ç—ã
            query = session.query(ProcessedRedditPost)

            if news_only:
                query = query.filter(ProcessedRedditPost.is_news == True)

            if sort_value == "processed_at_desc":
                query = query.order_by(ProcessedRedditPost.processed_at.desc())
            elif sort_value == "processed_at_asc":
                query = query.order_by(ProcessedRedditPost.processed_at.asc())
            elif sort_value == "score_desc":
                query = query.order_by(ProcessedRedditPost.score.desc())
            elif sort_value == "score_asc":
                query = query.order_by(ProcessedRedditPost.score.asc())

            processed_posts = query.limit(limit).all()

            if processed_posts:
                filter_text = " (—Ç–æ–ª—å–∫–æ –Ω–æ–≤–æ—Å—Ç–∏)" if news_only else ""
                st.caption(f"ü§ñ –ù–∞–π–¥–µ–Ω–æ: {len(processed_posts)} –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –ø–æ—Å—Ç–æ–≤{filter_text}")

                for proc_post in processed_posts:
                    raw_post = session.query(RedditPost).filter_by(post_id=proc_post.post_id).first()
                    render_processed_post_viewer(proc_post, raw_post, st.session_state.language)
            else:
                st.info("–ù–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –ø–æ—Å—Ç–æ–≤" + (" (–Ω–æ–≤–æ—Å—Ç–µ–π)" if news_only else ""))

        else:  # üìã –í—Å–µ –ø–æ—Å—Ç—ã
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –ø–æ—Å—Ç—ã —Å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å—é —Ä–∞–∑–≤–µ—Ä–Ω—É—Ç—å —Å—ã—Ä—ã–µ
            query = session.query(ProcessedRedditPost)

            if news_only:
                query = query.filter(ProcessedRedditPost.is_news == True)

            if sort_value == "processed_at_desc":
                query = query.order_by(ProcessedRedditPost.processed_at.desc())
            elif sort_value == "scraped_at_desc":
                # –ü—Ä–∏—Å–æ–µ–¥–∏–Ω—è–µ–º RedditPost –¥–ª—è —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏ –ø–æ scraped_at
                query = query.join(RedditPost, ProcessedRedditPost.post_id == RedditPost.post_id)
                query = query.order_by(RedditPost.scraped_at.desc())
            elif sort_value == "score_desc":
                query = query.order_by(ProcessedRedditPost.score.desc())

            processed_posts = query.limit(limit).all()

            if processed_posts:
                filter_text = " (—Ç–æ–ª—å–∫–æ –Ω–æ–≤–æ—Å—Ç–∏)" if news_only else ""
                st.caption(f"üìã –ù–∞–π–¥–µ–Ω–æ: {len(processed_posts)} –ø–æ—Å—Ç–æ–≤{filter_text}")

                for proc_post in processed_posts:
                    raw_post = session.query(RedditPost).filter_by(post_id=proc_post.post_id).first()
                    render_processed_post_viewer(proc_post, raw_post, st.session_state.language)
            else:
                st.info("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö" + (" (–Ω–æ–≤–æ—Å—Ç–µ–π)" if news_only else ""))

        session.close()

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏
        st.markdown("---")
        st.subheader("üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏")

        col_stat1, col_stat2, col_stat3, col_stat4 = st.columns(4, gap="small")

        session = get_session()
        total_posts = session.query(RedditPost).count()
        vectorized_posts = session.query(RedditPost).filter(RedditPost.qdrant_id.isnot(None)).count()
        total_processed = session.query(ProcessedRedditPost).count()
        news_count = session.query(ProcessedRedditPost).filter(ProcessedRedditPost.is_news == True).count()
        session.close()

        with col_stat1:
            st.metric("–í—Å–µ–≥–æ –ø–æ—Å—Ç–æ–≤", total_posts)
        with col_stat2:
            st.metric("–í–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–æ", vectorized_posts)
        with col_stat3:
            st.metric("–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ", total_processed)
        with col_stat4:
            st.metric("–ù–æ–≤–æ—Å—Ç–µ–π", news_count)

        # –ü—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä—ã
        col_prog1, col_prog2 = st.columns(2)
        with col_prog1:
            if total_posts > 0:
                vec_pct = vectorized_posts / total_posts
                st.progress(vec_pct, text=f"–í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è: {vec_pct:.1%}")
            else:
                st.progress(0.0, text="–í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è: 0%")

        with col_prog2:
            if total_posts > 0:
                proc_pct = total_processed / total_posts
                st.progress(proc_pct, text=f"–û–±—Ä–∞–±–æ—Ç–∫–∞: {proc_pct:.1%}")
            else:
                st.progress(0.0, text="–û–±—Ä–∞–±–æ—Ç–∫–∞: 0%")

    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö: {e}")
        import traceback

        st.code(traceback.format_exc())

# === TAB 5: ANALYTICS ===
with tab5:
    st.header(t('analytics_tab'))

    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("Reddit", f"{stats['reddit_posts']:,}")
    with col2:
        st.metric("Telegram", f"{stats['telegram_messages']:,}")
    with col3:
        st.metric("Medium", f"{stats['medium_articles']:,}")
    with col4:
        total = stats['reddit_posts'] + stats['telegram_messages'] + stats['medium_articles']
        st.metric(t('total'), f"{total:,}")

    st.markdown("---")

    if total > 0:
        st.subheader(t('activity'))

        chart_data = pd.DataFrame({
            t('platform'): ['Reddit', 'Telegram', 'Medium'],
            t('records_count'): [
                stats['reddit_posts'],
                stats['telegram_messages'],
                stats['medium_articles']
            ]
        })
        st.bar_chart(chart_data.set_index(t('platform')))
    else:
        st.info(t('no_data'))

# === TAB 6: SETTINGS ===
with tab6:
    st.header("‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏")

    st.subheader("ü§ñ LLM –û–±—Ä–∞–±–æ—Ç–∫–∞")
    col_llm1, col_llm2 = st.columns(2)
    with col_llm1:
        st.metric("–ú–æ–¥–µ–ª—å", st.session_state.settings['llm_model'])
        st.metric("Temperature", st.session_state.settings['llm_temperature'])
    with col_llm2:
        st.metric("Max Tokens", st.session_state.settings['llm_max_tokens'])
        st.metric("–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –ø–æ—Ç–æ–∫–æ–≤", st.session_state.settings['max_parallel_tasks'])

    st.markdown("---")

    # –ò–ó–ú–ï–ù–ï–ù–ò–ï: –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–∞—Å—Ç—Ä–æ–µ–∫ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
    st.subheader("üìã –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è")

    settings = st.session_state.settings

    # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –Ω–∞—Å—Ç—Ä–æ–µ–∫ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
    database_settings = {
        'postgres_user': settings['postgres_user'],
        'postgres_db': settings['postgres_db'],
        'postgres_port': settings['postgres_port']
    }

    reddit_settings = {
        'reddit_client_id': settings['reddit_client_id'],
        'reddit_client_secret': settings['reddit_client_secret'],
        'reddit_user_agent': settings['reddit_user_agent']
    }

    telegram_settings = {
        'telegram_api_id': settings['telegram_api_id'],
        'telegram_api_hash': settings['telegram_api_hash'],
        'telegram_phone': settings['telegram_phone']
    }

    llm_settings = {
        'llm_provider': settings['llm_provider'],
        'llm_model': settings['llm_model'],
        'llm_temperature': settings['llm_temperature'],
        'llm_max_tokens': settings['llm_max_tokens'],
        'llm_top_p': settings['llm_top_p'],
        'max_parallel_tasks': settings['max_parallel_tasks']
    }

    services_settings = {
        'qdrant_url': settings['qdrant_url'],
        'qdrant_port': settings['qdrant_port'],
        'ollama_base_url': settings['ollama_base_url'],
        'ollama_port': settings['ollama_port']
    }

    parsing_settings = {
        'default_max_posts': settings['default_max_posts'],
        'default_delay': settings['default_delay'],
        'default_sort': settings['default_sort'],
        'default_enable_llm': settings['default_enable_llm'],
        'batch_size': settings['batch_size'],
        'min_text_length': settings['min_text_length']
    }

    quality_settings = {
        'enable_semantic_dedup': settings['enable_semantic_dedup'],
        'enable_vectorization': settings['enable_vectorization']
    }

    ui_settings = {
        'logs_max_length': settings['logs_max_length'],
        'viewer_default_limit': settings['viewer_default_limit'],
        'show_debug_info': settings['show_debug_info'],
        'app_port': settings['app_port']
    }

    # –†–µ–Ω–¥–µ—Ä–∏–Ω–≥ –∫–∞—Ç–µ–≥–æ—Ä–∏–π
    render_settings_section("–ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö (PostgreSQL)", database_settings, "üóÑÔ∏è")
    render_settings_section("Reddit API", reddit_settings, "üî¥")
    render_settings_section("Telegram API", telegram_settings, "üí¨")
    render_settings_section("LLM & AI", llm_settings, "ü§ñ")
    render_settings_section("–°–µ—Ä–≤–∏—Å—ã (Qdrant, Ollama)", services_settings, "üîß")
    render_settings_section("–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–∞—Ä—Å–∏–Ω–≥–∞", parsing_settings, "üì•")
    render_settings_section("–ö–∞—á–µ—Å—Ç–≤–æ –∏ –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è", quality_settings, "‚ú®")
    render_settings_section("–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å", ui_settings, "üé®")

    # –≠–∫—Å–ø–æ—Ä—Ç –≤ JSON/YAML
    st.markdown("---")
    st.subheader("üíæ –≠–∫—Å–ø–æ—Ä—Ç –Ω–∞—Å—Ç—Ä–æ–µ–∫")

    col_export1, col_export2 = st.columns(2)

    with col_export1:
        if st.button("üìÑ –°–∫–∞—á–∞—Ç—å JSON", use_container_width=True):
            json_str = json.dumps(settings, indent=2, ensure_ascii=False)
            st.download_button(
                label="‚¨áÔ∏è –°–æ—Ö—Ä–∞–Ω–∏—Ç—å settings.json",
                data=json_str,
                file_name="settings.json",
                mime="application/json"
            )

    with col_export2:
        if st.button("üìã –ü–æ–∫–∞–∑–∞—Ç—å raw JSON", use_container_width=True):
            with st.expander("Raw JSON", expanded=True):
                st.json(settings)

# === FOOTER ===
st.markdown("---")
col_f1, col_f2, col_f3 = st.columns([2, 1, 1])
with col_f1:
    st.caption("PostgreSQL ‚Ä¢ Docker ‚Ä¢ N8N ‚Ä¢ Ollama ‚Ä¢ GPT-OSS")
with col_f2:
    current_model = st.session_state.settings['llm_model']
    st.caption(f"ü§ñ Model: {current_model}")
with col_f3:
    if st.button("üîÑ –û–±–Ω–æ–≤–∏—Ç—å", key="refresh_btn"):
        st.rerun()