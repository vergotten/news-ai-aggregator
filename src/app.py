"""
–í–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å Streamlit –¥–ª—è News Aggregator —Å –∂–∏–≤—ã–º–∏ –ª–æ–≥–∞–º–∏.

–≠—Ç–æ—Ç –º–æ–¥—É–ª—å –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –ø–∞–Ω–µ–ª—å —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
(Reddit, Habr, Telegram, Medium), –∏—Ö –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å –ø–æ–º–æ—â—å—é LLM, –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
–∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏.
"""

import streamlit as st
import sys
import os
from pathlib import Path
import pandas as pd
import time
import uuid
from collections import deque
import json
import multiprocessing
from queue import Empty
import requests
from datetime import datetime, timezone, timedelta

from src.scrapers.telegram_scraper import scrape_telegram_channels

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤ sys.path –¥–ª—è –∏–º–ø–æ—Ä—Ç–æ–≤
sys.path.insert(0, str(Path(__file__).parent.parent))

st.set_page_config(
    page_title="News Aggregator",
    page_icon="üì∞",
    layout="wide"
)

try:
    from src.config.config import get_config
    config = get_config()
    st.success("‚úÖ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
except FileNotFoundError as e:
    st.error("‚ùå –§–∞–π–ª .env –Ω–µ –Ω–∞–π–¥–µ–Ω!")
    st.error(str(e))
    st.stop()
except ValueError as e:
    st.error("‚ùå –ù–µ–≤–∞–ª–∏–¥–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è!")
    st.error(str(e))
    st.stop()
except Exception as e:
    st.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {e}")
    st.stop()

from src.utils.translations import TRANSLATIONS

def t(key: str, **kwargs) -> str:
    """–ü–µ—Ä–µ–≤–æ–¥ –∫–ª—é—á–∞ –Ω–∞ —Ç–µ–∫—É—â–∏–π —è–∑—ã–∫."""
    lang = st.session_state.get('language', 'ru')
    text = TRANSLATIONS.get(lang, TRANSLATIONS['ru']).get(key, key)
    if kwargs:
        return text.format(**kwargs)
    return text

# –ó–∞–≥—Ä—É–∑–∫–∞ CSS —Å—Ç–∏–ª–µ–π
css_path = Path(__file__).parent / "static" / "style.css"
if css_path.exists():
    with open(css_path) as f:
        st.markdown(f"<style>{f.read()}</style>", unsafe_allow_html=True)

try:
    from src.models.database import (
        init_db,
        get_stats_extended,
        # get_posts_by_subreddit,
        # get_processed_posts,
        # get_processed_by_subreddit,
        # get_medium_articles,
        # get_habr_articles,
        # get_session,
        # get_processing_statistics,
        # is_post_processed,
        # get_unprocessed_posts,
        RedditPost,
        ProcessedRedditPost,
        MediumArticle,
        TelegramMessage,
        HabrArticle,
        TelegramPost,
        TelegramPostRepository, # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π
    )
    from src.config_loader import get_config as get_sources_config
    from src.services.editorial_service import EditorialService
    from src.scrapers.reddit_scraper import get_reddit_client, scrape_subreddit

    init_db()
    sources_config = get_sources_config()
except Exception as e:
    st.error(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏: {e}")
    st.stop()

# ============================================================================
# –ù–ê–°–¢–†–û–ô–ö–ò –ò–ó –¶–ï–ù–¢–†–ê–õ–ò–ó–û–í–ê–ù–ù–û–ô –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–ò
# ============================================================================
# –°–æ–∑–¥–∞—ë–º SETTINGS –∏–∑ config –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
SETTINGS = {
    # PostgreSQL
    'postgres_user': config.POSTGRES_USER,
    'postgres_password': config.POSTGRES_PASSWORD,
    'postgres_db': config.POSTGRES_DB,
    'postgres_port': config.POSTGRES_PORT,

    # Reddit API
    'reddit_client_id': config.REDDIT_CLIENT_ID,
    'reddit_client_secret': config.REDDIT_CLIENT_SECRET,
    'reddit_user_agent': config.REDDIT_USER_AGENT,

    # Telegram API
    'telegram_api_id': config.TELEGRAM_API_ID,
    'telegram_api_hash': config.TELEGRAM_API_HASH,
    'telegram_phone': config.TELEGRAM_PHONE,

    # Qdrant
    'qdrant_port': config.QDRANT_PORT,
    'qdrant_grpc_port': config.QDRANT_GRPC_PORT,
    'qdrant_url': config.QDRANT_URL,

    # Ollama
    'ollama_port': config.OLLAMA_PORT,
    'ollama_base_url': config.OLLAMA_BASE_URL,

    # –ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ
    'app_port': config.APP_PORT,
    'tz': config.TZ,
    'adminer_port': config.ADMINER_PORT,

    # LLM Processing
    'max_parallel_tasks': config.MAX_PARALLEL_TASKS,

    # N8N
    'n8n_port': config.N8N_PORT,
    'n8n_db': config.N8N_DB,
    'n8n_basic_auth_active': config.N8N_BASIC_AUTH_ACTIVE,
    'n8n_basic_auth_user': config.N8N_BASIC_AUTH_USER,
    'n8n_basic_auth_password': config.N8N_BASIC_AUTH_PASSWORD,

    # LLM
    'llm_provider': config.LLM_PROVIDER,
    'llm_model': config.LLM_MODEL,
    'llm_temperature': config.LLM_TEMPERATURE,
    'llm_max_tokens': config.LLM_MAX_TOKENS,
    'llm_top_p': config.LLM_TOP_P,
    'llm_base_url': config.LLM_BASE_URL,

    # –ü–∞—Ä—Å–∏–Ω–≥
    'default_max_posts': config.DEFAULT_MAX_POSTS,
    'default_delay': config.DEFAULT_DELAY,
    'default_sort': config.DEFAULT_SORT,
    'default_enable_llm': config.DEFAULT_ENABLE_LLM,
    'batch_size': config.BATCH_SIZE,

    # –ö–∞—á–µ—Å—Ç–≤–æ
    'min_text_length': config.MIN_TEXT_LENGTH,
    'enable_semantic_dedup': config.ENABLE_SEMANTIC_DEDUP,
    'enable_vectorization': config.ENABLE_VECTORIZATION,

    # UI
    'logs_max_length': config.LOGS_MAX_LENGTH,
    'viewer_default_limit': config.VIEWER_DEFAULT_LIMIT,
    'show_debug_info': config.SHOW_DEBUG_INFO
}

# ============================================================================
# –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø SESSION STATE
# ============================================================================
# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑–æ–≤—ã—Ö –Ω–∞—Å—Ç—Ä–æ–µ–∫
if 'settings' not in st.session_state:
    st.session_state.settings = SETTINGS

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ª–æ–≥ –º–µ–Ω–µ–¥–∂–µ—Ä–∞
if 'log_manager' not in st.session_state:
    try:
        from src.utils.log_manager import get_log_manager
        st.session_state.log_manager = get_log_manager()
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –ª–æ–≥ –º–µ–Ω–µ–¥–∂–µ—Ä–∞: {e}")
        st.session_state.log_manager = None

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏—è –ø–∞—Ä—Å–∏–Ω–≥–∞
if 'parsing_logs' not in st.session_state:
    st.session_state.parsing_logs = deque(maxlen=st.session_state.settings['logs_max_length'])

if 'parsing_active' not in st.session_state:
    st.session_state.parsing_active = False

if 'parsing_in_progress' not in st.session_state:
    st.session_state.parsing_in_progress = False

if 'parsing_results' not in st.session_state:
    st.session_state.parsing_results = None

if 'parsing_progress' not in st.session_state:
    st.session_state.parsing_progress = {'current': 0, 'total': 0, 'status': ''}

if 'language' not in st.session_state:
    st.session_state.language = 'ru'

if 'log_session_counter' not in st.session_state:
    st.session_state.log_session_counter = 0

if 'current_session_id' not in st.session_state:
    st.session_state.current_session_id = None

if 'logs_restored' not in st.session_state:
    st.session_state.logs_restored = False

# ============================================================================
# –§–£–ù–ö–¶–ò–ò –í–û–°–°–¢–ê–ù–û–í–õ–ï–ù–ò–Ø –ò –ü–†–û–í–ï–†–ö–ò
# ============================================================================

def restore_logs():
    """–í–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –ª–æ–≥–∏ –∏–∑ Redis –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã."""
    if not st.session_state.logs_restored and st.session_state.get('log_manager'):
        try:
            log_manager = st.session_state.log_manager
            logs = log_manager.get_logs(limit=100)

            if logs:
                # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –ª–æ–≥–∏ –∏–∑ Redis –≤ —Ñ–æ—Ä–º–∞—Ç Streamlit
                formatted_logs = []
                for log in logs:
                    icon = {"INFO": "‚ÑπÔ∏è", "SUCCESS": "‚úÖ", "WARNING": "‚ö†Ô∏è", "ERROR": "‚ùå", "DEBUG": "üîç"}.get(log.get('level', 'INFO'), "üìù")
                    timestamp = log.get('timestamp', '')[:8]  # –¢–æ–ª—å–∫–æ –≤—Ä–µ–º—è
                    formatted_logs.append(f"{icon} `{timestamp}` {log.get('message', '')}")

                st.session_state.parsing_logs = deque(
                    formatted_logs,
                    maxlen=st.session_state.settings['logs_max_length']
                )

            st.session_state.logs_restored = True
        # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ó–∞–º–µ–Ω–µ–Ω –≥–æ–ª—ã–π except –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π —Ç–∏–ø –¥–ª—è –ª—É—á—à–µ–π –æ—Ç–ª–∞–¥–∫–∏
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –ª–æ–≥–æ–≤: {e}")

def check_active_sessions():
    """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –Ω–∞–ª–∏—á–∏–µ –∞–∫—Ç–∏–≤–Ω—ã—Ö —Å–µ—Å—Å–∏–π –ø–∞—Ä—Å–∏–Ω–≥–∞."""
    if st.session_state.get('log_manager'):
        try:
            log_manager = st.session_state.log_manager
            active_sessions = log_manager.get_active_sessions()

            if active_sessions:
                st.session_state.current_session_id = active_sessions[0]['id']
                st.session_state.parsing_active = True
                return True
        # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ó–∞–º–µ–Ω–µ–Ω –≥–æ–ª—ã–π except –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π —Ç–∏–ø –¥–ª—è –ª—É—á—à–µ–π –æ—Ç–ª–∞–¥–∫–∏
        except Exception as e:
            st.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –∞–∫—Ç–∏–≤–Ω—ã—Ö —Å–µ—Å—Å–∏–π: {e}")

    return False

# –í—ã–∑—ã–≤–∞–µ–º –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –∏ –ø—Ä–æ–≤–µ—Ä–∫—É –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ
restore_logs()
check_active_sessions()

# ============================================================================
# –ö–õ–ê–°–°–´ –ò –§–£–ù–ö–¶–ò–ò
# ============================================================================

class StreamlitLogger:
    """–õ–æ–≥–≥–µ—Ä, –∑–∞–ø–∏—Å—ã–≤–∞—é—â–∏–π —Å–æ–æ–±—â–µ–Ω–∏—è –≤ session_state –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ UI."""

    @staticmethod
    def log(message: str, level: str = "INFO") -> None:
        """–î–æ–±–∞–≤–∏—Ç—å –ª–æ–≥-—Å–æ–æ–±—â–µ–Ω–∏–µ."""
        from datetime import datetime, timezone, timedelta
        timestamp = datetime.now().strftime("%H:%M:%S")

        # Moscow time (UTC+3)
        moscow_tz = timezone(timedelta(hours=3))
        timestamp = datetime.now(moscow_tz).strftime("%H:%M:%S")

        icon = {"INFO": "‚ÑπÔ∏è", "SUCCESS": "‚úÖ", "WARNING": "‚ö†Ô∏è", "ERROR": "‚ùå", "DEBUG": "üîç"}.get(level, "üìù")
        log_entry = f"{icon} `{timestamp}` {message}"
        st.session_state.parsing_logs.append(log_entry)

        # –¢–∞–∫–∂–µ –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º –≤ –ª–æ–≥ –º–µ–Ω–µ–¥–∂–µ—Ä
        if st.session_state.get('log_manager'):
            try:
                st.session_state.log_manager.add_log(message, level, st.session_state.get('current_session_id'))
            except Exception as e:
                st.warning(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ª–æ–≥–∞: {e}")

    @staticmethod
    def add_separator(session_number: int) -> None:
        """–î–æ–±–∞–≤–∏—Ç—å —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å –º–µ–∂–¥—É —Å–µ—Å—Å–∏—è–º–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞."""
        from datetime import datetime, timezone, timedelta

        moscow_tz = timezone(timedelta(hours=3))
        separator = (
            f"\n{'=' * 80}\n"
            f"üÜï **–ù–û–í–ê–Ø –°–ï–°–°–ò–Ø #{session_number}** - "
            f"{datetime.now(moscow_tz).strftime('%Y-%m-%d %H:%M:%S')}\n"
            f"{'=' * 80}\n"
        )
        st.session_state.parsing_logs.append(separator)

    @staticmethod
    def clear() -> None:
        """–û—á–∏—Å—Ç–∏—Ç—å –ª–æ–≥–∏."""
        st.session_state.parsing_logs.clear()
        st.session_state.log_session_counter = 0

        # –û—á–∏—â–∞–µ–º —Ç–∞–∫–∂–µ –≤ Redis
        if st.session_state.get('log_manager'):
            try:
                st.session_state.log_manager.clear_logs()
            except Exception as e:
                st.warning(f"–û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ –ª–æ–≥–æ–≤: {e}")

# ============================================================================
# HABR SCRAPER - MULTIPROCESSING WRAPPER
# ============================================================================

def habr_scraper_worker(
        hubs: list,
        tags: list,
        max_articles: int,
        enable_llm: bool,
        enable_dedup: bool,
        log_queue: multiprocessing.Queue,
        result_queue: multiprocessing.Queue
):
    """Worker –ø—Ä–æ—Ü–µ—Å—Å –¥–ª—è –∑–∞–ø—É—Å–∫–∞ —Å–∫—Ä–∏–ø—Ç–∞-—Ä–∞–Ω–Ω–µ—Ä–∞ Habr."""
    import sys
    import subprocess
    import traceback
    from pathlib import Path

    project_root = Path(__file__).parent.parent
    runner_script = project_root / "src" / "scrapers" / "run_habr_scraper.py"

    def process_log_callback(message: str, level: str):
        try:
            log_queue.put({'message': message, 'level': level})
        except Exception as e:
            print(f"Error in log callback: {e}")

    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º —Å Scrapy
    env = os.environ.copy()
    env['PYTHONPATH'] = str(project_root)
    env['SCRAPY_SETTINGS_MODULE'] = 'src.scrapers.settings'

    try:
        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–º–∞–Ω–¥—ã —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –∞—Ä–≥—É–º–µ–Ω—Ç–∞–º–∏
        cmd = [
            sys.executable, str(runner_script),
            '--max-articles', str(max_articles)  # ‚úì hyphen, –Ω–µ underscore
        ]

        # –î–æ–±–∞–≤–ª—è–µ–º —Ö–∞–±—ã —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é
        if hubs:
            cmd.extend(['--hubs', ','.join(hubs)])

        # –§–ª–∞–≥–∏ - –ë–ï–ó —è–≤–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π True/False
        if enable_llm:
            cmd.append('--enable-llm')
        else:
            cmd.append('--no-llm')

        if enable_dedup:
            cmd.append('--enable-dedup')
        else:
            cmd.append('--no-dedup')

        process_log_callback(f"Executing command: {' '.join(cmd)}", "INFO")
        process_log_callback(f"Working directory: {project_root}", "DEBUG")

        # –ó–∞–ø—É—Å–∫ –ø—Ä–æ—Ü–µ—Å—Å–∞ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –≤—ã–≤–æ–¥–∞
        process = subprocess.Popen(
            cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
            universal_newlines=True,
            bufsize=1,
            cwd=str(project_root),
            env=env
        )

        # –ß—Ç–µ–Ω–∏–µ –≤—ã–≤–æ–¥–∞ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏
        output_lines = []
        while True:
            output = process.stdout.readline()
            if output == '' and process.poll() is not None:
                break
            if output:
                output_lines.append(output.strip())

                # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —É—Ä–æ–≤–Ω—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
                if "INFO" in output:
                    level = "INFO"
                elif "WARNING" in output or "WARN" in output:
                    level = "WARNING"
                elif "ERROR" in output or "CRITICAL" in output:
                    level = "ERROR"
                elif "SUCCESS" in output:
                    level = "SUCCESS"
                else:
                    level = "INFO"

                process_log_callback(output.strip(), level)

        # –ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–¥–∞ –≤–æ–∑–≤—Ä–∞—Ç–∞
        return_code = process.poll()

        # –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        if return_code == 0:
            process_log_callback("Habr scraping completed successfully.", "SUCCESS")

            # –ü–æ–ø—ã—Ç–∫–∞ –∏–∑–≤–ª–µ—á—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏–∑ –≤—ã–≤–æ–¥–∞
            saved = 0
            skipped = 0
            errors = 0

            for line in output_lines:
                if "–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ:" in line:
                    try:
                        saved = int(line.split("–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ:")[1].strip())
                    except (ValueError, IndexError):
                        pass
                elif "–ü—Ä–æ–ø—É—â–µ–Ω–æ:" in line:
                    try:
                        skipped = int(line.split("–ü—Ä–æ–ø—É—â–µ–Ω–æ:")[1].strip())
                    except (ValueError, IndexError):
                        pass
                elif "–û—à–∏–±–æ–∫:" in line:
                    try:
                        errors = int(line.split("–û—à–∏–±–æ–∫:")[1].strip())
                    except (ValueError, IndexError):
                        pass

            result = {
                'success': True,
                'saved': saved,
                'skipped': skipped,
                'semantic_duplicates': 0,
                'editorial_processed': 0,
                'errors': errors
            }
            result_queue.put(result)
        else:
            process_log_callback(f"Habr scraping failed with return code {return_code}.", "ERROR")
            result_queue.put({
                'success': False,
                'error': f'Process failed with code {return_code}',
                'saved': 0,
                'skipped': 0,
                'semantic_duplicates': 0,
                'editorial_processed': 0,
                'errors': 1
            })

    except Exception as e:
        error_msg = f"Critical error in habr_scraper_worker: {str(e)}"
        process_log_callback(error_msg, "ERROR")
        traceback.print_exc()
        result_queue.put({
            'success': False,
            'error': str(e),
            'saved': 0,
            'skipped': 0,
            'semantic_duplicates': 0,
            'editorial_processed': 0,
            'errors': 1
        })

def scrape_habr_with_live_logs(
    hubs: list,
    tags: list,
    max_articles: int,
    enable_llm: bool = True,
    enable_dedup: bool = True,
) -> dict:
    """
    –°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±–µ—Ä—Ç–∫–∞ –¥–ª—è –∑–∞–ø—É—Å–∫–∞ Habr –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å –∂–∏–≤—ã–º–∏ –ª–æ–≥–∞–º–∏.
    """
    logger = StreamlitLogger()

    # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é —Å–µ—Å—Å–∏—é
    if st.session_state.get('log_manager'):
        session_id = st.session_state.log_manager.create_session()
        st.session_state.current_session_id = session_id
    else:
        # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: `uuid` —Ç–µ–ø–µ—Ä—å –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω, –æ—à–∏–±–∫–∏ –Ω–µ –±—É–¥–µ—Ç
        session_id = str(uuid.uuid4())

    st.session_state.log_session_counter += 1
    logger.add_separator(st.session_state.log_session_counter)

    # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ñ–ª–∞–≥ –∞–∫—Ç–∏–≤–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞
    st.session_state.parsing_active = True
    st.session_state.parsing_in_progress = True

    progress_bar = st.progress(0)
    status_text = st.empty()
    log_container = st.expander("üìã **–õ–æ–≥–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞ Habr**", expanded=True)
    log_placeholder = log_container.empty()

    logger.log(f"üöÄ –ù–∞—á–∞–ª–æ –ø–∞—Ä—Å–∏–Ω–≥–∞ Habr", "INFO")
    logger.log(f"–°–µ—Å—Å–∏—è: {session_id[:8]}...", "DEBUG")
    logger.log(f"–•–∞–±—ã: {', '.join(hubs) if hubs else '–ü–æ —É–º–æ–ª—á–∞–Ω–∏—é'}", "INFO")
    logger.log(f"–¢–µ–≥–∏: {', '.join(tags) if tags else '–ü–æ —É–º–æ–ª—á–∞–Ω–∏—é'}", "INFO")
    logger.log(f"Max —Å—Ç–∞—Ç–µ–π: {max_articles}, LLM: {'ON' if enable_llm else 'OFF'}, Dedup: {'ON' if enable_dedup else 'OFF'}", "INFO")

    # –°–æ–∑–¥–∞–µ–º –æ—á–µ—Ä–µ–¥–∏ –¥–ª—è –º–µ–∂–ø—Ä–æ—Ü–µ—Å—Å–Ω–æ–≥–æ –æ–±–º–µ–Ω–∞
    log_queue = multiprocessing.Queue()
    result_queue = multiprocessing.Queue()

    # –ó–∞–ø—É—Å–∫–∞–µ–º worker –ø—Ä–æ—Ü–µ—Å—Å
    process = multiprocessing.Process(
        target=habr_scraper_worker,
        args=(hubs, tags, max_articles, enable_llm, enable_dedup, log_queue, result_queue)
    )

    try:
        process.start()

        # –ò–Ω–¥–∏–∫–∞—Ç–æ—Ä –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ (–Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–π)
        progress = 0.0
        progress_direction = 0.01

        # –ß–∏—Ç–∞–µ–º –ª–æ–≥–∏ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏
        while process.is_alive() or not log_queue.empty():
            try:
                # –ü–æ–ª—É—á–∞–µ–º –ª–æ–≥ —Å —Ç–∞–π–º–∞—É—Ç–æ–º
                log_entry = log_queue.get(timeout=0.1)
                logger.log(log_entry['message'], log_entry['level'])
                log_placeholder.markdown("\n".join(st.session_state.parsing_logs))
            except Empty:
                pass

            # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä (–ø—É–ª—å—Å–∏—Ä—É—é—â–∏–π —ç—Ñ—Ñ–µ–∫—Ç)
            progress += progress_direction
            if progress >= 1.0 or progress <= 0.0:
                progress_direction *= -1
            progress_bar.progress(min(max(progress, 0.0), 1.0))
            status_text.info("üîÑ –ü–∞—Ä—Å–∏–Ω–≥ Habr –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ...")

            time.sleep(0.1)

        # –ñ–¥–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞
        process.join(timeout=5)

        # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        try:
            result = result_queue.get(timeout=1)
        except Empty:
            result = {
                'success': False,
                'error': 'Timeout: —Ä–µ–∑—É–ª—å—Ç–∞—Ç –Ω–µ –ø–æ–ª—É—á–µ–Ω',
                'saved': 0,
                'skipped': 0,
                'semantic_duplicates': 0,
                'editorial_processed': 0,
                'errors': 1
            }

        # –§–∏–Ω–∞–ª—å–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ª–æ–≥–æ–≤
        logger.log("="*60, "DEBUG")

        if result.get('success'):
            saved = result.get('saved', 0)
            skipped = result.get('skipped', 0)
            semantic_dups = result.get('semantic_duplicates', 0)
            editorial = result.get('editorial_processed', 0)
            errors = result.get('errors', 0)

            logger.log(f"üéâ –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à—ë–Ω!", "SUCCESS")
            logger.log(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {saved}", "SUCCESS")
            logger.log(f"–ü—Ä–æ–ø—É—â–µ–Ω–æ: {skipped}", "INFO")
            if semantic_dups > 0:
                logger.log(f"–î—É–±–ª–∏–∫–∞—Ç–æ–≤: {semantic_dups}", "INFO")
            if enable_llm and editorial > 0:
                logger.log(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ LLM: {editorial}", "SUCCESS")
            if errors > 0:
                logger.log(f"–û—à–∏–±–æ–∫: {errors}", "WARNING")

            progress_bar.progress(1.0)
            status_text.success(f"‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω–æ! –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {saved} —Å—Ç–∞—Ç–µ–π")
        else:
            error = result.get('error', 'Unknown error')
            logger.log(f"‚ùå –û—à–∏–±–∫–∞: {error}", "ERROR")
            progress_bar.progress(0.0)
            status_text.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞")

        log_placeholder.markdown("\n".join(st.session_state.parsing_logs))

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        st.session_state.habr_parsing_results = result

        return result

    # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ó–∞–º–µ–Ω–µ–Ω –≥–æ–ª—ã–π except –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π —Ç–∏–ø –¥–ª—è –ª—É—á—à–µ–π –æ—Ç–ª–∞–¥–∫–∏
    except Exception as e:
        logger.log(f"–ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: {str(e)}", "ERROR")
        status_text.error(f"‚ùå –û—à–∏–±–∫–∞: {str(e)}")
        log_placeholder.markdown("\n".join(st.session_state.parsing_logs))

        if process.is_alive():
            process.terminate()
            process.join()

        return {
            'success': False,
            'error': str(e),
            'saved': 0,
            'skipped': 0,
            'semantic_duplicates': 0,
            'editorial_processed': 0,
            'errors': 1
        }
    finally:
        # –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Ñ–ª–∞–≥–∏
        st.session_state.parsing_active = False
        st.session_state.parsing_in_progress = False

        # –ó–∞–∫—Ä—ã–≤–∞–µ–º —Å–µ—Å—Å–∏—é
        if st.session_state.get('log_manager') and session_id:
            try:
                st.session_state.log_manager.close_session(session_id)
            except Exception as e:
                logger.log(f"–û—à–∏–±–∫–∞ –∑–∞–∫—Ä—ã—Ç–∏—è —Å–µ—Å—Å–∏–∏: {e}", "WARNING")


def scrape_with_live_logs(subreddits: list[str], max_posts: int, sort_by: str, delay: int, enable_llm: bool) -> list[dict]:
    """–°–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ —Å –ª–æ–≥–∞–º–∏ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏."""
    logger = StreamlitLogger()

    # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é —Å–µ—Å—Å–∏—é
    if st.session_state.get('log_manager'):
        session_id = st.session_state.log_manager.create_session()
        st.session_state.current_session_id = session_id
    else:
        # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: `uuid` —Ç–µ–ø–µ—Ä—å –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω, –æ—à–∏–±–∫–∏ –Ω–µ –±—É–¥–µ—Ç
        session_id = str(uuid.uuid4())

    st.session_state.log_session_counter += 1
    logger.add_separator(st.session_state.log_session_counter)

    # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ñ–ª–∞–≥ –∞–∫—Ç–∏–≤–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞
    st.session_state.parsing_active = True
    st.session_state.parsing_in_progress = True

    progress_bar = st.progress(0)
    status_text = st.empty()
    log_container = st.expander("üìã **–õ–æ–≥–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞**", expanded=True)
    log_placeholder = log_container.empty()

    results = []
    total_subs = len(subreddits)
    settings = st.session_state.settings
    logger.log(f"üöÄ –ù–∞—á–∞–ª–æ –ø–∞—Ä—Å–∏–Ω–≥–∞: {total_subs} subreddits", "INFO")
    logger.log(f"–°–µ—Å—Å–∏—è: {session_id[:8]}...", "DEBUG")
    logger.log(f"–ù–∞—Å—Ç—Ä–æ–π–∫–∏: max_posts={max_posts}, sort={sort_by}, LLM={'ON' if enable_llm else 'OFF'}", "DEBUG")
    logger.log(f"LLM: {settings['llm_model']}, temp={settings['llm_temperature']}", "DEBUG")

    try:
        logger.log("–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Reddit API...", "INFO")
        reddit = get_reddit_client()
        logger.log("‚úì Reddit API –ø–æ–¥–∫–ª—é—á–µ–Ω", "SUCCESS")

        for idx, sub in enumerate(subreddits, 1):
            progress = idx / total_subs
            progress_bar.progress(progress)
            status_text.info(f"üî• –ü–∞—Ä—Å–∏–Ω–≥ **r/{sub}** ({idx}/{total_subs})")

            logger.log(f"{'='*60}", "DEBUG")
            logger.log(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ r/{sub} [{idx}/{total_subs}]", "INFO")
            log_placeholder.markdown("\n".join(st.session_state.parsing_logs))

            try:
                result = scrape_subreddit(
                    subreddit_name=sub,
                    max_posts=max_posts,
                    sort_by=sort_by,
                    enable_llm=enable_llm,
                    log_callback=lambda msg, lvl: logger.log(msg, lvl)
                )
                results.append(result)

                if result.get('success'):
                    saved = result.get('saved', 0)
                    skipped = result.get('skipped', 0)
                    semantic_dups = result.get('semantic_duplicates', 0)
                    editorial = result.get('editorial_processed', 0)
                    msg = f"r/{sub}: —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ {saved}, –ø—Ä–æ–ø—É—â–µ–Ω–æ {skipped}"
                    if semantic_dups > 0:
                        msg += f", –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ {semantic_dups}"
                    if enable_llm and editorial > 0:
                        msg += f", –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ LLM {editorial}"
                    logger.log(msg, "SUCCESS")
                else:
                    error = result.get('error', 'Unknown error')
                    logger.log(f"r/{sub}: –æ—à–∏–±–∫–∞ - {error}", "ERROR")
            except Exception as e:
                logger.log(f"r/{sub}: –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ - {str(e)}", "ERROR")
                results.append({'success': False, 'subreddit': sub, 'error': str(e)})

            log_placeholder.markdown("\n".join(st.session_state.parsing_logs))

            if idx < total_subs:
                logger.log(f"‚è≥ –û–∂–∏–¥–∞–Ω–∏–µ {delay} —Å–µ–∫...", "DEBUG")
                time.sleep(delay)

        logger.log(f"{'='*60}", "DEBUG")
        total_saved = sum(r.get('saved', 0) for r in results if r.get('success'))
        total_semantic = sum(r.get('semantic_duplicates', 0) for r in results if r.get('success'))
        total_editorial = sum(r.get('editorial_processed', 0) for r in results if r.get('success'))
        success_count = sum(1 for r in results if r.get('success'))

        logger.log(f"üéâ –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à—ë–Ω!", "SUCCESS")
        logger.log(f"–£—Å–ø–µ—à–Ω–æ: {success_count}/{total_subs} subreddits", "INFO")
        logger.log(f"–í—Å–µ–≥–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {total_saved} –ø–æ—Å—Ç–æ–≤", "SUCCESS")
        if total_semantic > 0:
            logger.log(f"–î—É–±–ª–∏–∫–∞—Ç–æ–≤ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ: {total_semantic}", "INFO")
        if enable_llm and total_editorial > 0:
            logger.log(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ LLM: {total_editorial}", "SUCCESS")

        progress_bar.progress(1.0)
        status_text.success(f"‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω–æ! –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {total_saved} –ø–æ—Å—Ç–æ–≤")
        log_placeholder.markdown("\n".join(st.session_state.parsing_logs))

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        st.session_state.parsing_results = results

        return results

    # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ó–∞–º–µ–Ω–µ–Ω –≥–æ–ª—ã–π except –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π —Ç–∏–ø –¥–ª—è –ª—É—á—à–µ–π –æ—Ç–ª–∞–¥–∫–∏
    except Exception as e:
        logger.log(f"–ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: {str(e)}", "ERROR")
        status_text.error(f"‚ùå –û—à–∏–±–∫–∞: {str(e)}")
        log_placeholder.markdown("\n".join(st.session_state.parsing_logs))
        return []
    finally:
        # –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Ñ–ª–∞–≥–∏
        st.session_state.parsing_active = False
        st.session_state.parsing_in_progress = False

        # –ó–∞–∫—Ä—ã–≤–∞–µ–º —Å–µ—Å—Å–∏—é
        if st.session_state.get('log_manager') and session_id:
            try:
                st.session_state.log_manager.close_session(session_id)
            except Exception as e:
                logger.log(f"–û—à–∏–±–∫–∞ –∑–∞–∫—Ä—ã—Ç–∏—è —Å–µ—Å—Å–∏–∏: {e}", "WARNING")

# –ò–°–ü–†–ê–í–õ–ï–ù–û: –§—É–Ω–∫—Ü–∏—è –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ UI, –∑–∞–∫–æ–º–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∞ –¥–ª—è —á–∏—Å—Ç–æ—Ç—ã –∫–æ–¥–∞.
# –ï—Å–ª–∏ –≤ –±—É–¥—É—â–µ–º –ø–æ—Ç—Ä–µ–±—É–µ—Ç—Å—è –µ—ë –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å, –º–æ–∂–Ω–æ –±—É–¥–µ—Ç —Å–æ–∑–¥–∞—Ç—å –∫–Ω–æ–ø–∫—É –∏ –≤—ã–∑–≤–∞—Ç—å –µ—ë.
# def process_posts_with_live_logs(unprocessed_posts: list) -> dict:
#     """–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø–æ—Å—Ç–æ–≤ —á–µ—Ä–µ–∑ LLM —Å –∂–∏–≤—ã–º–∏ –ª–æ–≥–∞–º–∏."""
#     # ... (–∫–æ–¥ —Ñ—É–Ω–∫—Ü–∏–∏) ...
#     pass

def format_timedelta(td, lang='ru') -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞."""
    total_seconds = int(td.total_seconds())
    if total_seconds < 60:
        return f"{total_seconds} {t('sec')} {t('ago')}"
    elif total_seconds < 3600:
        return f"{total_seconds // 60} {t('min')} {t('ago')}"
    elif total_seconds < 86400:
        return f"{total_seconds // 3600} {t('hour')} {t('ago')}"
    else:
        days = total_seconds // 86400
        return f"{days} {t('days')} {t('ago')}"

def count_words(text: str) -> int:
    """–ü–æ–¥—Å—á–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å–ª–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ."""
    if not text:
        return 0
    return len(text.split())

# ============================================================================
# –§–£–ù–ö–¶–ò–ò –ö–û–ù–í–ï–†–¢–ê–¶–ò–ò –û–ë–™–ï–ö–¢–û–í –í –°–õ–û–í–ê–†–¨
# ============================================================================
# –≠—Ç–∏ —Ñ—É–Ω–∫—Ü–∏–∏ —Ä–µ—à–∞—é—Ç –ø—Ä–æ–±–ª–µ–º—É DetachedInstanceError, –ø—Ä–µ–æ–±—Ä–∞–∑—É—è ORM-–æ–±—ä–µ–∫—Ç—ã
# –≤ –æ–±—ã—á–Ω—ã–µ —Å–ª–æ–≤–∞—Ä–∏ Python, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–∂–Ω–æ –±–µ–∑–æ–ø–∞—Å–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤–Ω–µ —Å–µ—Å—Å–∏–∏.

def _reddit_post_to_dict(post: RedditPost) -> dict:
    """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç RedditPost –≤ —Å–ª–æ–≤–∞—Ä—å."""
    return {
        'id': post.id,
        'post_id': post.post_id,
        'title': post.title,
        'selftext': post.selftext,
        'url': post.url,
        'author': post.author,
        'subreddit': post.subreddit,
        'score': post.score,
        'num_comments': post.num_comments,
        'created_utc': post.created_utc.isoformat() if post.created_utc else None,
        'scraped_at': post.scraped_at.isoformat() if post.scraped_at else None,
        'qdrant_id': str(post.qdrant_id) if post.qdrant_id else None,
    }

def _processed_reddit_post_to_dict(post: ProcessedRedditPost, raw_post_data: dict = None) -> dict:
    """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç ProcessedRedditPost –≤ —Å–ª–æ–≤–∞—Ä—å."""
    return {
        'id': post.id,
        'post_id': post.post_id,
        'original_title': post.original_title,
        'original_text': post.original_text,
        'subreddit': post.subreddit,
        'author': post.author,
        'url': post.url,
        'score': post.score,
        'is_news': post.is_news,
        'original_summary': post.original_summary,
        'rewritten_post': post.rewritten_post,
        'title': post.title,
        'teaser': post.teaser,
        'image_prompt': post.image_prompt,
        'processed_at': post.processed_at.isoformat() if post.processed_at else None,
        'processing_time': post.processing_time,
        'model_used': post.model_used,
        'raw_post': raw_post_data # –í–∫–ª—é—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ –ø–æ—Å—Ç–∞
    }

def _habr_article_to_dict(article: HabrArticle) -> dict:
    """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç HabrArticle –≤ —Å–ª–æ–≤–∞—Ä—å."""
    return {
        'id': article.id,
        'article_id': article.article_id,
        'title': article.title,
        'content': article.content,
        'url': article.url,
        'author': article.author,
        'description': article.description,
        'categories': article.categories,
        'pub_date': article.pub_date.isoformat() if article.pub_date else None,
        'scraped_at': article.scraped_at.isoformat() if article.scraped_at else None,
        'reading_time': article.reading_time,
        'views': article.views,
        'rating': article.rating,
        'original_title': article.original_title,
        'original_content': article.original_content,
        'rewritten_post': article.rewritten_post,
        'teaser': article.teaser,
        'image_prompt': article.image_prompt,
        'is_news': article.is_news,
        'editorial_processed': article.editorial_processed,
        'telegram_title': article.telegram_title,
        'telegram_content': article.telegram_content,
        'telegram_hashtags': article.telegram_hashtags,
        'telegram_formatted': article.telegram_formatted,
        'telegram_character_count': article.telegram_character_count,
        'telegram_processed': article.telegram_processed,
        'language': article.language,
        'word_count': article.word_count,
        'reading_time_calculated': article.reading_time_calculated,
        'sentiment': article.sentiment,
        'keywords': article.keywords,
        'summary': article.summary,
        'difficulty_level': article.difficulty_level,
        'relevance_score': article.relevance_score,
        'processing_version': article.processing_version,
        'last_updated': article.last_updated.isoformat() if article.last_updated else None,
        'qdrant_id': str(article.qdrant_id) if article.qdrant_id else None,
        'images': article.images,
    }

def _telegram_post_to_dict(post: TelegramPost) -> dict:
    """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç TelegramPost –≤ —Å–ª–æ–≤–∞—Ä—å."""
    return {
        'id': post.id,
        'article_id': post.article_id,
        'content': post.content,
        'title': post.title,
        'hashtags': post.hashtags,
        'formatted_content': post.formatted_content,
        'character_count': post.character_count,
        'created_at': post.created_at.isoformat() if post.created_at else None,
        'published_at': post.published_at.isoformat() if post.published_at else None,
        'telegram_message_id': post.telegram_message_id,
        'is_published': post.is_published,
    }

# ============================================================================
# –§–£–ù–ö–¶–ò–ò-–•–ï–õ–ü–ï–†–´ –î–õ–Ø –†–ê–ë–û–¢–´ –° –î–ê–¢–ê–ú–ò
# ============================================================================

def _parse_iso_to_utc(dt_str: str | None) -> datetime | None:
    """
    –ë–µ–∑–æ–ø–∞—Å–Ω–æ –ø–∞—Ä—Å–∏—Ç ISO-—Å—Ç—Ä–æ–∫—É –≤ UTC-aware datetime-–æ–±—ä–µ–∫—Ç.
    –ï—Å–ª–∏ —Å—Ç—Ä–æ–∫–∞ –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç timezone, –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç—Å—è –º–æ—Å–∫–æ–≤—Å–∫–æ–µ –≤—Ä–µ–º—è (UTC+3).
    """
    if not dt_str:
        return None
    dt = datetime.fromisoformat(dt_str)
    if dt.tzinfo is None:
        # –ï—Å–ª–∏ datetime –Ω–∞–∏–≤–Ω—ã–π, –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º –º–æ—Å–∫–æ–≤—Å–∫–æ–µ –≤—Ä–µ–º—è
        moscow_tz = timezone(timedelta(hours=3))
        dt = dt.replace(tzinfo=moscow_tz)
    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ UTC –¥–ª—è –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏
    return dt.astimezone(timezone.utc)


# ============================================================================
# –§–£–ù–ö–¶–ò–ò –†–ï–ù–î–ï–†–ò–ù–ì–ê (–ò–°–ü–†–ê–í–õ–ï–ù–´ –î–õ–Ø –†–ê–ë–û–¢–´ –°–û –°–õ–û–í–ê–†–Ø–ú–ò)
# ============================================================================

def render_raw_post_viewer(post_data: dict, lang='ru') -> None:
    """–†–µ–Ω–¥–µ—Ä–∏–Ω–≥ —Å—ã—Ä–æ–≥–æ Reddit –ø–æ—Å—Ç–∞ —Å –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π."""
    has_vector = post_data.get('qdrant_id') is not None
    vector_badge = t('vectorized') if has_vector else t('no_vector')

    now = datetime.now(timezone.utc)
    created_utc_str = post_data.get('created_utc')
    scraped_at_str = post_data.get('scraped_at')

    created_time = datetime.fromisoformat(created_utc_str) if created_utc_str else now
    scraped_time = datetime.fromisoformat(scraped_at_str) if scraped_at_str else now

    time_since_created = now - created_time
    time_since_scraped = now - scraped_time

    with st.expander(f"r/{post_data.get('subreddit')} ‚Ä¢ {post_data.get('title', '')[:80]}"):
        col_badge1, col_badge2 = st.columns([1, 1], gap="small")
        with col_badge1:
            st.caption(vector_badge)
        with col_badge2:
            if has_vector:
                st.caption(f"`{post_data.get('qdrant_id', '')[:8]}...`")

        st.markdown("---")

        col_time1, col_time2 = st.columns(2, gap="small")
        with col_time1:
            st.markdown(f"**{t('published_reddit')}**")
            st.info(f"{created_time.strftime('%Y-%m-%d %H:%M:%S UTC')}")
            st.caption(format_timedelta(time_since_created, lang))
        with col_time2:
            st.markdown(f"**{t('received_db')}**")
            st.success(f"{scraped_time.strftime('%Y-%m-%d %H:%M:%S UTC')}")
            st.caption(format_timedelta(time_since_scraped, lang))

        st.markdown("---")

        col_a, col_b = st.columns([2, 1], gap="small")

        with col_a:
            st.markdown(f"**{t('original_title')}**")
            st.write(post_data.get('title', ''))

            st.markdown(f"**{t('original_text')}**")
            selftext = post_data.get('selftext', '')
            if selftext:
                st.text_area(
                    t('post_text'),
                    selftext,
                    height=200,
                    key=f"raw_{post_data.get('id')}",
                    label_visibility="collapsed"
                )
            else:
                st.caption(f"_{t('text_missing')}_")

        with col_b:
            st.metric(t('score'), post_data.get('score', 0))
            st.metric(t('comments'), post_data.get('num_comments', 0))
            st.caption(f"**{t('author')}** u/{post_data.get('author', '')}")

            if has_vector:
                st.success(t('in_qdrant'))
                with st.expander(t('qdrant_uuid')):
                    st.code(post_data.get('qdrant_id', ''))
            else:
                st.warning(t('no_vector'))

            if post_data.get('url'):
                st.link_button(t('open_original'), post_data.get('url'))


def render_processed_post_viewer(post_data: dict, lang='ru') -> None:
    """–†–µ–Ω–¥–µ—Ä–∏–Ω–≥ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–≥–æ –ø–æ—Å—Ç–∞ —Å –≤–∫–ª–∞–¥–∫–∞–º–∏."""
    status_icon = "üì∞" if post_data.get('is_news') else "‚ùå"
    raw_post_data = post_data.get('raw_post', {})

    # –ò—Å–ø–æ–ª—å–∑—É–µ–º teaser –∫–∞–∫ –æ—Å–Ω–æ–≤–Ω–æ–π –∑–∞–≥–æ–ª–æ–≤–æ–∫, –µ—Å–ª–∏ –µ—Å—Ç—å
    if post_data.get('is_news') and post_data.get('teaser'):
        title_display = post_data.get('teaser', '')[:80]
    elif post_data.get('title'):
        title_display = post_data.get('title', '')[:80]
    else:
        title_display = post_data.get('original_title', '')[:80]

    has_vector = raw_post_data.get('qdrant_id') is not None

    now = datetime.now(timezone.utc)

    with st.expander(f"{status_icon} r/{post_data.get('subreddit')} ‚Ä¢ {title_display}"):
        # Badges - –∫–æ–º–ø–∞–∫—Ç–Ω–µ–µ
        col_badge1, col_badge2, col_badge3, col_badge4 = st.columns([1, 1, 1, 1], gap="small")
        with col_badge1:
            if post_data.get('is_news'):
                st.success("‚úÖ News")
            else:
                st.error("‚ùå Not News")
        with col_badge2:
            if has_vector:
                st.info("ü§ñ Vector")
            else:
                st.warning("‚ö†Ô∏è No Vec")
        with col_badge3:
            st.caption(f"‚ö° {post_data.get('processing_time', 0)}ms")
        with col_badge4:
            st.caption(f"ü§ñ {post_data.get('model_used', 'gpt-oss')}")

        st.markdown("---")

        # Timeline - –∫–æ–º–ø–∞–∫—Ç–Ω–µ–µ
        if raw_post_data:
            col_timeline1, col_timeline2, col_timeline3 = st.columns(3, gap="small")

            # –ò–°–ü–û–õ–¨–ó–£–ï–ú –•–ï–õ–ü–ï–† –î–õ–Ø –ë–ï–ó–û–ü–ê–°–ù–û–ì–û –ü–ê–†–°–ò–ù–ì–ê –î–ê–¢
            created_time = _parse_iso_to_utc(raw_post_data.get('created_utc')) or now
            scraped_time = _parse_iso_to_utc(raw_post_data.get('scraped_at')) or now
            processed_time = _parse_iso_to_utc(post_data.get('processed_at')) or now

            with col_timeline1:
                st.markdown("**üìÖ –û–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ**")
                st.info(f"{created_time.strftime('%Y-%m-%d %H:%M')}")
                st.caption(format_timedelta(now - created_time, lang))
            with col_timeline2:
                st.markdown("**üíæ –ü–æ–ª—É—á–µ–Ω–æ**")
                st.success(f"{scraped_time.strftime('%Y-%m-%d %H:%M')}")
                st.caption(format_timedelta(now - scraped_time, lang))
            with col_timeline3:
                st.markdown("**ü§ñ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ**")
                st.warning(f"{processed_time.strftime('%Y-%m-%d %H:%M')}")
                st.caption(format_timedelta(now - processed_time, lang))

        st.markdown("---")

        if post_data.get('is_news'):
            tab_original, tab_llm, tab_meta = st.tabs(["üìÑ –û—Ä–∏–≥–∏–Ω–∞–ª", "ü§ñ LLM Output", "üìä –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ"])

            with tab_original:
                st.markdown("### üìå –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫")
                st.info(post_data.get('original_title', ''))

                st.markdown("### üìù –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç")
                original_text = post_data.get('original_text', '')
                if original_text:
                    full_original = f"{post_data.get('original_title', '')}\n\n{original_text}"
                    st.text_area(
                        "–ü–æ–ª–Ω—ã–π –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç",
                        full_original,
                        height=400,
                        key=f"orig_full_{post_data.get('id')}",
                        label_visibility="collapsed"
                    )
                    st.caption(f"üìè –î–ª–∏–Ω–∞: {len(full_original)} —Å–∏–º–≤–æ–ª–æ–≤ | –°–ª–æ–≤: {count_words(full_original)}")
                else:
                    st.caption("_–¢–µ–∫—Å—Ç –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç_")

            with tab_llm:
                # Teaser –∫–∞–∫ –æ—Å–Ω–æ–≤–Ω–æ–π –∑–∞–≥–æ–ª–æ–≤–æ–∫
                st.markdown("### ‚ú® –ó–∞–≥–æ–ª–æ–≤–æ–∫ (Teaser)")
                if post_data.get('teaser'):
                    st.success(f"**{post_data.get('teaser')}**")
                else:
                    st.caption("_–ù–µ —Å–æ–∑–¥–∞–Ω_")

                st.markdown("### üì∞ –†–µ–¥–∞–∫—Ç–æ—Ä—Å–∫–∏–π –∑–∞–≥–æ–ª–æ–≤–æ–∫")
                if post_data.get('title'):
                    st.info(post_data.get('title'))
                else:
                    st.caption("_–ù–µ —Å–æ–∑–¥–∞–Ω_")

                st.markdown("### ‚úèÔ∏è –ü–µ—Ä–µ–ø–∏—Å–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç (LLM Output)")
                rewritten_post = post_data.get('rewritten_post', '')
                if rewritten_post:
                    st.text_area(
                        "–ü–æ–ª–Ω—ã–π –ø–µ—Ä–µ–ø–∏—Å–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –æ—Ç LLM",
                        rewritten_post,
                        height=400,
                        key=f"llm_full_{post_data.get('id')}",
                        label_visibility="collapsed"
                    )

                    # –î–æ–±–∞–≤–ª–µ–Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Å–ª–æ–≤–∞–º
                    original_text = f"{post_data.get('original_title', '')}\n\n{post_data.get('original_text', '')}"
                    original_len = len(original_text)
                    original_words = count_words(original_text)
                    llm_len = len(rewritten_post)
                    llm_words = count_words(rewritten_post)

                    diff_chars = llm_len - original_len
                    diff_words = llm_words - original_words
                    diff_pct = (diff_chars / original_len * 100) if original_len > 0 else 0

                    col_stat1, col_stat2, col_stat3, col_stat4 = st.columns(4, gap="small")
                    with col_stat1:
                        st.metric("–û—Ä–∏–≥–∏–Ω–∞–ª", f"{original_words} —Å–ª–æ–≤")
                        st.caption(f"{original_len} —Å–∏–º–≤–æ–ª–æ–≤")
                    with col_stat2:
                        st.metric("LLM Output", f"{llm_words} —Å–ª–æ–≤")
                        st.caption(f"{llm_len} —Å–∏–º–≤–æ–ª–æ–≤")
                    with col_stat3:
                        st.metric("Œî –°–ª–æ–≤", f"{diff_words:+d}")
                    with col_stat4:
                        st.metric("Œî –°–∏–º–≤–æ–ª–æ–≤", f"{diff_chars:+d} ({diff_pct:+.1f}%)")
                else:
                    st.warning("_–ü–µ—Ä–µ–ø–∏—Å–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –Ω–µ —Å–æ–∑–¥–∞–Ω_")

                # –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω –ø—Ä–æ–º–ø—Ç
                st.markdown("### üé® –ü—Ä–æ–º–ø—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è")
                if post_data.get('image_prompt'):
                    st.code(post_data.get('image_prompt'), language="text")
                else:
                    st.caption("_–ù–µ —Å–æ–∑–¥–∞–Ω_")

                st.markdown("### üìã –ö—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ (Summary)")
                if post_data.get('original_summary'):
                    with st.expander("–ü–æ–∫–∞–∑–∞—Ç—å summary"):
                        st.write(post_data.get('original_summary'))
                else:
                    st.caption("_–ù–µ —Å–æ–∑–¥–∞–Ω–æ_")

            with tab_meta:
                st.markdown("### üìä –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏")

                # –£–º–µ–Ω—å—à–µ–Ω gap
                col_m1, col_m2 = st.columns(2, gap="small")

                with col_m1:
                    st.markdown("**ü§ñ –ú–æ–¥–µ–ª—å**")
                    st.info(post_data.get('model_used', 'gpt-oss'))

                    st.markdown("**‚ö° –í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏**")
                    processing_time = post_data.get('processing_time', 0)
                    st.info(f"{processing_time}ms ({processing_time / 1000:.2f}s)")

                    processed_at_str = post_data.get('processed_at')
                    st.markdown("**üìÖ –î–∞—Ç–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏**")
                    st.info(datetime.fromisoformat(processed_at_str).strftime('%Y-%m-%d %H:%M:%S UTC') if processed_at_str else 'N/A')

                with col_m2:
                    st.markdown("**üì∞ –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è**")
                    if post_data.get('is_news'):
                        st.success("‚úÖ –ù–æ–≤–æ—Å—Ç—å")
                    else:
                        st.error("‚ùå –ù–µ –Ω–æ–≤–æ—Å—Ç—å")

                    st.markdown("**üéØ –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è**")
                    if has_vector:
                        st.success("‚úÖ –í Qdrant")
                        if raw_post_data.get('qdrant_id'):
                            st.code(raw_post_data.get('qdrant_id'), language="text")
                    else:
                        st.warning("‚ö†Ô∏è –ù–µ –≤–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω")

                    st.markdown("**‚¨ÜÔ∏è Score**")
                    st.info(f"{post_data.get('score', 0)} upvotes")

            st.markdown("---")
            if post_data.get('url'):
                st.link_button("üîó –û—Ç–∫—Ä—ã—Ç—å –æ—Ä–∏–≥–∏–Ω–∞–ª –Ω–∞ Reddit", post_data.get('url'))
        else:
            st.warning("**‚ùå –ù–µ —è–≤–ª—è–µ—Ç—Å—è –Ω–æ–≤–æ—Å—Ç—å—é**")
            st.caption(f"**–û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫:** {post_data.get('original_title', '')}")
            st.caption(f"**Subreddit:** r/{post_data.get('subreddit', '')}")
            st.caption(f"**–ê–≤—Ç–æ—Ä:** u/{post_data.get('author', '')}")

            if post_data.get('original_text'):
                with st.expander("üìÑ –ü–æ–∫–∞–∑–∞—Ç—å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç"):
                    st.text_area(
                        "–û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç",
                        post_data.get('original_text', ''),
                        height=200,
                        key=f"not_news_{post_data.get('id')}",
                        label_visibility="collapsed"
                    )


def render_habr_article_viewer(article_data: dict, lang='ru') -> None:
    """–†–µ–Ω–¥–µ—Ä–∏–Ω–≥ —Å—Ç–∞—Ç—å–∏ —Å Habr."""
    has_vector = article_data.get('qdrant_id') is not None
    is_processed = article_data.get('editorial_processed')
    is_news = article_data.get('is_news')

    # –ò–∫–æ–Ω–∫–∞ —Å—Ç–∞—Ç—É—Å–∞
    if is_news:
        status_icon = "üì∞"
    elif is_processed:
        status_icon = "ü§ñ"
    else:
        status_icon = "üìÑ"

    # –ó–∞–≥–æ–ª–æ–≤–æ–∫ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
    if is_news and article_data.get('teaser'):
        title_display = article_data.get('teaser', '')[:80]
    elif article_data.get('title'):
        title_display = article_data.get('title', '')[:80]
    else:
        title_display = article_data.get('title', '')[:80]

    now = datetime.now(timezone.utc)

    with st.expander(f"{status_icon} Habr ‚Ä¢ {title_display}"):
        # Badges
        col_badge1, col_badge2, col_badge3 = st.columns([1, 1, 1], gap="small")
        with col_badge1:
            if is_news:
                st.success("‚úÖ News")
            elif is_processed:
                st.info("ü§ñ Processed")
            else:
                st.warning("üìÑ Raw")
        with col_badge2:
            if has_vector:
                st.info("ü§ñ Vector")
            else:
                st.warning("‚ö†Ô∏è No Vec")
        with col_badge3:
            if article_data.get('rating') is not None:
                st.caption(f"‚≠ê {article_data.get('rating')}")
            else:
                st.caption("‚≠ê N/A")

        st.markdown("---")

        # Timeline
        col_time1, col_time2 = st.columns(2, gap="small")
        with col_time1:
            st.markdown("**üìÖ –û–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ –Ω–∞ Habr**")

            # –ò–°–ü–û–õ–¨–ó–£–ï–ú –•–ï–õ–ü–ï–† –î–õ–Ø –ë–ï–ó–û–ü–ê–°–ù–û–ì–û –ü–ê–†–°–ò–ù–ì–ê –î–ê–¢
            pub_time = _parse_iso_to_utc(article_data.get('pub_date'))
            if pub_time:
                st.info(f"{pub_time.strftime('%Y-%m-%d %H:%M')}")
                st.caption(format_timedelta(now - pub_time, lang))
            else:
                st.caption("_–î–∞—Ç–∞ –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–∞_")

        with col_time2:
            st.markdown("**üíæ –ü–æ–ª—É—á–µ–Ω–æ –≤ –ë–î**")

            # –ò–°–ü–û–õ–¨–ó–£–ï–ú –•–ï–õ–ü–ï–† –î–õ–Ø –ë–ï–ó–û–ü–ê–°–ù–û–ì–û –ü–ê–†–°–ò–ù–ì–ê –î–ê–¢
            scraped_time = _parse_iso_to_utc(article_data.get('scraped_at')) or now
            st.success(f"{scraped_time.strftime('%Y-%m-%d %H:%M')}")
            st.caption(format_timedelta(now - scraped_time, lang))

        st.markdown("---")

        # –ö–æ–Ω—Ç–µ–Ω—Ç
        if is_news:
            tab_original, tab_llm, tab_meta = st.tabs(["üìÑ –û—Ä–∏–≥–∏–Ω–∞–ª", "ü§ñ LLM Output", "üìä –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ"])

            with tab_original:
                st.markdown("### üìå –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫")
                st.info(article_data.get('title', ''))

                st.markdown("### üìù –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç")
                content = article_data.get('content', '')
                if content:
                    st.text_area(
                        "–ü–æ–ª–Ω—ã–π –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç",
                        content,
                        height=400,
                        key=f"habr_orig_{article_data.get('id')}",
                        label_visibility="collapsed"
                    )
                    st.caption(f"üìè –î–ª–∏–Ω–∞: {len(content)} —Å–∏–º–≤–æ–ª–æ–≤ | –°–ª–æ–≤: {count_words(content)}")
                else:
                    st.caption("_–¢–µ–∫—Å—Ç –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç_")

            with tab_llm:
                st.markdown("### ‚ú® –ó–∞–≥–æ–ª–æ–≤–æ–∫ (Teaser)")
                if article_data.get('teaser'):
                    st.success(f"**{article_data.get('teaser')}**")
                else:
                    st.caption("_–ù–µ —Å–æ–∑–¥–∞–Ω_")

                st.markdown("### üì∞ –†–µ–¥–∞–∫—Ç–æ—Ä—Å–∫–∏–π –∑–∞–≥–æ–ª–æ–≤–æ–∫")
                if article_data.get('title'):
                    st.info(article_data.get('title'))
                else:
                    st.caption("_–ù–µ —Å–æ–∑–¥–∞–Ω_")

                st.markdown("### ‚úèÔ∏è –ü–µ—Ä–µ–ø–∏—Å–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç (LLM Output)")
                rewritten_post = article_data.get('rewritten_post', '')
                if rewritten_post:
                    st.text_area(
                        "–ü–æ–ª–Ω—ã–π –ø–µ—Ä–µ–ø–∏—Å–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –æ—Ç LLM",
                        rewritten_post,
                        height=400,
                        key=f"habr_llm_{article_data.get('id')}",
                        label_visibility="collapsed"
                    )

                    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
                    original_len = len(article_data.get('content', ''))
                    original_words = count_words(article_data.get('content', ''))
                    llm_len = len(rewritten_post)
                    llm_words = count_words(rewritten_post)

                    diff_chars = llm_len - original_len
                    diff_words = llm_words - original_words
                    diff_pct = (diff_chars / original_len * 100) if original_len > 0 else 0

                    col_stat1, col_stat2, col_stat3, col_stat4 = st.columns(4, gap="small")
                    with col_stat1:
                        st.metric("–û—Ä–∏–≥–∏–Ω–∞–ª", f"{original_words} —Å–ª–æ–≤")
                        st.caption(f"{original_len} —Å–∏–º–≤–æ–ª–æ–≤")
                    with col_stat2:
                        st.metric("LLM Output", f"{llm_words} —Å–ª–æ–≤")
                        st.caption(f"{llm_len} —Å–∏–º–≤–æ–ª–æ–≤")
                    with col_stat3:
                        st.metric("Œî –°–ª–æ–≤", f"{diff_words:+d}")
                    with col_stat4:
                        st.metric("Œî –°–∏–º–≤–æ–ª–æ–≤", f"{diff_chars:+d} ({diff_pct:+.1f}%)")
                else:
                    st.warning("_–ü–µ—Ä–µ–ø–∏—Å–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –Ω–µ —Å–æ–∑–¥–∞–Ω_")

                st.markdown("### üé® –ü—Ä–æ–º–ø—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è")
                if article_data.get('image_prompt'):
                    st.code(article_data.get('image_prompt'), language="text")
                else:
                    st.caption("_–ù–µ —Å–æ–∑–¥–∞–Ω_")

            with tab_meta:
                st.markdown("### üìä –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å—Ç–∞—Ç—å–∏")

                col_m1, col_m2 = st.columns(2, gap="small")

                with col_m1:
                    st.markdown("**‚úçÔ∏è –ê–≤—Ç–æ—Ä**")
                    st.info(article_data.get('author', 'Unknown'))

                    st.markdown("**üìö –ö–∞—Ç–µ–≥–æ—Ä–∏–∏**")
                    if article_data.get('categories'):
                        st.info(article_data.get('categories'))
                    else:
                        st.caption("_–ù–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏–π_")

                    st.markdown("**‚è±Ô∏è –í—Ä–µ–º—è —á—Ç–µ–Ω–∏—è**")
                    if article_data.get('reading_time'):
                        st.info(f"{article_data.get('reading_time')} –º–∏–Ω")
                    else:
                        st.caption("_–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ_")

                with col_m2:
                    st.markdown("**üëÅÔ∏è –ü—Ä–æ—Å–º–æ—Ç—Ä—ã**")
                    if article_data.get('views') is not None:
                        st.info(f"{article_data.get('views'):,}")
                    else:
                        st.caption("_–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ_")

                    st.markdown("**‚≠ê –†–µ–π—Ç–∏–Ω–≥**")
                    if article_data.get('rating') is not None:
                        st.info(f"{article_data.get('rating')}")
                    else:
                        st.caption("_–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ_")

                    st.markdown("**üéØ –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è**")
                    if has_vector:
                        st.success("‚úÖ –í Qdrant")
                        st.code(article_data.get('qdrant_id'), language="text")
                    else:
                        st.warning("‚ö†Ô∏è –ù–µ –≤–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω")

            st.markdown("---")
            if article_data.get('url'):
                st.link_button("üîó –û—Ç–∫—Ä—ã—Ç—å –æ—Ä–∏–≥–∏–Ω–∞–ª –Ω–∞ Habr", article_data.get('url'))
        else:
            # –î–ª—è –Ω–µ–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö/–Ω–µ-–Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π
            col_a, col_b = st.columns([2, 1], gap="small")

            with col_a:
                st.markdown("### üìå –ó–∞–≥–æ–ª–æ–≤–æ–∫")
                st.info(article_data.get('title', ''))

                st.markdown("### üìù –ö–æ–Ω—Ç–µ–Ω—Ç")
                content = article_data.get('content', '')
                if content:
                    st.text_area(
                        "–ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç",
                        content,
                        height=300,
                        key=f"habr_raw_{article_data.get('id')}",
                        label_visibility="collapsed"
                    )
                    st.caption(f"üìè –î–ª–∏–Ω–∞: {len(content)} —Å–∏–º–≤–æ–ª–æ–≤")
                else:
                    st.caption("_–¢–µ–∫—Å—Ç –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç_")

            with col_b:
                st.markdown("**‚úçÔ∏è –ê–≤—Ç–æ—Ä**")
                st.caption(article_data.get('author', 'Unknown'))

                if article_data.get('rating') is not None:
                    st.metric("‚≠ê –†–µ–π—Ç–∏–Ω–≥", article_data.get('rating'))

                if article_data.get('views') is not None:
                    st.metric("üëÅÔ∏è –ü—Ä–æ—Å–º–æ—Ç—Ä—ã", f"{article_data.get('views'):,}")

                if article_data.get('reading_time'):
                    st.metric("‚è±Ô∏è –ß—Ç–µ–Ω–∏–µ", f"{article_data.get('reading_time')} –º–∏–Ω")

                if has_vector:
                    st.success("‚úÖ –í–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–æ")
                else:
                    st.warning("‚ö†Ô∏è –ù–µ –≤–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–æ")

                if article_data.get('url'):
                    st.link_button("üîó –û—Ç–∫—Ä—ã—Ç—å", article_data.get('url'))

def render_telegram_post_viewer(post_data: dict, lang='ru') -> None:
    """
    –†–µ–Ω–¥–µ—Ä–∏–Ω–≥ Telegram –ø–æ—Å—Ç–∞.
    –¢–µ–ø–µ—Ä—å –ø—Ä–∏–Ω–∏–º–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å post_data, —á—Ç–æ –±–µ–∑–æ–ø–∞—Å–Ω–æ –∏ –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç DetachedInstanceError.
    """
    if not post_data:
        st.warning("–î–∞–Ω–Ω—ã–µ –ø–æ—Å—Ç–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç.")
        return

    now = datetime.now(timezone.utc)

    # –ò–°–ü–û–õ–¨–ó–£–ï–ú –•–ï–õ–ü–ï–† –î–õ–Ø –ë–ï–ó–û–ü–ê–°–ù–û–ì–û –ü–ê–†–°–ò–ù–ì–ê –î–ê–¢
    created_time = _parse_iso_to_utc(post_data.get('created_at')) or now
    published_time = _parse_iso_to_utc(post_data.get('published_at'))

    status_icon = "‚úÖ" if post_data.get('is_published') else "üìù"
    title_display = post_data.get('title', '–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞')[:80]

    with st.expander(f"{status_icon} Telegram ‚Ä¢ {title_display}"):
        # Badges
        col_badge1, col_badge2, col_badge3 = st.columns([1, 1, 1], gap="small")
        with col_badge1:
            st.success("–û–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ" if post_data.get('is_published') else "–ß–µ—Ä–Ω–æ–≤–∏–∫")
        with col_badge2:
            st.caption(f"üìè {post_data.get('character_count', 0)} —Å–∏–º–≤–æ–ª–æ–≤")
        with col_badge3:
            if post_data.get('telegram_message_id'):
                st.caption(f"ID: {post_data.get('telegram_message_id')}")

        st.markdown("---")

        # Timeline
        col_time1, col_time2 = st.columns(2, gap="small")
        with col_time1:
            st.markdown("**üìÖ –°–æ–∑–¥–∞–Ω**")
            st.info(f"{created_time.strftime('%Y-%m-%d %H:%M')}")
            st.caption(format_timedelta(now - created_time, lang))
        with col_time2:
            st.markdown("**üì§ –û–ø—É–±–ª–∏–∫–æ–≤–∞–Ω**")
            if published_time:
                st.success(f"{published_time.strftime('%Y-%m-%d %H:%M')}")
                st.caption(format_timedelta(now - published_time, lang))
            else:
                st.caption("–ù–µ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω")

        st.markdown("---")

        # Content
        col_a, col_b = st.columns([2, 1], gap="small")

        with col_a:
            st.markdown("### üìå –ó–∞–≥–æ–ª–æ–≤–æ–∫")
            st.info(post_data.get('title', '–ù–µ—Ç –∑–∞–≥–æ–ª–æ–≤–∫–∞'))

            st.markdown("### üìù –ö–æ–Ω—Ç–µ–Ω—Ç")
            st.text_area(
                "–°–æ–¥–µ—Ä–∂–∏–º–æ–µ –ø–æ—Å—Ç–∞",
                post_data.get('content', '–ù–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ'),
                height=200,
                key=f"telegram_content_{post_data.get('id')}", # –ò—Å–ø–æ–ª—å–∑—É–µ–º ID –¥–ª—è —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏ –∫–ª—é—á–∞
                label_visibility="collapsed"
            )

            if post_data.get('formatted_content'):
                with st.expander("–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç"):
                    st.markdown(post_data.get('formatted_content'))

            if post_data.get('hashtags'):
                st.markdown("### üè∑Ô∏è –•–µ—à—Ç–µ–≥–∏")
                st.info(post_data.get('hashtags'))

        with col_b:
            st.markdown("**üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞**")
            st.metric("–°–∏–º–≤–æ–ª—ã", post_data.get('character_count', 0))

            if post_data.get('article_id'):
                st.caption(f"ID –°—Ç–∞—Ç—å–∏: {post_data.get('article_id')}")
                if st.button("–û—Ç–∫—Ä—ã—Ç—å —Å—Ç–∞—Ç—å—é", key=f"open_article_{post_data.get('id')}"):
                    # –ó–¥–µ—Å—å –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –ø–µ—Ä–µ—Ö–æ–¥ –∫ —Å—Ç–∞—Ç—å–µ
                    pass

            if post_data.get('is_published') and post_data.get('telegram_message_id'):
                st.markdown("**üì± –í Telegram**")
                st.success(f"–°–æ–æ–±—â–µ–Ω–∏–µ ID: {post_data.get('telegram_message_id')}")
                # –ó–¥–µ—Å—å –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –∫–Ω–æ–ø–∫—É –¥–ª—è –ø–µ—Ä–µ—Ö–æ–¥–∞ –∫ —Å–æ–æ–±—â–µ–Ω–∏—é
                if st.button("–û—Ç–∫—Ä—ã—Ç—å –≤ Telegram", key=f"open_telegram_{post_data.get('id')}"):
                    # –ó–¥–µ—Å—å –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –ø–µ—Ä–µ—Ö–æ–¥ –∫ —Å–æ–æ–±—â–µ–Ω–∏—é –≤ Telegram
                    pass

        st.markdown("---")

def get_telegram_posts(limit=50, include_published=True, include_drafts=True):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ Telegram –ø–æ—Å—Ç–æ–≤ —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π."""
    from src.models.database import get_db_session

    with get_db_session() as session:
        query = session.query(TelegramPost)

        if not include_published:
            query = query.filter_by(is_published=False)
        if not include_drafts:
            query = query.filter_by(is_published=True)

        posts = query.order_by(TelegramPost.created_at.desc()).limit(limit).all()
        # –ö–õ–Æ–ß–ï–í–û–ï –ò–ó–ú–ï–ù–ï–ù–ò–ï: –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –æ–±—ä–µ–∫—Ç—ã –≤ —Å–ª–æ–≤–∞—Ä–∏ –ø–µ—Ä–µ–¥ –≤–æ–∑–≤—Ä–∞—Ç–æ–º
        return [_telegram_post_to_dict(p) for p in posts]

def render_settings_section(title: str, settings_dict: dict, icon: str = "‚öôÔ∏è"):
    """
    –†–µ–Ω–¥–µ—Ä–∏–Ω–≥ —Å–µ–∫—Ü–∏–∏ –Ω–∞—Å—Ç—Ä–æ–µ–∫ —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏.

    Args:
        title: –ó–∞–≥–æ–ª–æ–≤–æ–∫ —Å–µ–∫—Ü–∏–∏
        settings_dict: –°–ª–æ–≤–∞—Ä—å –Ω–∞—Å—Ç—Ä–æ–µ–∫ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
        icon: –≠–º–æ–¥–∑–∏ –∏–∫–æ–Ω–∫–∞
    """
    with st.expander(f"{icon} {title}", expanded=False):
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –∫–æ–ª–æ–Ω–∫–∏ –ø–æ 2
        items = list(settings_dict.items())
        cols_per_row = 2

        for i in range(0, len(items), cols_per_row):
            cols = st.columns(cols_per_row)
            for j, col in enumerate(cols):
                if i + j < len(items):
                    key, value = items[i + j]
                    with col:
                        # –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–ª—é—á–∞ (—á–µ–ª–æ–≤–µ–∫–æ—á–∏—Ç–∞–µ–º–æ)
                        display_key = key.replace('_', ' ').title()

                        # –ú–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–∞—Ä–æ–ª–µ–π –∏ —Å–µ–∫—Ä–µ—Ç–æ–≤
                        if 'password' in key.lower() or 'secret' in key.lower():
                            display_value = "***" if value else "Not set"
                        else:
                            display_value = str(value)

                        st.metric(display_key, display_value)

# ============================================================================
# –û–°–ù–û–í–ù–û–ô –ò–ù–¢–ï–†–§–ï–ô–°
# ============================================================================

# === HEADER ===
col_title, col_spacer, col_lang = st.columns([3, 0.5, 1])

with col_title:
    st.title(t('title'))
    st.caption(t('subtitle'))

with col_lang:
    col_ru, col_en = st.columns(2)
    with col_ru:
        if st.button("üá∑üá∫ RU", key="lang_ru", use_container_width=True,
                     type="primary" if st.session_state.language == 'ru' else "secondary"):
            st.session_state.language = 'ru'
            st.rerun()
    with col_en:
        if st.button("üá¨üáß EN", key="lang_en", use_container_width=True,
                     type="primary" if st.session_state.language == 'en' else "secondary"):
            st.session_state.language = 'en'
            st.rerun()

# –ò–Ω–¥–∏–∫–∞—Ç–æ—Ä –∞–∫—Ç–∏–≤–Ω–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–∞
if st.session_state.get('parsing_active', False):
    session_id = st.session_state.get('current_session_id', 'Unknown')
    st.warning(f"üîÑ –ê–∫—Ç–∏–≤–µ–Ω –ø—Ä–æ—Ü–µ—Å—Å –ø–∞—Ä—Å–∏–Ω–≥–∞ (—Å–µ—Å—Å–∏—è: {session_id[:8] if session_id != 'Unknown' else session_id}...)")

# === API STATUS ===
col1, col2, col3, col4 = st.columns(4)
with col1:
    if os.getenv("REDDIT_CLIENT_ID"):
        st.success(t('reddit_api'))
    else:
        st.warning(t('reddit_api'))
with col2:
    if os.getenv("TELEGRAM_API_ID"):
        st.success(t('telegram_api'))
    else:
        st.warning(t('telegram_api'))
with col3:
    st.success(t('database'))
with col4:
    # Habr –Ω–µ —Ç—Ä–µ–±—É–µ—Ç API –∫–ª—é—á–µ–π, –≤—Å–µ–≥–¥–∞ –¥–æ—Å—Ç—É–ø–µ–Ω
    st.success("üü¢ Habr")

st.markdown("---")

try:
    stats = get_stats_extended()
# –ò–°–ü–†–ê–í–õ–ï–ù–û: –ó–∞–º–µ–Ω–µ–Ω –≥–æ–ª—ã–π except –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π —Ç–∏–ø –¥–ª—è –ª—É—á—à–µ–π –æ—Ç–ª–∞–¥–∫–∏
except Exception as e:
    st.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É: {e}")
    stats = {
        'reddit_posts': 0,
        'telegram_messages': 0,
        'medium_articles': 0,
        'habr_articles': 0,
        'telegram_posts': 0,
        'latest_reddit': None,
        'latest_telegram': None,
        'latest_medium': None,
        'latest_habr': None
    }

# === TABS ===
tab1, tab2, tab3, tab4, tab5, tab6, tab7, tab8 = st.tabs([
    t('reddit_tab'),
    t('telegram_tab'),
    t('medium_tab'),
    "üá∑üá∫ Habr",
    "üì± Telegram –ü–æ—Å—Ç—ã",
    f"üìä {t('data_viewer_tab')}",
    f"‚öôÔ∏è {t('settings_tab')}",
    "üîå API"
])

# === TAB 1: REDDIT PARSER ===
with tab1:
    st.markdown('<div class="reddit-section">', unsafe_allow_html=True)
    st.header(f"{t('reddit_tab')} Parser")

    col1, col2 = st.columns([2, 1])

    with col1:
        st.subheader(t('settings'))

        all_subreddits = sources_config.get_reddit_subreddits()
        categories = sources_config.get_reddit_categories()

        category_filter = st.selectbox(
            t('filter_category'),
            [t('all_categories')] + categories,
            index=0,
            key="reddit_category"
        )

        if category_filter == t('all_categories'):
            filtered_subs = all_subreddits
        else:
            filtered_subs = sources_config.get_reddit_subreddits(category=category_filter)

        if 'reddit_selected' not in st.session_state:
            st.session_state.reddit_selected = []
        if 'reddit_widget_key' not in st.session_state:
            st.session_state.reddit_widget_key = 0

        col_sel1, col_sel2 = st.columns([3, 1])

        with col_sel2:
            st.write("")
            st.write("")
            if st.button(t('select_all'), key="select_all_reddit"):
                st.session_state.reddit_selected = filtered_subs.copy()
                st.session_state.reddit_widget_key += 1
                st.rerun()

        with col_sel1:
            selected_subs = st.multiselect(
                t('subreddits'),
                filtered_subs,
                default=[s for s in st.session_state.reddit_selected if s in filtered_subs],
                key=f"reddit_multiselect_{st.session_state.reddit_widget_key}"
            )
            st.session_state.reddit_selected = selected_subs

        settings = st.session_state.settings

        col_a, col_b, col_c, col_d = st.columns(4)
        with col_a:
            max_posts = st.slider(
                t('max_posts'),
                min_value=1,
                max_value=200,
                value=max(1, settings['default_max_posts']),
                key="reddit_max_posts",
                help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Å—Ç–æ–≤ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –∏–∑ –∫–∞–∂–¥–æ–≥–æ subreddit (1-200)",
                disabled=st.session_state.parsing_in_progress
            )
        with col_b:
            delay = st.slider(
                t('delay_sec'),
                3, 30,
                settings['default_delay'],
                key="reddit_delay",
                disabled=st.session_state.parsing_in_progress
            )
        with col_c:
            sort_by = st.selectbox(
                t('sort'),
                ["hot", "new", "top"],
                index=["hot", "new", "top"].index(settings['default_sort']),
                key="reddit_sort",
                disabled=st.session_state.parsing_in_progress
            )
        with col_d:
            enable_llm = st.checkbox(
                t('editorial'),
                value=settings['default_enable_llm'],
                key="reddit_llm",
                disabled=st.session_state.parsing_in_progress
            )

        st.markdown("---")

        # –°—Ç–∞—Ç—É—Å –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        if st.session_state.parsing_results:
            total_saved = sum(r.get('saved', 0) for r in st.session_state.parsing_results if r.get('success'))
            success_count = sum(1 for r in st.session_state.parsing_results if r.get('success'))
            total_count = len(st.session_state.parsing_results)

            st.success(f"‚úÖ –ü–æ—Å–ª–µ–¥–Ω–∏–π –ø–∞—Ä—Å–∏–Ω–≥: {total_saved} –ø–æ—Å—Ç–æ–≤ –∏–∑ {success_count}/{total_count} subreddits")

        # –ö–Ω–æ–ø–∫–∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è
        col_btn1, col_btn2, col_btn3 = st.columns([2, 1, 1])

        with col_btn1:
            start_button_disabled = (
                    not selected_subs or
                    not os.getenv("REDDIT_CLIENT_ID") or
                    st.session_state.parsing_in_progress
            )

            if st.button(
                    "üöÄ " + t('start_parsing') if not st.session_state.parsing_in_progress else "‚è∏Ô∏è –ü–∞—Ä—Å–∏–Ω–≥ –∞–∫—Ç–∏–≤–µ–Ω...",
                    type="primary",
                    use_container_width=True,
                    key="reddit_parse_btn",
                    disabled=start_button_disabled
            ):
                if not selected_subs:
                    st.error(t('select_subreddits'))
                elif not os.getenv("REDDIT_CLIENT_ID"):
                    st.error(t('api_not_configured'))
                else:
                    st.markdown("---")
                    results = scrape_with_live_logs(
                        subreddits=selected_subs,
                        max_posts=max_posts,
                        sort_by=sort_by,
                        delay=delay,
                        enable_llm=enable_llm
                    )
                    st.rerun()

        with col_btn2:
            if st.button(
                    "üîÑ –û–±–Ω–æ–≤–∏—Ç—å",
                    type="secondary",
                    use_container_width=True,
                    key="refresh_page_btn"
            ):
                st.rerun()

        with col_btn3:
            if st.button(
                    "üóëÔ∏è –û—á–∏—Å—Ç–∏—Ç—å",
                    type="secondary",
                    use_container_width=True,
                    key="clear_logs_btn",
                    disabled=st.session_state.parsing_in_progress
            ):
                StreamlitLogger.clear()
                st.session_state.parsing_results = None
                st.success("–õ–æ–≥–∏ –æ—á–∏—â–µ–Ω—ã!")
                time.sleep(0.5)
                st.rerun()

        # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –ª–æ–≥–æ–≤
        if st.session_state.parsing_logs:
            with st.expander("üìú –í—Å–µ –ª–æ–≥–∏ —Å–µ—Å—Å–∏–∏", expanded=False):
                st.markdown("\n".join(list(st.session_state.parsing_logs)))

    with col2:
        st.subheader(t('statistics'))

        # FIXED: Safe statistics retrieval with defaults
        reddit_count = stats.get('reddit_posts', 0)
        st.metric(t('posts'), f"{reddit_count:,}")

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏
        try:
            from src.models.database import get_db_session, ProcessedRedditPost, HabrArticle, get_session

            with get_db_session() as session:
                # Reddit —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
                reddit_processed_count = session.query(ProcessedRedditPost).count()
                reddit_news_count = session.query(ProcessedRedditPost).filter_by(is_news=True).count()

                # Habr —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
                habr_processed_count = session.query(HabrArticle).filter_by(editorial_processed=True).count()
                habr_news_count = session.query(HabrArticle).filter_by(is_news=True).count()

                # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
                processed_count = reddit_processed_count + habr_processed_count
                news_count = reddit_news_count + habr_news_count

                # –ü—Ä–æ—Ü–µ–Ω—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏
                processing_rate = (processed_count / reddit_count * 100) if reddit_count > 0 else 0

                st.metric("–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ", f"{processed_count:,}")
                st.metric("–ù–æ–≤–æ—Å—Ç–µ–π", f"{news_count:,}")

                if reddit_count > 0:
                    st.progress(
                        processing_rate / 100,
                        text=f"–û–±—Ä–∞–±–æ—Ç–∫–∞: {processing_rate:.1f}%"
                    )
        # `logger` –Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω, –∑–∞–º–µ–Ω–µ–Ω –Ω–∞ `st.error`
        except Exception as e:
            st.caption(f"‚ö†Ô∏è –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞")
            st.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {e}")

    st.markdown('</div>', unsafe_allow_html=True)

# === TAB 2: TELEGRAM PARSER ===
with tab2:
    st.markdown('<div class="telegram-section">', unsafe_allow_html=True)
    st.header(f"{t('telegram_tab')} Parser")

    # –£–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –æ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ
    st.warning("‚ö†Ô∏è **–í —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ** - –¢—Ä–µ–±—É–µ—Ç –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ Telegram API")

    col_info, col_settings = st.columns([2, 1])

    with col_info:
        st.markdown("""
        ### –ß—Ç–æ –Ω—É–∂–Ω–æ –¥–ª—è —Ä–∞–±–æ—Ç—ã:

        1. **–ü–æ–ª—É—á–∏—Ç—å Telegram API**
           - –ó–∞–π—Ç–∏ –Ω–∞ https://my.telegram.org/apps
           - –°–æ–∑–¥–∞—Ç—å –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –∏ –ø–æ–ª—É—á–∏—Ç—å `api_id` –∏ `api_hash`

        2. **–ù–∞—Å—Ç—Ä–æ–∏—Ç—å –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è**:
           ```bash
           TELEGRAM_API_ID=–≤–∞—à_api_id
           TELEGRAM_API_HASH=–≤–∞—à_api_hash
           TELEGRAM_PHONE=+7xxxxxxxxxx
           ```

        3. **–ü—Ä–∞–≤–∞ –¥–æ—Å—Ç—É–ø–∞**
           - –ë–æ—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä–æ–º –∫–∞–Ω–∞–ª–æ–≤
           - –£–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ –±–æ—Ç –∏–º–µ–µ—Ç –ø—Ä–∞–≤–∞ –Ω–∞ —á—Ç–µ–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏–π

        ### –¢–µ–∫—É—â–∏–π —Å—Ç–∞—Ç—É—Å:
        """)

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
        config_status = []
        if os.getenv("TELEGRAM_API_ID"):
            config_status.append("‚úÖ TELEGRAM_API_ID –Ω–∞—Å—Ç—Ä–æ–µ–Ω")
        else:
            config_status.append("‚ùå TELEGRAM_API_ID –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω")

        if os.getenv("TELEGRAM_API_HASH"):
            config_status.append("‚úÖ TELEGRAM_API_HASH –Ω–∞—Å—Ç—Ä–æ–µ–Ω")
        else:
            config_status.append("‚ùå TELEGRAM_API_HASH –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω")

        if os.getenv("TELEGRAM_PHONE"):
            config_status.append("‚úÖ TELEGRAM_PHONE –Ω–∞—Å—Ç—Ä–æ–µ–Ω")
        else:
            config_status.append("‚ùå TELEGRAM_PHONE –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω")

        for status in config_status:
            st.markdown(f"- {status}")

        if all("‚úÖ" in status for status in config_status):
            st.success("üéâ –í—Å–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –≤ –ø–æ—Ä—è–¥–∫–µ! –ú–æ–∂–µ—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–∞—Ä—Å–µ—Ä.")
        else:
            st.error("‚ö†Ô∏è –ù–∞—Å—Ç—Ä–æ–π—Ç–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞—Ä—Å–µ—Ä–∞.")

    with col_settings:
        st.subheader("–¢–µ—Å—Ç–æ–≤—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏")

        # –î–µ–º–æ-–∫–∞–Ω–∞–ª—ã (–º–æ–∂–Ω–æ –∏–∑–º–µ–Ω–∏—Ç—å)
        demo_channels = [
            "@telegram",  # –û—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π –∫–∞–Ω–∞–ª Telegram
            "@durov",  # –ö–∞–Ω–∞–ª –ü–∞–≤–ª–∞ –î—É—Ä–æ–≤–∞
            "@python_news",  # –ù–æ–≤–æ—Å—Ç–∏ Python (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω)
        ]

        selected_channels = st.multiselect(
            "–í—ã–±–µ—Ä–∏—Ç–µ –∫–∞–Ω–∞–ª—ã –¥–ª—è —Ç–µ—Å—Ç–∞:",
            demo_channels,
            default=[],
            key="telegram_demo_channels",
            help="–í—ã–±–µ—Ä–∏—Ç–µ –∫–∞–Ω–∞–ª—ã –∏–∑ —Å–ø–∏—Å–∫–∞ –∏–ª–∏ –≤–≤–µ–¥–∏—Ç–µ —Å–≤–æ–∏"
        )

        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞
        col_a, col_b, col_c = st.columns(3)
        with col_a:
            tg_max_msgs = st.slider(
                "–°–æ–æ–±—â–µ–Ω–∏–π —Å –∫–∞–Ω–∞–ª–∞:",
                10, 500,
                100,
                key="tg_max_msgs"
            )
        with col_b:
            tg_delay = st.slider(
                "–ó–∞–¥–µ—Ä–∂–∫–∞ (—Å–µ–∫):",
                3, 30,
                5,
                key="tg_delay"
            )
        with col_c:
            tg_enable_llm = st.checkbox(
                "LLM Preprocessing",
                value=False,
                key="tg_llm",
                disabled=True
            )
            st.caption("–ü–æ–∫–∞ –Ω–µ –¥–æ—Å—Ç—É–ø–Ω–æ")

        st.markdown("---")

        # –ö–Ω–æ–ø–∫–∞ –∑–∞–ø—É—Å–∫–∞
        button_disabled = (
                not selected_channels or
                not all([os.getenv("TELEGRAM_API_ID"), os.getenv("TELEGRAM_API_HASH"), os.getenv("TELEGRAM_PHONE")])
        )

        if button_disabled:
            st.warning("‚ö†Ô∏è –ù–∞—Å—Ç—Ä–æ–π—Ç–µ Telegram API –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞—Ä—Å–µ—Ä–∞")

        if st.button(
                "üöÄ –ó–∞–ø—É—Å—Ç–∏—Ç—å –ø–∞—Ä—Å–∏–Ω–≥ Telegram",
                type="primary",
                use_container_width=True,
                key="telegram_parse_btn",
                disabled=button_disabled
        ):
            if not selected_channels:
                st.error("–í—ã–±–µ—Ä–∏—Ç–µ —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω –∫–∞–Ω–∞–ª")
            else:
                st.markdown("---")

                # –í—ã–∑–æ–≤ scraper —Å live logs
                results = scrape_telegram_channels(
                    channels=selected_channels,
                    limit=tg_max_msgs,
                    delay=tg_delay,
                    enable_llm=tg_enable_llm,
                    log_callback=lambda msg, lvl: StreamlitLogger.log(msg, lvl)
                )

                # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                st.markdown("### –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–∞—Ä—Å–∏–Ω–≥–∞:")
                for result in results:
                    if result.get('success'):
                        st.success(f"‚úÖ {result.get('channel', 'Unknown')}: {result.get('messages_saved', 0)} —Å–æ–æ–±—â–µ–Ω–∏–π")
                    else:
                        st.error(f"‚ùå {result.get('channel', 'Unknown')}: {result.get('error', 'Unknown error')}")

                st.session_state.parsing_results = results
                st.rerun()

    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    st.markdown("---")
    st.subheader("–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞")
    # st.metric("–°–æ–æ–±—â–µ–Ω–∏–π", f"{stats['telegram_messages']:,}")

    # if stats['latest_telegram']:
    #     st.caption(f"–ü–æ—Å–ª–µ–¥–Ω–µ–µ: {stats['latest_telegram']}")

    # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ
    with st.expander("‚ÑπÔ∏è –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ"):
        st.markdown("""
        ### –¢–µ–∫—É—â–∏–π —Å—Ç–∞—Ç—É—Å:
        - **–í–µ—Ä—Å–∏—è**: 0.1 (–≤ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ)
        - **–°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å**: –ê–ª—å—Ñ–∞
        - **–ü–æ–¥–¥–µ—Ä–∂–∫–∞**: –ë–∞–∑–æ–≤–∞—è

        ### –ü–ª–∞–Ω–∏—Ä—É–µ–º—ã–µ —É–ª—É—á—à–µ–Ω–∏—è:
        - [ ] LLM –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å–æ–æ–±—â–µ–Ω–∏–π
        - [ ] –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ —Ç–∏–ø–∞–º –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        - [ ] –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –º–µ–¥–∏–∞-–≤–ª–æ–∂–µ–Ω–∏–π
        - [ ] –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —è–∑—ã–∫–∞
        - [ ] –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–µ–π

        ### –ò–∑–≤–µ—Å—Ç–Ω—ã–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è:
        - –¢—Ä–µ–±—É—é—Ç—Å—è –ø—Ä–∞–≤–∞ –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä–∞ –Ω–∞ –∫–∞–Ω–∞–ª–∞—Ö
        - Rate limiting –æ—Ç Telegram API
        - –ù–µ—Ç –ø–æ–¥–¥–µ—Ä–∂–∫–∏ –ø—Ä–∏–≤–∞—Ç–Ω—ã—Ö –∫–∞–Ω–∞–ª–æ–≤

        ### –î–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤:
        ```python
        # –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
        from src.scrapers.telegram_scraper import scrape_telegram_channels

        results = await scrape_telegram_channels(
            channels=['@channel1', '@channel2'],
            limit=100
        )
        ```
        """)

    st.markdown('</div>', unsafe_allow_html=True)

# === TAB 3: MEDIUM ===
with tab3:
    st.markdown('<div class="medium-section">', unsafe_allow_html=True)
    st.header(f"{t('medium_tab')} Parser")
    st.info(t('in_development'))
    st.markdown('</div>', unsafe_allow_html=True)

# === TAB 4: HABR PARSER ===
with tab4:
    st.markdown('<div class="habr-section">', unsafe_allow_html=True)
    st.header("üá∑üá∫ Habr Parser (Enhanced)")

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ —Å–µ—Ä–≤–∏—Å–æ–≤
    qdrant_available = False
    ollama_available = False

    try:
        import requests

        qdrant_url = os.getenv("QDRANT_URL", "http://qdrant:6333")
        resp = requests.get(f"{qdrant_url}/collections", timeout=2)
        qdrant_available = resp.status_code == 200
    except:
        pass

    try:
        ollama_url = os.getenv("OLLAMA_BASE_URL", "http://ollama:11434")
        resp = requests.get(f"{ollama_url}/api/tags", timeout=2)
        ollama_available = resp.status_code == 200
    except:
        pass

    # –°—Ç–∞—Ç—É—Å —Å–µ—Ä–≤–∏—Å–æ–≤
    col_status1, col_status2 = st.columns(2)
    with col_status1:
        if qdrant_available:
            st.success("‚úÖ Qdrant –¥–æ—Å—Ç—É–ø–µ–Ω (–¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –≤–∫–ª—é—á–µ–Ω–∞)")
        else:
            st.warning("‚ö†Ô∏è Qdrant –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω (–¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –æ—Ç–∫–ª—é—á–µ–Ω–∞)")

    with col_status2:
        if ollama_available:
            st.success("‚úÖ Ollama –¥–æ—Å—Ç—É–ø–µ–Ω (LLM –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–∞)")
        else:
            st.warning("‚ö†Ô∏è Ollama –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω (LLM –æ–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞)")

    st.markdown("---")

    col1, col2 = st.columns([2, 1])

    with col1:
        st.subheader("‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞")

        # –ü–æ–ª—É—á–∞–µ–º —Ö–∞–±—ã –∏ —Ç–µ–≥–∏ –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞
        try:
            all_hubs = sources_config.get_habr_hubs()
            all_tags = sources_config.get_habr_tags()
            categories = sources_config.get_habr_categories()
        except Exception as e:
            st.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {e}")
            all_hubs = []
            all_tags = []
            categories = []

        # –§–∏–ª—å—Ç—Ä –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        habr_category_filter = st.selectbox(
            "üìÅ –§–∏–ª—å—Ç—Ä –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏:",
            ["–í—Å–µ"] + categories,
            index=0,
            key="habr_category",
            help="–§–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å —Ö–∞–±—ã –∏ —Ç–µ–≥–∏ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏"
        )

        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Ö–∞–±–æ–≤ –∏ —Ç–µ–≥–æ–≤
        if habr_category_filter == "–í—Å–µ":
            filtered_hubs = all_hubs
            filtered_tags = all_tags
        else:
            filtered_hubs = sources_config.get_habr_hubs(category=habr_category_filter)
            filtered_tags = sources_config.get_habr_tags(category=habr_category_filter)

        # Session state –¥–ª—è –≤—ã–±–æ—Ä–∞
        if 'habr_selected_hubs' not in st.session_state:
            st.session_state.habr_selected_hubs = []
        if 'habr_selected_tags' not in st.session_state:
            st.session_state.habr_selected_tags = []
        if 'habr_widget_key' not in st.session_state:
            st.session_state.habr_widget_key = 0

        # –í—ã–±–æ—Ä —Ö–∞–±–æ–≤
        with st.expander("üè∑Ô∏è –í—ã–±–æ—Ä —Ö–∞–±–æ–≤", expanded=True):
            col_hub1, col_hub2 = st.columns([3, 1])
            with col_hub2:
                st.write("")
                st.write("")
                if st.button("–í—ã–±—Ä–∞—Ç—å –≤—Å–µ", key="select_all_habr_hubs", use_container_width=True):
                    st.session_state.habr_selected_hubs = filtered_hubs.copy()
                    st.session_state.habr_widget_key += 1
                    st.rerun()

                if st.button("–û—á–∏—Å—Ç–∏—Ç—å", key="clear_habr_hubs", use_container_width=True):
                    st.session_state.habr_selected_hubs = []
                    st.session_state.habr_widget_key += 1
                    st.rerun()

            with col_hub1:
                selected_hubs = st.multiselect(
                    f"–•–∞–±—ã ({len(filtered_hubs)} –¥–æ—Å—Ç—É–ø–Ω–æ):",
                    filtered_hubs,
                    default=[h for h in st.session_state.habr_selected_hubs if h in filtered_hubs],
                    key=f"habr_hubs_multiselect_{st.session_state.habr_widget_key}",
                    help="–í—ã–±–µ—Ä–∏—Ç–µ —Ö–∞–±—ã –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞. –ï—Å–ª–∏ –Ω–µ –≤—ã–±—Ä–∞–Ω–æ - –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –≤—Å–µ —Ö–∞–±—ã"
                )
                st.session_state.habr_selected_hubs = selected_hubs

                if not selected_hubs:
                    st.info(f"üí° –ë—É–¥—É—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω—ã –≤—Å–µ —Ö–∞–±—ã ({len(filtered_hubs)})")

        # –í—ã–±–æ—Ä —Ç–µ–≥–æ–≤
        with st.expander("üîñ –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ç–µ–≥–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)", expanded=False):
            col_tag1, col_tag2 = st.columns([3, 1])
            with col_tag2:
                st.write("")
                st.write("")
                if st.button("–í—ã–±—Ä–∞—Ç—å –≤—Å–µ", key="select_all_habr_tags", use_container_width=True):
                    st.session_state.habr_selected_tags = filtered_tags.copy()
                    st.session_state.habr_widget_key += 1
                    st.rerun()

                if st.button("–û—á–∏—Å—Ç–∏—Ç—å", key="clear_habr_tags", use_container_width=True):
                    st.session_state.habr_selected_tags = []
                    st.session_state.habr_widget_key += 1
                    st.rerun()

            with col_tag1:
                selected_tags = st.multiselect(
                    f"–¢–µ–≥–∏ ({len(filtered_tags)} –¥–æ—Å—Ç—É–ø–Ω–æ):",
                    filtered_tags,
                    default=[t for t in st.session_state.habr_selected_tags if t in filtered_tags],
                    key=f"habr_tags_multiselect_{st.session_state.habr_widget_key}",
                    help="–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ç–µ–≥–∏ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"
                )
                st.session_state.habr_selected_tags = selected_tags

        st.markdown("---")

        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞
        st.subheader("üéõÔ∏è –ü–∞—Ä–∞–º–µ—Ç—Ä—ã")

        col_a, col_b, col_c = st.columns(3)

        with col_a:
            habr_max_articles = st.number_input(
                "Max —Å—Ç–∞—Ç–µ–π:",
                min_value=1,
                max_value=200,
                value=10,
                step=5,
                key="habr_max_articles",
                help="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç–∞—Ç–µ–π –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞"
            )

        with col_b:
            habr_enable_llm = st.checkbox(
                "ü§ñ LLM –û–±—Ä–∞–±–æ—Ç–∫–∞",
                value=ollama_available,
                key="habr_llm",
                disabled=not ollama_available,
                help="–í–∫–ª—é—á–∏—Ç—å —Ä–µ–¥–∞–∫—Ç–æ—Ä—Å–∫—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É —á–µ—Ä–µ–∑ LLM (—Ç—Ä–µ–±—É–µ—Ç Ollama)"
            )

        with col_c:
            habr_enable_dedup = st.checkbox(
                "üîç –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è",
                value=qdrant_available,
                key="habr_dedup",
                disabled=not qdrant_available,
                help="–í–∫–ª—é—á–∏—Ç—å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—é —á–µ—Ä–µ–∑ Qdrant"
            )

        st.markdown("---")

        # –°—Ç–∞—Ç—É—Å –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞
        if 'habr_parsing_results' in st.session_state and st.session_state.habr_parsing_results:
            result = st.session_state.habr_parsing_results

            if result.get('success'):
                saved = result.get('saved', 0)
                skipped = result.get('skipped', 0)
                semantic_dups = result.get('semantic_duplicates', 0)
                editorial = result.get('editorial_processed', 0)
                errors = result.get('errors', 0)

                col_res1, col_res2, col_res3, col_res4 = st.columns(4)
                with col_res1:
                    st.metric("‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ", saved)
                with col_res2:
                    st.metric("‚è≠Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω–æ", skipped)
                with col_res3:
                    st.metric("üîÑ –î—É–±–ª–∏–∫–∞—Ç–æ–≤", semantic_dups)
                with col_res4:
                    st.metric("üìù LLM", editorial)

                if errors > 0:
                    st.warning(f"‚ö†Ô∏è –û—à–∏–±–æ–∫: {errors}")
            else:
                st.error(f"‚ùå –û—à–∏–±–∫–∞: {result.get('error', 'Unknown')}")

        # –ö–Ω–æ–ø–∫–∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è
        col_btn1, col_btn2, col_btn3, col_btn4 = st.columns([2, 1, 1, 1])

        with col_btn1:
            if st.button(
                    "üöÄ –ó–∞–ø—É—Å—Ç–∏—Ç—å –ø–∞—Ä—Å–∏–Ω–≥",
                    type="primary",
                    use_container_width=True,
                    key="habr_parse_btn",
                    disabled=st.session_state.parsing_in_progress
            ):
                st.markdown("---")

                # Clear logs before starting
                StreamlitLogger.clear()

                # –ó–∞–ø—É—Å–∫ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å live logs
                result = scrape_habr_with_live_logs(
                    hubs=selected_hubs if selected_hubs else [],
                    tags=selected_tags if selected_tags else [],
                    max_articles=habr_max_articles,
                    enable_llm=habr_enable_llm,
                    enable_dedup=habr_enable_dedup  # ‚Üê –ù–û–í–´–ô –ü–ê–†–ê–ú–ï–¢–†
                )

                st.session_state.habr_parsing_results = result
                st.rerun()

        with col_btn2:
            if st.button(
                    "üì• Export",
                    type="secondary",
                    use_container_width=True,
                    key="habr_export_btn",
                    disabled=not ('habr_parsing_results' in st.session_state)
            ):
                if 'habr_parsing_results' in st.session_state:
                    result = st.session_state.habr_parsing_results
                    json_str = json.dumps(result, indent=2, ensure_ascii=False)
                    st.download_button(
                        label="‚¨áÔ∏è –°–∫–∞—á–∞—Ç—å JSON",
                        data=json_str,
                        file_name=f"habr_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                        mime="application/json"
                    )

        with col_btn3:
            if st.button(
                    "üóëÔ∏è –û—á–∏—Å—Ç–∏—Ç—å –ª–æ–≥–∏",
                    type="secondary",
                    use_container_width=True,
                    key="habr_clear_logs_btn",
                    disabled=st.session_state.parsing_in_progress
            ):
                StreamlitLogger.clear()
                if 'habr_parsing_results' in st.session_state:
                    del st.session_state.habr_parsing_results
                st.success("–õ–æ–≥–∏ –æ—á–∏—â–µ–Ω—ã!")
                time.sleep(0.5)
                st.rerun()

        with col_btn4:
            if st.button(
                    "üîÑ –û–±–Ω–æ–≤–∏—Ç—å",
                    type="secondary",
                    use_container_width=True,
                    key="habr_refresh_btn"
            ):
                st.rerun()

        # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –ª–æ–≥–æ–≤
        if st.session_state.parsing_logs:
            with st.expander("üìú –õ–æ–≥–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞", expanded=False):
                st.markdown("\n".join(list(st.session_state.parsing_logs)))

    with col2:
        st.subheader("üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞")

        try:
            # –ë–∞–∑–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            total_articles = stats.get('habr_articles', 0)
            st.metric("üìö –í—Å–µ–≥–æ —Å—Ç–∞—Ç–µ–π", f"{total_articles:,}")

            if stats.get('latest_habr'):
                st.caption(f"üïê –ü–æ—Å–ª–µ–¥–Ω—è—è: {stats['latest_habr']}")

            st.markdown("---")

            # –î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            session = get_session()

            # –û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ —Å—Ç–∞—Ç—å–∏
            processed = session.query(HabrArticle).filter(
                HabrArticle.editorial_processed == True
            ).count()

            # –ù–æ–≤–æ—Å—Ç–∏
            news = session.query(HabrArticle).filter(
                HabrArticle.is_news == True
            ).count()

            # –í–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã–µ
            vectorized = session.query(HabrArticle).filter(
                HabrArticle.qdrant_id.isnot(None)
            ).count()

            st.metric("ü§ñ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ LLM", f"{processed:,}")
            st.metric("üì∞ –ù–æ–≤–æ—Å—Ç–µ–π", f"{news:,}")
            st.metric("üîç –í Qdrant", f"{vectorized:,}")

            # –ü—Ä–æ–≥—Ä–µ—Å—Å –æ–±—Ä–∞–±–æ—Ç–∫–∏
            if total_articles > 0:
                processing_rate = (processed / total_articles) * 100
                st.progress(
                    processing_rate / 100,
                    text=f"–û–±—Ä–∞–±–æ—Ç–∫–∞: {processing_rate:.1f}%"
                )

                news_rate = (news / total_articles) * 100
                st.progress(
                    news_rate / 100,
                    text=f"–ù–æ–≤–æ—Å—Ç–∏: {news_rate:.1f}%"
                )

            # –¢–æ–ø –∫–∞—Ç–µ–≥–æ—Ä–∏–π
            st.markdown("---")
            st.caption("üìÅ –¢–æ–ø –∫–∞—Ç–µ–≥–æ—Ä–∏–π")

            from sqlalchemy import func

            top_categories = session.query(
                HabrArticle.categories,
                func.count(HabrArticle.id)
            ).group_by(
                HabrArticle.categories
            ).order_by(
                func.count(HabrArticle.id).desc()
            ).limit(5).all()

            for cat, count in top_categories:
                if cat:
                    st.caption(f"‚Ä¢ {cat.split(',')[0]}: {count}")

            session.close()

        except Exception as e:
            st.error(f"‚ùå –û—à–∏–±–∫–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {e}")

    st.markdown('</div>', unsafe_allow_html=True)

# === TAB 5: TELEGRAM POSTS ===
with tab5:
    st.header("üì± Telegram –ü–æ—Å—Ç—ã")

    # –§–∏–ª—å—Ç—Ä—ã
    col1, col2, col3 = st.columns(3)
    with col1:
        include_published = st.checkbox("–û–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ", value=True)
    with col2:
        include_drafts = st.checkbox("–ß–µ—Ä–Ω–æ–≤–∏–∫–∏", value=True)
    with col3:
        limit = st.number_input("–õ–∏–º–∏—Ç", min_value=10, max_value=100, value=20)

    # –ü–æ–ª—É—á–µ–Ω–∏–µ –ø–æ—Å—Ç–æ–≤
    posts_data = get_telegram_posts(limit=limit, include_published=include_published, include_drafts=include_drafts)

    if not posts_data:
        st.info("–ù–µ—Ç –ø–æ—Å—Ç–æ–≤ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è")
    else:
        # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –ø–æ—Å—Ç–æ–≤
        for post_data in posts_data:
            render_telegram_post_viewer(post_data, st.session_state.language)

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        st.subheader("üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞")
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("–í—Å–µ–≥–æ –ø–æ—Å—Ç–æ–≤", len(posts_data))
        with col2:
            published_count = sum(1 for p in posts_data if p.get('is_published'))
            st.metric("–û–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ", published_count)
        with col3:
            draft_count = sum(1 for p in posts_data if not p.get('is_published'))
            st.metric("–ß–µ—Ä–Ω–æ–≤–∏–∫–∏", draft_count)

# === TAB 6: DATA VIEWER ===
with tab6:
    st.header("üìä –ü—Ä–æ—Å–º–æ—Ç—Ä –¥–∞–Ω–Ω—ã—Ö")

    # –í—ã–±–æ—Ä –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    data_source = st.radio(
        "–ò—Å—Ç–æ—á–Ω–∏–∫ –¥–∞–Ω–Ω—ã—Ö:",
        ["Reddit (—Å—ã—Ä—ã–µ)", "Reddit (–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ)", "Telegram", "Medium", "Habr", "Telegram –ü–æ—Å—Ç—ã"],
        horizontal=True,
        key="data_source_radio"
    )

    st.markdown("---")

    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
    col_filter, col_sort, col_limit = st.columns([2, 2, 1])

    with col_limit:
        limit = st.slider(
            "–ó–∞–ø–∏—Å–µ–π",
            10, 500,
            st.session_state.settings['viewer_default_limit'],
            key="unified_limit"
        )

    # –°–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
    if "Reddit" in data_source:
        with col_filter:
            if "–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ" in data_source:
                news_only = st.checkbox("–¢–æ–ª—å–∫–æ –Ω–æ–≤–æ—Å—Ç–∏", value=False, key="news_filter")
            else:
                news_only = False

        with col_sort:
            if "—Å—ã—Ä—ã–µ" in data_source:
                sort_options = {
                    "–ü–æ–ª—É—á–µ–Ω—ã (–Ω–æ–≤—ã–µ)": "scraped_at_desc",
                    "–û–ø—É–±–ª–∏–∫–æ–≤–∞–Ω—ã (–Ω–æ–≤—ã–µ)": "created_utc_desc",
                    "–†–µ–π—Ç–∏–Ω–≥ ‚¨Ü": "score_desc"
                }
            else:
                sort_options = {
                    "–û–±—Ä–∞–±–æ—Ç–∞–Ω—ã (–Ω–æ–≤—ã–µ)": "processed_at_desc",
                    "–†–µ–π—Ç–∏–Ω–≥ ‚¨Ü": "score_desc"
                }

            sort_by = st.selectbox("–°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞", list(sort_options.keys()), key="reddit_sort_viewer")
            sort_value = sort_options[sort_by]

    elif data_source == "Habr":
        with col_filter:
            habr_filter = st.selectbox(
                "–§–∏–ª—å—Ç—Ä:",
                ["–í—Å–µ", "–¢–æ–ª—å–∫–æ –Ω–æ–≤–æ—Å—Ç–∏", "–¢–æ–ª—å–∫–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ"],
                key="habr_filter"
            )

        with col_sort:
            habr_sort_options = {
                "–ü–æ–ª—É—á–µ–Ω—ã (–Ω–æ–≤—ã–µ)": "scraped_at_desc",
                "–û–ø—É–±–ª–∏–∫–æ–≤–∞–Ω—ã (–Ω–æ–≤—ã–µ)": "pub_date_desc",
                "–†–µ–π—Ç–∏–Ω–≥ ‚¨Ü": "rating_desc"
            }
            habr_sort_by = st.selectbox("–°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞", list(habr_sort_options.keys()), key="habr_sort_viewer")
            habr_sort_value = habr_sort_options[habr_sort_by]

    elif data_source == "Telegram –ü–æ—Å—Ç—ã":
        with col_filter:
            tg_filter = st.selectbox(
                "–§–∏–ª—å—Ç—Ä:",
                ["–í—Å–µ", "–¢–æ–ª—å–∫–æ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–Ω—ã–µ", "–¢–æ–ª—å–∫–æ —á–µ—Ä–Ω–æ–≤–∏–∫–∏"],
                key="tg_filter"
            )

        with col_sort:
            tg_sort_options = {
                "–°–æ–∑–¥–∞–Ω—ã (–Ω–æ–≤—ã–µ)": "created_at_desc",
                "–û–ø—É–±–ª–∏–∫–æ–≤–∞–Ω—ã (–Ω–æ–≤—ã–µ)": "published_at_desc",
                "–°–∏–º–≤–æ–ª–æ–≤ ‚¨Ü": "character_count_desc"
            }
            tg_sort_by = st.selectbox("–°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞", list(tg_sort_options.keys()), key="tg_sort_viewer")
            tg_sort_value = tg_sort_options[tg_sort_by]

    st.markdown("---")

    # –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
    try:
        session = get_session()

        if data_source == "Reddit (—Å—ã—Ä—ã–µ)":
            query = session.query(RedditPost)

            if sort_value == "scraped_at_desc":
                query = query.order_by(RedditPost.scraped_at.desc())
            elif sort_value == "created_utc_desc":
                query = query.order_by(RedditPost.created_utc.desc())
            elif sort_value == "score_desc":
                query = query.order_by(RedditPost.score.desc())

            posts = query.limit(limit).all()
            posts_data = [_reddit_post_to_dict(p) for p in posts]

            if posts_data:
                st.caption(f"üî¥ –ù–∞–π–¥–µ–Ω–æ: {len(posts_data)} —Å—ã—Ä—ã—Ö –ø–æ—Å—Ç–æ–≤")
                for post_data in posts_data:
                    render_raw_post_viewer(post_data, st.session_state.language)
            else:
                st.info("–ù–µ—Ç —Å—ã—Ä—ã—Ö –ø–æ—Å—Ç–æ–≤")

        elif data_source == "Reddit (–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ)":
            query = session.query(ProcessedRedditPost)

            if news_only:
                query = query.filter(ProcessedRedditPost.is_news == True)

            if sort_value == "processed_at_desc":
                query = query.order_by(ProcessedRedditPost.processed_at.desc())
            elif sort_value == "score_desc":
                query = query.order_by(ProcessedRedditPost.score.desc())

            processed_posts = query.limit(limit).all()

            # –ü–æ–ª—É—á–∞–µ–º ID —Å—ã—Ä—ã—Ö –ø–æ—Å—Ç–æ–≤ –¥–ª—è –æ–¥–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
            post_ids = [p.post_id for p in processed_posts]
            raw_posts_map = {p.post_id: _reddit_post_to_dict(p) for p in session.query(RedditPost).filter(RedditPost.post_id.in_(post_ids)).all()}

            processed_posts_data = []
            for proc_post in processed_posts:
                raw_post_data = raw_posts_map.get(proc_post.post_id)
                processed_posts_data.append(_processed_reddit_post_to_dict(proc_post, raw_post_data))

            if processed_posts_data:
                filter_text = " (—Ç–æ–ª—å–∫–æ –Ω–æ–≤–æ—Å—Ç–∏)" if news_only else ""
                st.caption(f"ü§ñ –ù–∞–π–¥–µ–Ω–æ: {len(processed_posts_data)} –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –ø–æ—Å—Ç–æ–≤{filter_text}")

                for post_data in processed_posts_data:
                    render_processed_post_viewer(post_data, st.session_state.language)
            else:
                st.info("–ù–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –ø–æ—Å—Ç–æ–≤" + (" (–Ω–æ–≤–æ—Å—Ç–µ–π)" if news_only else ""))

        elif data_source == "Habr":
            query = session.query(HabrArticle)

            # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ–∏–ª—å—Ç—Ä—ã
            if habr_filter == "–¢–æ–ª—å–∫–æ –Ω–æ–≤–æ—Å—Ç–∏":
                query = query.filter(HabrArticle.is_news == True)
            elif habr_filter == "–¢–æ–ª—å–∫–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ":
                query = query.filter(HabrArticle.editorial_processed == True)

            # –ü—Ä–∏–º–µ–Ω—è–µ–º —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫—É
            if habr_sort_value == "scraped_at_desc":
                query = query.order_by(HabrArticle.scraped_at.desc())
            elif habr_sort_value == "pub_date_desc":
                query = query.order_by(HabrArticle.pub_date.desc().nullslast())
            elif habr_sort_value == "rating_desc":
                query = query.order_by(HabrArticle.rating.desc().nullslast())

            articles = query.limit(limit).all()
            articles_data = [_habr_article_to_dict(a) for a in articles]

            if articles_data:
                filter_text = ""
                if habr_filter != "–í—Å–µ":
                    filter_text = f" ({habr_filter.lower()})"
                st.caption(f"üá∑üá∫ –ù–∞–π–¥–µ–Ω–æ: {len(articles_data)} —Å—Ç–∞—Ç–µ–π Habr{filter_text}")

                for article_data in articles_data:
                    render_habr_article_viewer(article_data, st.session_state.language)
            else:
                st.info(f"–ù–µ—Ç —Å—Ç–∞—Ç–µ–π Habr{' (' + habr_filter.lower() + ')' if habr_filter != '–í—Å–µ' else ''}")

        elif data_source == "Telegram":
            messages = session.query(TelegramMessage).order_by(
                TelegramMessage.date.desc()
            ).limit(limit).all()

            if messages:
                st.caption(f"üí¨ –ù–∞–π–¥–µ–Ω–æ: {len(messages)} —Å–æ–æ–±—â–µ–Ω–∏–π")
                for msg in messages:
                    with st.expander(f"@{msg.channel_username} ‚Ä¢ {msg.text[:80] if msg.text else '[Media]'}"):
                        st.markdown(f"**–ö–∞–Ω–∞–ª:** {msg.channel_title}")
                        st.markdown(f"**–î–∞—Ç–∞:** {msg.date}")
                        if msg.text:
                            st.text_area("–¢–µ–∫—Å—Ç", msg.text, height=200, key=f"tg_{msg.id}")
                        if msg.has_media:
                            st.caption(f"üìé –ú–µ–¥–∏–∞: {msg.media_type}")
                        st.caption(f"üëÅÔ∏è –ü—Ä–æ—Å–º–æ—Ç—Ä—ã: {msg.views} | –ü–µ—Ä–µ—Å—ã–ª–æ–∫: {msg.forwards}")
            else:
                st.info("–ù–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏–π Telegram")

        elif data_source == "Medium":
            articles = session.query(MediumArticle).order_by(
                MediumArticle.published_date.desc()
            ).limit(limit).all()

            if articles:
                st.caption(f"üìù –ù–∞–π–¥–µ–Ω–æ: {len(articles)} —Å—Ç–∞—Ç–µ–π")
                for art in articles:
                    with st.expander(f"Medium ‚Ä¢ {art.title[:80]}"):
                        st.markdown(f"**–ê–≤—Ç–æ—Ä:** {art.author}")
                        st.markdown(f"**–î–∞—Ç–∞:** {art.published_date}")
                        if art.description:
                            st.write(art.description)
                        if art.full_text:
                            st.text_area("–ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç", art.full_text, height=300, key=f"med_{art.id}")
                        st.link_button("–û—Ç–∫—Ä—ã—Ç—å –Ω–∞ Medium", art.url)
            else:
                st.info("–ù–µ—Ç —Å—Ç–∞—Ç–µ–π Medium")

        elif data_source == "Telegram –ü–æ—Å—Ç—ã":
            query = session.query(TelegramPost)

            # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ–∏–ª—å—Ç—Ä—ã
            if tg_filter == "–¢–æ–ª—å–∫–æ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–Ω—ã–µ":
                query = query.filter(TelegramPost.is_published == True)
            elif tg_filter == "–¢–æ–ª—å–∫–æ —á–µ—Ä–Ω–æ–≤–∏–∫–∏":
                query = query.filter(TelegramPost.is_published == False)

            # –ü—Ä–∏–º–µ–Ω—è–µ–º —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫—É
            if tg_sort_value == "created_at_desc":
                query = query.order_by(TelegramPost.created_at.desc())
            elif tg_sort_value == "published_at_desc":
                query = query.order_by(TelegramPost.published_at.desc().nullslast())
            elif tg_sort_value == "character_count_desc":
                query = query.order_by(TelegramPost.character_count.desc())

            posts = query.limit(limit).all()
            posts_data = [_telegram_post_to_dict(p) for p in posts]

            if posts_data:
                filter_text = ""
                if tg_filter != "–í—Å–µ":
                    filter_text = f" ({tg_filter.lower()})"
                st.caption(f"üì± –ù–∞–π–¥–µ–Ω–æ: {len(posts_data)} –ø–æ—Å—Ç–æ–≤{filter_text}")

                for post_data in posts_data:
                    render_telegram_post_viewer(post_data, st.session_state.language)
            else:
                st.info(f"–ù–µ—Ç –ø–æ—Å—Ç–æ–≤{' (' + tg_filter.lower() + ')' if tg_filter != '–í—Å–µ' else ''}")

        session.close()

    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö: {e}")
        import traceback
        st.code(traceback.format_exc())

# === TAB 7: SETTINGS ===
with tab7:
    st.header("‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏")

    st.subheader("ü§ñ LLM –û–±—Ä–∞–±–æ—Ç–∫–∞")
    col_llm1, col_llm2 = st.columns(2)
    with col_llm1:
        st.metric("–ú–æ–¥–µ–ª—å", st.session_state.settings['llm_model'])
        st.metric("Temperature", st.session_state.settings['llm_temperature'])
    with col_llm2:
        st.metric("Max Tokens", st.session_state.settings['llm_max_tokens'])
        st.metric("–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –ø–æ—Ç–æ–∫–æ–≤", st.session_state.settings['max_parallel_tasks'])

    st.markdown("---")

    # –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–∞—Å—Ç—Ä–æ–µ–∫ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
    st.subheader("üìã –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è")

    settings = st.session_state.settings

    # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –Ω–∞—Å—Ç—Ä–æ–µ–∫ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
    database_settings = {
        'postgres_user': settings['postgres_user'],
        'postgres_db': settings['postgres_db'],
        'postgres_port': settings['postgres_port']
    }

    reddit_settings = {
        'reddit_client_id': settings['reddit_client_id'],
        'reddit_client_secret': settings['reddit_client_secret'],
        'reddit_user_agent': settings['reddit_user_agent']
    }

    telegram_settings = {
        'telegram_api_id': settings['telegram_api_id'],
        'telegram_api_hash': settings['telegram_api_hash'],
        'telegram_phone': settings['telegram_phone']
    }

    llm_settings = {
        'llm_provider': settings['llm_provider'],
        'llm_model': settings['llm_model'],
        'llm_temperature': settings['llm_temperature'],
        'llm_max_tokens': settings['llm_max_tokens'],
        'llm_top_p': settings['llm_top_p'],
        'max_parallel_tasks': settings['max_parallel_tasks']
    }

    services_settings = {
        'qdrant_url': settings['qdrant_url'],
        'qdrant_port': settings['qdrant_port'],
        'ollama_base_url': settings['ollama_base_url'],
        'ollama_port': settings['ollama_port']
    }

    parsing_settings = {
        'default_max_posts': settings['default_max_posts'],
        'default_delay': settings['default_delay'],
        'default_sort': settings['default_sort'],
        'default_enable_llm': settings['default_enable_llm'],
        'batch_size': settings['batch_size'],
        'min_text_length': settings['min_text_length']
    }

    quality_settings = {
        'enable_semantic_dedup': settings['enable_semantic_dedup'],
        'enable_vectorization': settings['enable_vectorization']
    }

    ui_settings = {
        'logs_max_length': settings['logs_max_length'],
        'viewer_default_limit': settings['viewer_default_limit'],
        'show_debug_info': settings['show_debug_info'],
        'app_port': settings['app_port']
    }

    # –†–µ–Ω–¥–µ—Ä–∏–Ω–≥ –∫–∞—Ç–µ–≥–æ—Ä–∏–π
    render_settings_section("–ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö (PostgreSQL)", database_settings, "üóÑÔ∏è")
    render_settings_section("Reddit API", reddit_settings, "üî¥")
    render_settings_section("Telegram API", telegram_settings, "üí¨")
    render_settings_section("LLM & AI", llm_settings, "ü§ñ")
    render_settings_section("–°–µ—Ä–≤–∏—Å—ã (Qdrant, Ollama)", services_settings, "üîß")
    render_settings_section("–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–∞—Ä—Å–∏–Ω–≥–∞", parsing_settings, "üî•")
    render_settings_section("–ö–∞—á–µ—Å—Ç–≤–æ –∏ –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è", quality_settings, "‚ú®")
    render_settings_section("–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å", ui_settings, "üé®")

    # –≠–∫—Å–ø–æ—Ä—Ç –≤ JSON/YAML
    st.markdown("---")
    st.subheader("üíæ –≠–∫—Å–ø–æ—Ä—Ç –Ω–∞—Å—Ç—Ä–æ–µ–∫")

    col_export1, col_export2 = st.columns(2)

    with col_export1:
        if st.button("üìÑ –°–∫–∞—á–∞—Ç—å JSON", use_container_width=True):
            json_str = json.dumps(settings, indent=2, ensure_ascii=False)
            st.download_button(
                label="‚¨áÔ∏è –°–æ—Ö—Ä–∞–Ω–∏—Ç—å settings.json",
                data=json_str,
                file_name="settings.json",
                mime="application/json"
            )

    with col_export2:
        if st.button("üìã –ü–æ–∫–∞–∑–∞—Ç—å raw JSON", use_container_width=True):
            with st.expander("Raw JSON", expanded=True):
                st.json(settings)

# === TAB 8: API ===
with tab8:
    st.header("üîå API –î–æ—Å—Ç—É–ø")

    col_api1, col_api2 = st.columns([2, 1])

    with col_api1:
        st.subheader("–≠–Ω–¥–ø–æ–∏–Ω—Ç—ã API")

        st.code("""
–ë–∞–∑–æ–≤—ã–π URL: http://localhost:8000

GET /stats - –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
GET /habr/articles - –°—Ç–∞—Ç—å–∏ Habr
GET /habr/articles/{id} - –ö–æ–Ω–∫—Ä–µ—Ç–Ω–∞—è —Å—Ç–∞—Ç—å—è
GET /reddit/posts - –ü–æ—Å—Ç—ã Reddit
GET /telegram/posts - Telegram –ø–æ—Å—Ç—ã
GET /logs - –õ–æ–≥–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞
DELETE /logs - –û—á–∏—Å—Ç–∏—Ç—å –ª–æ–≥–∏
GET /health - –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è
GET /sessions - –ê–∫—Ç–∏–≤–Ω—ã–µ —Å–µ—Å—Å–∏–∏
        """, language="bash")

        st.markdown("#### –ü—Ä–∏–º–µ—Ä—ã –∑–∞–ø—Ä–æ—Å–æ–≤:")

        col_ex1, col_ex2 = st.columns(2)

        with col_ex1:
            st.code("""
# –ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
curl http://localhost:8000/stats

# –ü–æ–ª—É—á–∏—Ç—å 10 —Å—Ç–∞—Ç–µ–π Habr
curl "http://localhost:8000/habr/articles?limit=10"

# –ü–æ–ª—É—á–∏—Ç—å —Ç–æ–ª—å–∫–æ –Ω–æ–≤–æ—Å—Ç–∏
curl "http://localhost:8000/habr/articles?is_news=true"
            """, language="bash")

        with col_ex2:
            st.code("""
# –ü–æ–ª—É—á–∏—Ç—å –ª–æ–≥–∏
curl http://localhost:8000/logs

# –û—á–∏—Å—Ç–∏—Ç—å –ª–æ–≥–∏
curl -X DELETE http://localhost:8000/logs

# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∑–¥–æ—Ä–æ–≤—å–µ
curl http://localhost:8000/health
            """, language="bash")

    with col_api2:
        st.subheader("–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ API")

        if st.button("üìä –ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É", use_container_width=True):
            try:
                response = requests.get("http://api:8000/stats", timeout=5)
                if response.status_code == 200:
                    data = response.json()
                    st.success("‚úÖ –£—Å–ø–µ—à–Ω—ã–π –æ—Ç–≤–µ—Ç")
                    st.json(data)
                else:
                    st.error(f"‚ùå –û—à–∏–±–∫–∞: {response.status_code}")
                    st.text(response.text)
            except Exception as e:
                st.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è: {e}")

        if st.button("üìã –ü–æ–ª—É—á–∏—Ç—å –ª–æ–≥–∏", use_container_width=True):
            try:
                response = requests.get("http://api:8000/logs?limit=20", timeout=5)
                if response.status_code == 200:
                    data = response.json()
                    st.success(f"‚úÖ –ü–æ–ª—É—á–µ–Ω–æ {len(data)} –∑–∞–ø–∏—Å–µ–π")

                    # –û—Ç–æ–±—Ä–∞–∂–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 5 –∑–∞–ø–∏—Å–µ–π
                    for log in data[-5:]:
                        level = log.get('level', 'INFO')
                        icon = {"INFO": "‚ÑπÔ∏è", "SUCCESS": "‚úÖ", "WARNING": "‚ö†Ô∏è", "ERROR": "‚ùå"}.get(level, "üìù")
                        st.caption(f"{icon} {log.get('timestamp', '')[:8]} {log.get('message', '')}")
                else:
                    st.error(f"‚ùå –û—à–∏–±–∫–∞: {response.status_code}")
                    st.text(response.text)
            except Exception as e:
                st.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è: {e}")

        if st.button("üè• –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∑–¥–æ—Ä–æ–≤—å–µ", use_container_width=True):
            try:
                response = requests.get("http://api:8000/health", timeout=5)
                if response.status_code == 200:
                    data = response.json()
                    st.success("‚úÖ API –∑–¥–æ—Ä–æ–≤")
                    st.json(data)
                else:
                    st.error(f"‚ùå –û—à–∏–±–∫–∞: {response.status_code}")
            except Exception as e:
                st.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è: {e}")

        if st.button("üîÑ –ü–æ–ª—É—á–∏—Ç—å –∞–∫—Ç–∏–≤–Ω—ã–µ —Å–µ—Å—Å–∏–∏", use_container_width=True):
            try:
                response = requests.get("http://api:8000/sessions", timeout=5)
                if response.status_code == 200:
                    data = response.json()
                    sessions = data.get('sessions', [])
                    st.success(f"‚úÖ –ê–∫—Ç–∏–≤–Ω—ã—Ö —Å–µ—Å—Å–∏–π: {len(sessions)}")

                    for session in sessions:
                        with st.expander(f"–°–µ—Å—Å–∏—è: {session.get('id', '')[:8]}..."):
                            st.json(session)
                else:
                    st.error(f"‚ùå –û—à–∏–±–∫–∞: {response.status_code}")
            except Exception as e:
                st.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è: {e}")

# === FOOTER ===
st.markdown("---")
col_f1, col_f2, col_f3 = st.columns([2, 1, 1])
with col_f1:
    st.caption("PostgreSQL ‚Ä¢ Docker ‚Ä¢ N8N ‚Ä¢ Ollama ‚Ä¢ GPT-OSS ‚Ä¢ Scrapy")
with col_f2:
    current_model = st.session_state.settings['llm_model']
    st.caption(f"ü§ñ Model: {current_model}")
with col_f3:
    if st.button("üîÑ –û–±–Ω–æ–≤–∏—Ç—å", key="refresh_btn"):
        st.rerun()