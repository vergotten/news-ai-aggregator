services:
  postgres:
    image: postgres:15-alpine
    container_name: news-aggregator-db
    restart: unless-stopped
    environment:
      POSTGRES_USER: ${POSTGRES_USER:-newsaggregator}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-changeme123}
      POSTGRES_DB: ${POSTGRES_DB:-news_aggregator}
      PGDATA: /var/lib/postgresql/data/pgdata
    ports:
      - "${POSTGRES_PORT:-5432}:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./init-db.sql:/docker-entrypoint-initdb.d/init.sql:ro
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-newsaggregator} -d ${POSTGRES_DB:-news_aggregator}"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    networks:
      - news-aggregator-network

  qdrant:
    image: qdrant/qdrant:latest
    container_name: news-aggregator-qdrant
    restart: unless-stopped
    environment:
      QDRANT__TELEMETRY_DISABLED: "true"
      QDRANT__LOG_LEVEL: INFO
    ports:
      - "${QDRANT_PORT:-6333}:6333"
      - "${QDRANT_GRPC_PORT:-6334}:6334"
    volumes:
      - qdrant_data:/qdrant/storage
    networks:
      - news-aggregator-network
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:6333/healthz"]
      interval: 30s
      timeout: 10s
      retries: 3

  ollama:
    image: ollama/ollama:latest
    container_name: news-aggregator-ollama
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [ gpu ]
    environment:
      OLLAMA_HOST: "0.0.0.0:11434"
      # Force GPU support
      OLLAMA_NUM_GPU: 1
      # Allow partial GPU usage (layers that fit in VRAM go to GPU, rest on CPU)
      OLLAMA_MAX_LOADED_LAYERS: 20
      # Optional: disable telemetry and adjust model cache
      OLLAMA_TELEMETRY: "false"
    ports:
      - "${OLLAMA_PORT:-11434}:11434"
    volumes:
      - ollama_data:/root/.ollama
    networks:
      - news-aggregator-network
    healthcheck:
      test: [ "CMD", "ollama", "list" ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    runtime: nvidia


  ollama-init:
    image: ollama/ollama:latest
    container_name: news-aggregator-ollama-init
    depends_on:
      ollama:
        condition: service_healthy
    volumes:
      - ollama_data:/root/.ollama
    networks:
      - news-aggregator-network
    environment:
      OLLAMA_HOST: "http://ollama:11434"
    entrypoint: [ "/bin/sh", "-c" ]
    command:
      - |
        echo "=============================================="
        echo "Ollama Model Initialization"
        echo "=============================================="
        
        echo "Waiting for Ollama server to be fully ready..."
        sleep 15
        
        echo ""
        echo "Checking existing models..."
        MODELS=$$(ollama list 2>/dev/null || echo "")
        echo "$$MODELS"
        echo ""
        
        # ============================================
        # Embeddings model (required for vectorization)
        # ============================================
        if echo "$$MODELS" | grep -q "nomic-embed-text"; then
          echo "✓ nomic-embed-text already installed"
        else
          echo "Installing nomic-embed-text (274MB, ~1 minute)..."
          ollama pull nomic-embed-text
          echo "✓ nomic-embed-text installed"
        fi
        
        echo ""
        
        # ============================================
        # Main LLM model - llama3.2:3b (ACTIVE)
        # ============================================
        # if echo "$$MODELS" | grep -q "llama3.2:3b"; then
        #   echo "✓ llama3.2:3b already installed"
        # else
        #   echo "=============================================="
        #   echo "Installing llama3.2:3b"
        #   echo "Size: ~2GB | Time: 2-5 minutes"
        #   echo "=============================================="
        #   ollama pull llama3.2:3b
        #   echo "✓ llama3.2:3b installed"
        # fi
        
        # ============================================
        # GPT-OSS model (COMMENTED OUT - heavy model)
        # Uncomment if you need more powerful model
        # Requires: 16GB+ RAM, 13GB disk, 15-30 min download
        # ============================================
         if echo "$$MODELS" | grep -q "gpt-oss:20b"; then
           echo "✓ gpt-oss:20b already installed"
         else
           echo "=============================================="
           echo "Installing gpt-oss:20b"
           echo "Size: ~13GB | Time: 15-30 minutes"
           echo "=============================================="
           ollama pull gpt-oss:20b
           echo "✓ gpt-oss:20b installed"
         fi
        
        echo ""
        echo "=============================================="
        echo "Final model list:"
        echo "=============================================="
        ollama list
        echo ""
        echo "✓ Ollama initialization complete"
        echo "=============================================="
    restart: "no"

  app:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: news-aggregator-app
    restart: unless-stopped
    depends_on:
      postgres:
        condition: service_healthy
      qdrant:
        condition: service_started
      ollama:
        condition: service_healthy
      ollama-init:
        condition: service_completed_successfully
    environment:
      DATABASE_URL: postgresql://${POSTGRES_USER:-newsaggregator}:${POSTGRES_PASSWORD:-changeme123}@postgres:5432/${POSTGRES_DB:-news_aggregator}
      REDDIT_CLIENT_ID: ${REDDIT_CLIENT_ID}
      REDDIT_CLIENT_SECRET: ${REDDIT_CLIENT_SECRET}
      REDDIT_USER_AGENT: ${REDDIT_USER_AGENT:-NewsAggregator/1.0}
      TELEGRAM_API_ID: ${TELEGRAM_API_ID}
      TELEGRAM_API_HASH: ${TELEGRAM_API_HASH}
      TELEGRAM_PHONE: ${TELEGRAM_PHONE}
      PYTHONUNBUFFERED: 1
      TZ: ${TZ:-UTC}
      STREAMLIT_SERVER_PORT: ${STREAMLIT_SERVER_PORT:-8501}
      STREAMLIT_SERVER_ADDRESS: ${STREAMLIT_SERVER_ADDRESS:-0.0.0.0}
      QDRANT_URL: http://qdrant:6333
      OLLAMA_BASE_URL: http://ollama:11434
    ports:
      - "${APP_PORT:-8501}:8501"
    volumes:
      - ./src:/app/src:ro
      - ./config:/app/config:ro
      - app_logs:/app/logs
      - app_data:/app/data
    networks:
      - news-aggregator-network

  n8n:
    image: n8nio/n8n:latest
    container_name: news-aggregator-n8n
    restart: unless-stopped
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      DB_TYPE: postgresdb
      DB_POSTGRESDB_HOST: postgres
      DB_POSTGRESDB_PORT: 5432
      DB_POSTGRESDB_DATABASE: ${N8N_DB:-n8n}
      DB_POSTGRESDB_USER: ${POSTGRES_USER:-newsaggregator}
      DB_POSTGRESDB_PASSWORD: ${POSTGRES_PASSWORD:-changeme123}
      N8N_BASIC_AUTH_ACTIVE: ${N8N_BASIC_AUTH_ACTIVE:-true}
      N8N_BASIC_AUTH_USER: ${N8N_BASIC_AUTH_USER:-admin}
      N8N_BASIC_AUTH_PASSWORD: ${N8N_BASIC_AUTH_PASSWORD:-changeme123}
      N8N_ENFORCE_SETTINGS_FILE_PERMISSIONS: ${N8N_ENFORCE_SETTINGS_FILE_PERMISSIONS:-true}
      TZ: ${TZ:-UTC}
    ports:
      - "${N8N_PORT:-5678}:5678"
    volumes:
      - n8n_data:/home/node/.n8n
    networks:
      - news-aggregator-network

  adminer:
    image: adminer:latest
    container_name: news-aggregator-adminer
    restart: unless-stopped
    ports:
      - "${ADMINER_PORT:-8080}:8080"
    networks:
      - news-aggregator-network

volumes:
  postgres_data:
  n8n_data:
  ollama_data:
  qdrant_data:
  app_logs:
  app_data:

networks:
  news-aggregator-network:
    driver: bridge